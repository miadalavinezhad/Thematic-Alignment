title,abstract,year,venue
PLoS Computational Biology Issue Image | Vol. 17(9) October 2021.,"The tomato flowers are characterized by possessing poricidal anthers, which restrict the exit of the pollen to a tiny opening on the apex of the anther. To extract pollen efficiently, some visiting bees grasp the anthers and quickly contracting their flight muscles, producing vibrations and an audible sound. The vibrations are transferred to the anthers, shaking and stimulating the pollen inside them to leave by the pores, a phenomenon known as floral sonication or buzz-pollination. DOI: pcbi.1009426 Image Credit: Priscila de CE1;ssia Souza AraFA;jo (co-author of the manuscript) photographed this bee visiting flowers of tomato plants grown at the experimental fields of the Federal University of ViE7;osa (Minas Gerais State, Brazil). We confirm that the image can publish under the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/). The authors own the copyright for the image and confirm that agree with open Access License of PLOS Computational Biology.",2021,PLoS Computational Biology
PLoS Computational Biology Issue Image | Vol. 1(9) October 2021.,"The tomato flowers are characterized by possessing poricidal anthers, which restrict the exit of the pollen to a tiny opening on the apex of the anther. To extract pollen efficiently, some visiting bees grasp the anthers and quickly contracting their flight muscles, producing vibrations and an audible sound. The vibrations are transferred to the anthers, shaking and stimulating the pollen inside them to leave by the pores, a phenomenon known as floral sonication or buzz-pollination. DOI: pcbi.1009426 Image Credit: Priscila Souza AraFA;jo (co-author of the manuscript) photographed this bee visiting flowers of tomato plants grown at the experimental fields of the Federal University of ViE7;osa (Minas Gerais State, Brazil). We confirm that the image can publish under the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/). The authors own the copyright for the image and confirm that agree with open Access License of PLOS Computational Biology.",2021,PLoS Computational Biology
Improving reproducibility in computational biology research,"1 Department of Biomedical Engineering, University of Virginia, Charlottesville, Virginia, United States of America, 2 Department of Biomedical Engineering, Johns Hopkins University, Baltimore, Maryland, United States of America, 3 Department of Bioengineering, University of Washington, Seattle, Washington, United States of America, 4 Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand",2020,PLoS Comput. Biol.
Ten simple rules for designing learning experiences that involve enhancing computational biology Wikipedia articles,"Wikipedia is the largest and most visited encyclopedia on the World Wide Web [1]. Wikipedia is frequently accessed as an educational resource in computational biology, with the articles on Bioinformatics and CRISPR being viewed 413,000 and 1.18 million times, respectively, in 2019 [2]. However, academics remain skeptical of Wikipedia as a reliable source of knowledge [3]. A common complaint by educators is the perceived lack of quality of information found on Wikipedia. Some educators also worry that the platform discourages deeper learner engagement, providing learners with a crutch instead of ways to engage in rigorous, secondary research within a discipline. Both of these concerns often overshadow the advantages of free and readily available knowledge. Given that Wikipedia is one of the most visited websites and an established platform for knowledge seekers, it makes sense to address these concerns and help learners make the most of what they would do anyway. Mentored contributions from students to open platforms like Wikipedia offer opportunities for improved rigor, quality, depth, and reliability of the information indexed and make it relatable to a wide audience. There have been stellar examples of such mentored contributions, resulting in well-curated additions to domain-specific knowledge amenable for consumption by both public and specialist audiences. For instance, educators around the world have mentored students to either improve or create Wikipedia articles and enter the annual International Society for Computational Biology (ISCB) Wikipedia Competition, a grassroots initiative designed to improve the coverage and depth of computational biology topics [4,5]. Entries are judged based on the quality of writing, figures, and depth of subject knowledge [6]. Winning entries have included important topics in computational biology, such as chromosome conformation capture [7], molecular phylogenetics [8], and the Ruzzo-Tompa algorithm [9]. Some educators have gone further, replacing the writing of traditional term papers with the rigorous editing or creation of new articles in Wikipedia, resulting in class assignments that enhance a vital, publicly accessible resource of field-specific information. Trainees at 24 United States universities participated in a pilot of the Wikipedia Education Program [10] during the 2010–2011 academic year. A 2012 article reviewed the experiences of four professors who participated, each assigning trainees to write Wikipedia articles on a course topic in place of a term paper [11]. Although each professor tailored assignments to her/his particular class, all found the assignments “extremely useful” in improving trainees’ learning. Similar positive outcomes have been reported in other studies, including larger introductory courses of over 100 PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
"10 simple rules for teaching wet-lab experimentation to computational biology students, i.e., turning computer mice into lab rats","1 Joint Carnegie Mellon–University of Pittsburgh PhD Program in Computational Biology, Pittsburgh, Pennsylvania, United States of America, 2 Department of Computational and Systems Biology, University of Pittsburgh School of Medicine, Pittsburgh, Pennsylvania, United States of America, 3 Computational Biology Department, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America",2020,PLoS Comput. Biol.
Putting benchmarks in their rightful place: The heart of computational biology,"Research in computational biology has given rise to a vast number of methods developed to solve scientific problems. For areas in which many approaches exist, researchers have a hard time deciding which tool to select to address a scientific challenge, as essentially all publications introducing a new method will claim better performance than all others. Not all of these claims can be correct. Equally, for this same reason, developers struggle to demonstrate convincingly that they created a new and superior algorithm or implementation. Moreover, the developer community often has difficulty discerning which new approaches constitute true scientific advances for the field. The obvious answer to this conundrum is to develop benchmarks—meaning standard points of reference that facilitate evaluating the performance of different tools—allowing both users and developers to compare multiple tools in an unbiased fashion.",2018,PLoS Comput. Biol.
Establishing a computational biology flipped classroom,"In a flipped classroom, students complete automated modules to replace a traditional lecture, allowing the time devoted for the lecture to be devoted to constructive tasks reinforcing student knowledge. Yet although the flipped classroom offers a compelling approach for fostering a constructivist, student-centric learning environment, research on the efficacy of flipped classes has been mixed. For that matter, is it possible to successfully flip a classroom in an advanced, heavily specialized course like a bioinformatics algorithms course? Over the past several years, the author has implemented a flipped version of such a course and will discuss some of the successes and pitfalls encountered.",2019,PLoS Comput. Biol.
"Women are underrepresented in computational biology: An analysis of the scholarly literature in biology, computer science and computational biology","While women are generally underrepresented in STEM fields, there are noticeable differences between fields. For instance, the gender ratio in biology is more balanced than in computer science. We were interested in how this difference is reflected in the interdisciplinary field of computational/quantitative biology. To this end, we examined the proportion of female authors in publications from the PubMed and arXiv databases. There are fewer female authors on research papers in computational biology, as compared to biology in general. This is true across authorship position, year, and journal impact factor. A comparison with arXiv shows that quantitative biology papers have a higher ratio of female authors than computer science papers, placing computational biology in between its two parent fields in terms of gender representation. Both in biology and in computational biology, a female last author increases the probability of other authors on the paper being female, pointing to a potential role of female PIs in influencing the gender balance.",2017,PLoS Comput. Biol.
Ten Simple Rules for Developing Usable Software in Computational Biology,"The rise of high-throughput technologies in molecular biology has led to a massive amount of publicly available data. While computational method development has been a cornerstone of biomedical research for decades, the rapid technological progress in the wet lab makes it difficult for software development to keep pace. Wet lab scientists rely heavily on computational methods, especially since more research is now performed in silico. However, suitable tools do not always exist, and not everyone has the skills to write complex software. Computational biologists are required to close this gap, but they often lack formal training in software engineering. To alleviate this, several related challenges have been previously addressed in the Ten Simple Rules series, including reproducibility [1], effectiveness [2], and open-source development of software [3, 4]. 
 
Here, we want to shed light on issues concerning software usability. Usability is commonly defined as “a measure of interface quality that refers to the effectiveness, efficiency, and satisfaction with which users can perform tasks with a tool” [5]. Considering the subjective nature of this topic, a broad consensus may be hard to achieve. Nevertheless, good usability is imperative for achieving wide acceptance of a software tool in the community. In many cases, academic software starts out as a prototype that solves one specific task and is not geared for a larger user group. As soon as the developer realizes that the complexity of the problems solved by the software could make it widely applicable, the software will grow to meet the new demands. At least by this point, if not sooner, usability should become a priority. Unfortunately, efforts in scientific software development are constrained by limited funding, time, and rapid turnover of group members. As a result, scientific software is often poorly documented, non-intuitive, non-robust with regards to input data and parameters, and hard to install. For many use cases, there is a plethora of tools that appear very similar and make it difficult for the user to select the one that best fits their needs. Not surprisingly, a substantial fraction of these tools are probably abandonware; i.e., these are no longer actively developed or supported in spite of their potential value to the scientific community. 
 
To our knowledge, software development as part of scientific research is usually carried out by individuals or small teams with no more than two or three members. Hence, the responsibility of designing, implementing, testing, and documenting the code rests on few shoulders. Additionally, there is pressure to produce publishable results or, at least, to contribute analysis work to ongoing projects. Consequently, academic software is typically released as a prototype. We acknowledge that such a tool cannot adhere to and should not be judged by the standards that we take for granted for production grade software. However, widespread use of a tool is typically in the interest of a researcher. To this end, we propose ten simple rules that, in our experience, have a considerable impact on improving usability of scientific software.",2017,PLoS Comput. Biol.
The ISCB Student Council Internship Program: Expanding computational biology capacity worldwide,"Education and training are two essential ingredients for a successful career. On one hand, universities provide students a curriculum for specializing in one’s field of study, and on the other, internships complement coursework and provide invaluable training experience for a fruitful career. Consequently, undergraduates and graduates are encouraged to undertake an internship during the course of their degree. The opportunity to explore one’s research interests in the early stages of their education is important for students because it improves their skill set and gives their career a boost. In the long term, this helps to close the gap between skills and employability among students across the globe and balance the research capacity in the field of computational biology. However, training opportunities are often scarce for computational biology students, particularly for those who reside in less-privileged regions. Aimed at helping students develop research and academic skills in computational biology and alleviating the divide across countries, the Student Council of the International Society for Computational Biology introduced its Internship Program in 2009. The Internship Program is committed to providing access to computational biology training, especially for students from developing regions, and improving competencies in the field. Here, we present how the Internship Program works and the impact of the internship opportunities so far, along with the challenges associated with this program.",2018,PLoS Comput. Biol.
The Development of Computational Biology in South Africa: Successes Achieved and Lessons Learnt,"Bioinformatics is now a critical skill in many research and commercial environments as biological data are increasing in both size and complexity. South African researchers recognized this need in the mid-1990s and responded by working with the government as well as international bodies to develop initiatives to build bioinformatics capacity in the country. Significant injections of support from these bodies provided a springboard for the establishment of computational biology units at multiple universities throughout the country, which took on teaching, basic research and support roles. Several challenges were encountered, for example with unreliability of funding, lack of skills, and lack of infrastructure. However, the bioinformatics community worked together to overcome these, and South Africa is now arguably the leading country in bioinformatics on the African continent. Here we discuss how the discipline developed in the country, highlighting the challenges, successes, and lessons learnt.",2016,PLoS Comput. Biol.
Computational Biology: Moving into the Future One Click at a Time,"Philip Bourne (Fig 1) began his scientific career in the wet lab, like many of his computational biology contemporaries. He earned his PhD in physical chemistry from the Flinders University of South Australia and pursued postdoctoral training at the University of Sheffield, where he began studying protein structure. Bourne accepted his first academic position in 1995 in the Department of Pharmacology at the University of California, San Diego (UCSD), rose to the rank of professor, and was associate vice chancellor for Innovation and Industry Alliances of the Office of Research Affairs. During his time at UCSD, he built a broad research program that used bioinformatics and systems biology to examine protein structure and function, evolution, drug discovery, disease, and immunology. Bourne also developed the Research Collaboratory for Structural Bioinformatics (RCSB) Protein Data Bank (PDB) and Immune Epitope Database (IEDB), which have become valuable data resources for the research community. In 2014, Bourne accepted the newly created position of associate director for data science (ADDS) at the National Institutes of Health (NIH), and he has been tasked with leading an NIH-wide initiative to better utilize the vast and growing collections of biomedical data in more effective and innovative ways. Bourne has been deeply involved with the International Society for Computational Biology (ISCB) throughout his career and is the founding editor-in-chief (EIC) of PLOS Computational Biology, an official journal of ISCB. He has been a firm believer in open access to scientific literature and the effective dissemination of data and results, for which PLOS Computational Biology is an exemplary model. Bourne believes that open access is more than just the ability to read free articles, and he said, “The future is using this content in really effective ways.”He referenced an article he cowrote with J. Lynn Fink and Mark Gerstein in 2008, titled “Open Access: Taking Full Advantage of the Content,” which argued that the full potential of open access has not been realized, as “no killer apps” exist that troll the literature and cross-reference other databases to come up with new discoveries [1]. Bourne believes that the scientific",2015,PLoS Comput. Biol.
ISCB Ebola Award for Important Future Research on the Computational Biology of Ebola Virus,"Speed is of the essence in combating Ebola; thus, computational approaches should form a significant component of Ebola research. As for the development of any modern drug, computational biology is uniquely positioned to contribute through comparative analysis of the genome sequences of Ebola strains as well as 3-D protein modeling. Other computational approaches to Ebola may include large-scale docking studies of Ebola proteins with human proteins and with small-molecule libraries, computational modeling of the spread of the virus, computational mining of the Ebola literature, and creation of a curated Ebola database. Taken together, such computational efforts could significantly accelerate traditional scientific approaches. In recognition of the need for important and immediate solutions from the field of computational biology against Ebola, the International Society for Computational Biology (ISCB) announces a prize for an important computational advance in fighting the Ebola virus. ISCB will confer the ISCB Fight against Ebola Award, along with a prize of US$2,000, at its July 2016 annual meeting (ISCB Intelligent Systems for Molecular Biology [ISMB] 2016, Orlando, Florida).",2015,PLoS Comput. Biol.
"Computational Modeling, Formal Analysis, and Tools for Systems Biology","As the amount of biological data in the public domain grows, so does the range of modeling and analysis techniques employed in systems biology. In recent years, a number of theoretical computer science developments have enabled modeling methodology to keep pace. The growing interest in systems biology in executable models and their analysis has necessitated the borrowing of terms and methods from computer science, such as formal analysis, model checking, static analysis, and runtime verification. Here, we discuss the most important and exciting computational methods and tools currently available to systems biologists. We believe that a deeper understanding of the concepts and theory highlighted in this review will produce better software practice, improved investigation of complex biological processes, and even new ideas and better feedback into computer science.",2016,PLoS Comput. Biol.
Computational Biology and Bioinformatics in Nigeria,"Over the past few decades, major advances in the field of molecular biology, coupled with advances in genomic technologies, have led to an explosive growth in the biological data generated by the scientific community. The critical need to process and analyze such a deluge of data and turn it into useful knowledge has caused bioinformatics to gain prominence and importance. Bioinformatics is an interdisciplinary research area that applies techniques, methodologies, and tools in computer and information science to solve biological problems. In Nigeria, bioinformatics has recently played a vital role in the advancement of biological sciences. As a developing country, the importance of bioinformatics is rapidly gaining acceptance, and bioinformatics groups comprised of biologists, computer scientists, and computer engineers are being constituted at Nigerian universities and research institutes. In this article, we present an overview of bioinformatics education and research in Nigeria. We also discuss professional societies and academic and research institutions that play central roles in advancing the discipline in Nigeria. Finally, we propose strategies that can bolster bioinformatics education and support from policy makers in Nigeria, with potential positive implications for other developing countries.",2014,PLoS Comput. Biol.
A New Online Computational Biology Curriculum,A recent proliferation of Massive Open Online Courses (MOOCs) and other web-based educational resources has greatly increased the potential for effective self-study in many fields. This article introduces a catalog of several hundred free video courses of potential interest to those wishing to expand their knowledge of bioinformatics and computational biology. The courses are organized into eleven subject areas modeled on university departments and are accompanied by commentary and career advice.,2014,PLoS Comput. Biol.
Computational Cancer Biology: An Evolutionary Perspective,"Cancer is a leading cause of death worldwide and represents one of the biggest biomedical research challenges of our time. Tumor progression is caused by somatic evolution of cell populations. Cancer cells expand because of the accumulation of selectively advantageous mutations, and expanding clones give rise to new cell subpopulations with increasingly higher somatic fitness (Fig 1). In the 1970s, Nowell and others established this somatic evolutionary view of cancer [1]. Today, computational biologists have the opportunity to take advantage of large-scale molecular profiling data in order to carve out the principles of tumor evolution and to elucidate how it manifests across cancer types. Analogous to other evolutionary studies, mathematical modeling will be key to the success of understanding the somatic evolution of cancer [2]. 
 
 
 
Fig 1 
 
Schematic representation of neoplastic transformation. 
 
 
 
In general, cancer research involves a range of clinical, epidemiological, and molecular approaches, as well as mathematical and computational modeling. An early and very successful example of mathematical modeling was the work of Nordling [3] and of Armitage and Doll [4]. In the 1950s, long before cancer genome data was available, they analyzed cancer incidence data and postulated, based on the observed age-incidence curves, that cancer is a multistep process. In search of these rate-limiting events, cancer progression was then linked to the accumulation of genomic alterations. Since then, the evolutionary perspective on cancer has proven useful in many instances, and the mathematical theory of cancer evolution has been developed much further. However, little clinical benefit could be gained from this approach so far. Much of evolutionary modeling in general, and of cancer in particular, has remained conceptual or qualitative, either because of strong simplifications in the interest of mathematical tractability or lack of informative data. 
 
Next-generation sequencing (NGS) technologies and their various applications have changed this situation fundamentally [5]. Today, cancer cells can be analyzed in great detail at the molecular level, and tumor cell populations can be sampled extensively. Driven by this technological revolution, large numbers of high-dimensional molecular profiles of tumors, and even of individual cancer cells, are collected by cancer genome consortia, as well as by many individual labs. Large catalogs of cancer genomes, epigenomes, transcriptomes, proteomes, and other molecular profiles are generated to assess variation among tumors from different patients (intertumor heterogeneity) as well as among individual cells of single tumors (intratumor heterogeneity). These data hold the promise not only of new cancer biology discoveries but also of progress in cancer diagnostics and treatment. 
 
Analyzing these complex data and interpreting them in the context of ongoing somatic evolution, disease progression, and treatment response is a major challenge, and the prospects to improve cancer treatment depend critically on progress with these computational and statistical tasks. In the following, we briefly summarize the current state of the art in the field and highlight major challenges that lie ahead, including (i) reconstruction of evolutionary history based on different types of genomic alterations, (ii) functional interpretation of mutations, and (iii) predictive modeling of the evolutionary dynamics of cancer. We argue that an interdisciplinary approach, including statistical and computational data analysis as well as evolutionary modeling of cancer, will be essential for translating technological advances into clinical benefits.",2016,PLoS Comput. Biol.
ISCB Computational Biology Wikipedia Competition,"The International Society for Computational Biology is pleased to announce the 2013 ISCB Computational Biology Wikipedia competition. The competition, in which entrants create or improve the content of any Wikipedia article in the field of computational biology, is open to all students and trainees. Further information about the competition can be found here: http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Computational_Biology/ISCB_competition_announcement_2013 
 
The mission of the ISCB is to promote the use of computational biology and to help educate the next generation of computational biologists. The society has numerous activities that help to address these aims, including conferences, training and mentoring initiatives, and an active student council. 
 
As the world's largest online encyclopedia, Wikipedia has become an indispensable resource for those seeking information on all scientific and technical topics. The English language version of Wikipedia contains over 4.2 million articles, and Wikipedia is now available in 286 languages. The global rise in smartphone use, which allows access to Wikipedia, means that a large fraction of the world's population can now gain access to the world's knowledge. Wikipedia is the most successful example of crowd-sourcing with about 80,000 active editors updating its content each month. 
 
But is Wikipedia a good source of information for computational biology? Certainly, many people are reading the articles. For example, the Bioinformatics article has been visited 1,600 times per day over the last 3 months. Wikipedia contains articles on algorithms, biological databases, software packages, and biographies of eminent computational biologists. The computational biology content ranges from incomplete, a mere “stub” of an article in Wikipedia parlance, to highly detailed Featured Articles. A group of Wikipedia editors have formed the Computational Biology Wikiproject (http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Computational_Biology). This group oversees the computational biology articles and rates them for their importance and their quality. Figure 1 shows the current state of the articles (see also Figure S1). In total, there are over 1,140 articles that have been considered as falling under Computational Biology. There are a small number of articles that have been brought up to the highest levels of quality (Featured Article and Good Article) such as Multiple Sequence Alignment, Genome Wide Association Study, and Folding@home. 
 
 
 
Figure 1 
 
The computational biology articles rated by quality and importance by the Wikipedia Computational Biology Wikiproject. 
 
 
 
The 2012 competition began 9th September 2012 (coinciding with the start of the European Conference on Computational Biology) and finished four months later on the 10th January 2013. Each article entered in the competition was reviewed for a difference in article quality between these two dates. In 2012, there were 13 substantive entries into the competition. Six of these articles were shortlisted by members of the ISCB Student Council and then considered by the judging panel. The judging panel considered articles based on the criteria of clarity of the writing, depth of knowledge of the subject, and quality of figures and images used. In one case, it was clear that the article was largely derived from a published review, and was not considered further. For the other entries, the quantity and quality of the contributions were very good, and it was a challenge to rank the articles. After much deliberation, the judging panel selected the following as the winners of the 2012 ISCB Wikipedia competition: 
 
 
1st prize: James Estevez for improvements to the Genomics Article. 
 
 
2nd prize: Benjamin Moore for improvements to the European Nucleotide Archive article. 
 
 
3rd prize: Luis Pedro Coelho for improvements to the Bioimage Analysis article. 
 
 
 
We are keen to grow the depth and quality of computational biology articles and wish to encourage the widest possible range of students and trainees to take part. We envisage that teachers, tutors, and lecturers could use the competition as an opportunity to train students in literature research on topics of computational biology. This approach to literature review provides the students with a thorough grounding in the subject area of the article. In addition, the collaborative writing environment of Wikipedia encourages critical thinking and improves literature research skills. Furthermore, compared to traditional literature reviews carried out by students, which typically end up unread in a filing cabinet, contributing to Wikipedia means that the students' scholarly contributions will be publicly visible. 
 
We hope that the ISCB Wikipedia competition will continue to grow and help improve the quality of Computational Biology information freely available on the Internet. We are interested in improving not just the articles in Wikipedia, but also the associated media, such as images and figures on Wikimedia Commons, and data through Wikidata. We encourage you to get involved by either entering the competition if you are a student or trainee, or getting your own students to participate.",2013,PLoS Comput. Biol.
Integrating Interactive Computational Modeling in Biology Curricula,"While the use of computer tools to simulate complex processes such as computer circuits is normal practice in fields like engineering, the majority of life sciences/biological sciences courses continue to rely on the traditional textbook and memorization approach. To address this issue, we explored the use of the Cell Collective platform as a novel, interactive, and evolving pedagogical tool to foster student engagement, creativity, and higher-level thinking. Cell Collective is a Web-based platform used to create and simulate dynamical models of various biological processes. Students can create models of cells, diseases, or pathways themselves or explore existing models. This technology was implemented in both undergraduate and graduate courses as a pilot study to determine the feasibility of such software at the university level. First, a new (In Silico Biology) class was developed to enable students to learn biology by “building and breaking it” via computer models and their simulations. This class and technology also provide a non-intimidating way to incorporate mathematical and computational concepts into a class with students who have a limited mathematical background. Second, we used the technology to mediate the use of simulations and modeling modules as a learning tool for traditional biological concepts, such as T cell differentiation or cell cycle regulation, in existing biology courses. Results of this pilot application suggest that there is promise in the use of computational modeling and software tools such as Cell Collective to provide new teaching methods in biology and contribute to the implementation of the “Vision and Change” call to action in undergraduate biology education by providing a hands-on approach to biology.",2015,PLoS Comput. Biol.
Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks,"1 Design Lab, UC San Diego, La Jolla, California, United States of America, 2 Center for Computational Biology and Bioinformatics, UC San Diego, La Jolla, California, United States of America, 3 Department of Pediatrics, UC San Diego, La Jolla, California, United States of America, 4 Data Science Hub, San Diego Supercomputer Center, UC San Diego, La Jolla, California, United States of America, 5 Departments of Bioengineering, and Computer Science and Engineering, and Center for Microbiome Innovation, UC San Diego, La Jolla, California, United States of America, 6 Bioinformatics and Systems Biology Graduate Program, UC San Diego, La Jolla, California, United States of America, 7 Department of Statistics and Berkeley Institute for Data Science, UC Berkeley, and Lawrence Berkeley National Laboratory, Berkeley, California, United States of America",2019,PLoS Comput. Biol.
Chaste: An Open Source C++ Library for Computational Physiology and Biology,"Chaste — Cancer, Heart And Soft Tissue Environment — is an open source C++ library for the computational simulation of mathematical models developed for physiology and biology. Code development has been driven by two initial applications: cardiac electrophysiology and cancer development. A large number of cardiac electrophysiology studies have been enabled and performed, including high-performance computational investigations of defibrillation on realistic human cardiac geometries. New models for the initiation and growth of tumours have been developed. In particular, cell-based simulations have provided novel insight into the role of stem cells in the colorectal crypt. Chaste is constantly evolving and is now being applied to a far wider range of problems. The code provides modules for handling common scientific computing components, such as meshes and solvers for ordinary and partial differential equations (ODEs/PDEs). Re-use of these components avoids the need for researchers to ‘re-invent the wheel’ with each new project, accelerating the rate of progress in new applications. Chaste is developed using industrially-derived techniques, in particular test-driven development, to ensure code quality, re-use and reliability. In this article we provide examples that illustrate the types of problems Chaste can be used to solve, which can be run on a desktop computer. We highlight some scientific studies that have used or are using Chaste, and the insights they have provided. The source code, both for specific releases and the development version, is available to download under an open source Berkeley Software Distribution (BSD) licence at http://www.cs.ox.ac.uk/chaste, together with details of a mailing list and links to documentation and tutorials.",2013,PLoS Comput. Biol.
Support Vector Machines and Kernels for Computational Biology,"The increasing wealth of biological data coming from a large variety of platforms and the continued development of new high-throughput methods for probing biological systems require increasingly more sophisticated computational approaches. Putting all these data in simple-to-use databases is a first step; but realizing the full potential of the data requires algorithms that automatically extract regularities from the data, which can then lead to biological insight. 
 
Many of the problems in computational biology are in the form of prediction: starting from prediction of a gene's structure, prediction of its function, interactions, and role in disease. Support vector machines (SVMs) and related kernel methods are extremely good at solving such problems [1]–[3]. SVMs are widely used in computational biology due to their high accuracy, their ability to deal with high-dimensional and large datasets, and their flexibility in modeling diverse sources of data [2], [4]–[6]. 
 
The simplest form of a prediction problem is binary classification: trying to discriminate between objects that belong to one of two categories—positive (+1) or negative (−1). SVMs use two key concepts to solve this problem: large margin separation and kernel functions. The idea of large margin separation can be motivated by classification of points in two dimensions (see Figure 1). A simple way to classify the points is to draw a straight line and call points lying on one side positive and on the other side negative. If the two sets are well separated, one would intuitively draw the separating line such that it is as far as possible away from the points in both sets (see Figures 2 and ​and3).3). This intuitive choice captures the idea of large margin separation, which is mathematically formulated in the section Classification with Large Margin. 
 
 
 
 
 
 
Open in a separate window 
 
 
Figure 1 
 
A linear classifier separating two classes of points (squares and circles) in two dimensions. 
The decision boundary divides the space into two sets depending on the sign of f(x) = 〈w,x〉+b. The grayscale level represents the value of the discriminant function f(x): dark for low values and a light shade for high values.",2008,PLoS Comput. Biol.
A First Attempt to Bring Computational Biology into Advanced High School Biology Classrooms,"Computer science has become ubiquitous in many areas of biological research, yet most high school and even college students are unaware of this. As a result, many college biology majors graduate without adequate computational skills for contemporary fields of biology. The absence of a computational element in secondary school biology classrooms is of growing concern to the computational biology community and biology teachers who would like to acquaint their students with updated approaches in the discipline. We present a first attempt to correct this absence by introducing a computational biology element to teach genetic evolution into advanced biology classes in two local high schools. Our primary goal was to show students how computation is used in biology and why a basic understanding of computation is necessary for research in many fields of biology. This curriculum is intended to be taught by a computational biologist who has worked with a high school advanced biology teacher to adapt the unit for his/her classroom, but a motivated high school teacher comfortable with mathematics and computing may be able to teach this alone. In this paper, we present our curriculum, which takes into consideration the constraints of the required curriculum, and discuss our experiences teaching it. We describe the successes and challenges we encountered while bringing this unit to high school students, discuss how we addressed these challenges, and make suggestions for future versions of this curriculum.We believe that our curriculum can be a valuable seed for further development of computational activities aimed at high school biology students. Further, our experiences may be of value to others teaching computational biology at this level. Our curriculum can be obtained at http://ecsite.cs.colorado.edu/?page_id=149#biology or by contacting the authors.",2011,PLoS Comput. Biol.
Parameter Estimation and Model Selection in Computational Biology,"A central challenge in computational modeling of biological systems is the determination of the model parameters. Typically, only a fraction of the parameters (such as kinetic rate constants) are experimentally measured, while the rest are often fitted. The fitting process is usually based on experimental time course measurements of observables, which are used to assign parameter values that minimize some measure of the error between these measurements and the corresponding model prediction. The measurements, which can come from immunoblotting assays, fluorescent markers, etc., tend to be very noisy and taken at a limited number of time points. In this work we present a new approach to the problem of parameter selection of biological models. We show how one can use a dynamic recursive estimator, known as extended Kalman filter, to arrive at estimates of the model parameters. The proposed method follows. First, we use a variation of the Kalman filter that is particularly well suited to biological applications to obtain a first guess for the unknown parameters. Secondly, we employ an a posteriori identifiability test to check the reliability of the estimates. Finally, we solve an optimization problem to refine the first guess in case it should not be accurate enough. The final estimates are guaranteed to be statistically consistent with the measurements. Furthermore, we show how the same tools can be used to discriminate among alternate models of the same biological process. We demonstrate these ideas by applying our methods to two examples, namely a model of the heat shock response in E. coli, and a model of a synthetic gene regulation system. The methods presented are quite general and may be applied to a wide class of biological systems where noisy measurements are used for parameter estimation or model selection.",2010,PLoS Comput. Biol.
Nonnegative Matrix Factorization: An Analytical and Interpretive Tool in Computational Biology,"In the last decade, advances in high-throughput technologies such as DNA microarrays have made it possible to simultaneously measure the expression levels of tens of thousands of genes and proteins. This has resulted in large amounts of biological data requiring analysis and interpretation. Nonnegative matrix factorization (NMF) was introduced as an unsupervised, parts-based learning paradigm involving the decomposition of a nonnegative matrix V into two nonnegative matrices, W and H, via a multiplicative updates algorithm. In the context of a p×n gene expression matrix V consisting of observations on p genes from n samples, each column of W defines a metagene, and each column of H represents the metagene expression pattern of the corresponding sample. NMF has been primarily applied in an unsupervised setting in image and natural language processing. More recently, it has been successfully utilized in a variety of applications in computational biology. Examples include molecular pattern discovery, class comparison and prediction, cross-platform and cross-species analysis, functional characterization of genes and biomedical informatics. In this paper, we review this method as a data analytical and interpretive tool in computational biology with an emphasis on these applications.",2008,PLoS Comput. Biol.
Computational Predictions Provide Insights into the Biology of TAL Effector Target Sites,"Transcription activator-like (TAL) effectors are injected into host plant cells by Xanthomonas bacteria to function as transcriptional activators for the benefit of the pathogen. The DNA binding domain of TAL effectors is composed of conserved amino acid repeat structures containing repeat-variable diresidues (RVDs) that determine DNA binding specificity. In this paper, we present TALgetter, a new approach for predicting TAL effector target sites based on a statistical model. In contrast to previous approaches, the parameters of TALgetter are estimated from training data computationally. We demonstrate that TALgetter successfully predicts known TAL effector target sites and often yields a greater number of predictions that are consistent with up-regulation in gene expression microarrays than an existing approach, Target Finder of the TALE-NT suite. We study the binding specificities estimated by TALgetter and approve that different RVDs are differently important for transcriptional activation. In subsequent studies, the predictions of TALgetter indicate a previously unreported positional preference of TAL effector target sites relative to the transcription start site. In addition, several TAL effectors are predicted to bind to the TATA-box, which might constitute one general mode of transcriptional activation by TAL effectors. Scrutinizing the predicted target sites of TALgetter, we propose several novel TAL effector virulence targets in rice and sweet orange. TAL-mediated induction of the candidates is supported by gene expression microarrays. Validity of these targets is also supported by functional analogy to known TAL effector targets, by an over-representation of TAL effector targets with similar function, or by a biological function related to pathogen infection. Hence, these predicted TAL effector virulence targets are promising candidates for studying the virulence function of TAL effectors. TALgetter is implemented as part of the open-source Java library Jstacs, and is freely available as a web-application and a command line program.",2013,PLoS Comput. Biol.
A Quick Guide to Organizing Computational Biology Projects,"Most bioinformatics coursework focuses on algorithms, with perhaps some components devoted to learning programming skills and learning how to use existing bioinformatics software. Unfortunately, for students who are preparing for a research career, this type of curriculum fails to address many of the day-to-day organizational challenges associated with performing computational experiments. In practice, the principles behind organizing and documenting computational experiments are often learned on the fly, and this learning is strongly influenced by personal predilections as well as by chance interactions with collaborators or colleagues. 
 
The purpose of this article is to describe one good strategy for carrying out computational experiments. I will not describe profound issues such as how to formulate hypotheses, design experiments, or draw conclusions. Rather, I will focus on relatively mundane issues such as organizing files and directories and documenting progress. These issues are important because poor organizational choices can lead to significantly slower research progress. I do not claim that the strategies I outline here are optimal. These are simply the principles and practices that I have developed over 12 years of bioinformatics research, augmented with various suggestions from other researchers with whom I have discussed these issues.",2009,PLoS Comput. Biol.
A Quick Guide to Teaching R Programming to Computational Biology Students,"The name “R” refers to the computational environment initially created by Robert Gentleman and Robert Ihaka, similar in nature to the “S” statistical environment developed at Bell Laboratories (http://www.r-project.org/about.html) [1]. It has since been developed and maintained by a strong team of core developers (R-core), who are renowned researchers in computational disciplines. R has gained wide acceptance as a reliable and powerful modern computational environment for statistical computing and visualisation, and is now used in many areas of scientific computation. R is free software, released under the GNU General Public License; this means anyone can see all its source code, and there are no restrictive, costly licensing arrangements. One of the main reasons that computational biologists use R is the Bioconductor project (http://www.bioconductor.org), which is a set of packages for R to analyse genomic data. These packages have, in many cases, been provided by researchers to complement descriptions of algorithms in journal articles. Many computational biologists regard R and Bioconductor as fundamental tools for their research. R is a modern, functional programming language that allows for rapid development of ideas, together with object-oriented features for rigorous software development. The rich set of inbuilt functions makes it ideal for high-volume analysis or statistical simulations, and the packaging system means that code provided by others can easily be shared. Finally, it generates high-quality graphical output so that all stages of a study, from modelling/analysis to publication, can be undertaken within R. For detailed discussion of the merits of R in computational biology, see [2].",2009,PLoS Comput. Biol.
The Need for Centralization of Computational Biology Resources,"Biomedical research is benefiting from the wealth of new data generated in the laboratory through new instrumentation, greater computational resources, and massive repositories of public domain data. Using these data to make scientific discoveries is sometimes straightforward, but can be complicated by the number and breadth of public sources available to the researcher as well as by the plethora of tools from which to choose. Complex searches, analyses, or even storage needs require more computational expertise than that available within an individual laboratory. As biomedical researchers develop more computational skills, this may change over time. Having a centralized group of experts in computational biology can be of great value to the experimental biologist, and, recognizing this, many organizations have invested in building a team of computational biologists, bioinformaticists, and research IT services to address the needs of the investigators. This Editorial presents our views on the benefits and challenges of centralizing these activities. 
 
In order to benefit from expertise among existing teams of experts around the world, the “Bioinfo-Core” group was formed during the ISMB 2002 meeting in Edmonton, Canada, with approximately 25 initial members. Since then, the group has expanded in both organization and interest. Our worldwide membership now includes more than 150 people who administer centralized bioinformatics and research computing facilities within diverse organizations, including academia, independent research institutes, academic medical centers, and industry. Additionally, the group holds quarterly meetings via teleconference, continues an annual face-to-face meeting at ISMB (averaging 40–60 people), and hosts a mailing list and Wiki (http://www.bioinfo-core.org) to further communication.",2009,PLoS Comput. Biol.
Computational Biology in Colombia,"High-throughput techniques are somewhat restricted in developing countries. However, computational resources have evolved in recent years to become available to the general public, with greater ability to solve intense computational problems at low cost. Therefore, the vast amount of information that is currently being generated and the need for finding the underpinnings of several issues in biology, have been the impetus of the computational biology area in Latin America. Colombia is no exception, as its rich genetic diversity has convened the attention of several institutions, including both governmental and academic departments, to find how, where, and when these resources could be employed to its benefit. In this review, we introduce the efforts being made throughout the country to spread the word and establish a strong network from a mid- and long-term perspective.",2009,PLoS Comput. Biol.
Computational Biology Resources Lack Persistence and Usability,"Innovation in computational biology research is predicated on the availability of published methods and computational resources. These resources facilitate the generation of new hypotheses and observations both on the part of the creators and the scientists who use them. These methods and resources include Web servers, databases, and software, both complex and simple, that implement a specific procedure or algorithm. Usually, a resource is maintained by the laboratory in which it was initially developed. We would assert that there is a growing level of frustration among scientists who attempt to use many of these resources and find that they no longer exist or are not properly maintained. Whether you agree or disagree with this statement and the evidence that follows, we welcome your thoughts and invite you to add a Comment to this article to share your own experiences and perspectives. 
 
It is timely to visit this situation in more detail. The International Society for Computational Biology (ISCB) is reviewing its position on software sharing, and this journal is now doing the same (the views expressed here are not necessarily those of the journal—this is a personal perspective and not an editorial). To help us gain a better understanding of the resource situation, we took on two simple experiments: first, a review of the persistence of Web servers, and, second, an experience creating a metaserver—a Web site where users can come and run a variety of methods to compare results. Here is what we found.",2008,PLoS Comput. Biol.
Structural Identifiability of Dynamic Systems Biology Models,"A powerful way of gaining insight into biological systems is by creating a nonlinear differential equation model, which usually contains many unknown parameters. Such a model is called structurally identifiable if it is possible to determine the values of its parameters from measurements of the model outputs. Structural identifiability is a prerequisite for parameter estimation, and should be assessed before exploiting a model. However, this analysis is seldom performed due to the high computational cost involved in the necessary symbolic calculations, which quickly becomes prohibitive as the problem size increases. In this paper we show how to analyse the structural identifiability of a very general class of nonlinear models by extending methods originally developed for studying observability. We present results about models whose identifiability had not been previously determined, report unidentifiabilities that had not been found before, and show how to modify those unidentifiable models to make them identifiable. This method helps prevent problems caused by lack of identifiability analysis, which can compromise the success of tasks such as experiment design, parameter estimation, and model-based optimization. The procedure is called STRIKE-GOLDD (STRuctural Identifiability taKen as Extended-Generalized Observability with Lie Derivatives and Decomposition), and it is implemented in a MATLAB toolbox which is available as open source software. The broad applicability of this approach facilitates the analysis of the increasingly complex models used in systems biology and other areas.",2016,PLoS Comput. Biol.
A Quick Guide for Computer-Assisted Instruction in Computational Biology and Bioinformatics,"Computational Biology and Bioinformatics (CBB) are indispensable components in the training of life scientists [1]–[3]. Current curricula in the life sciences should prepare graduates who master quantitative and computer skills for increased levels of performance [4]–[6]. Equally important is that the application of the curricula is driven by an appropriate instructional paradigm and effective learning experiences. Teaching and learning with computers bring specific issues that should be considered beforehand by any instructor. The following Quick Guide for Computer-Assisted Instruction (CAI) outlines ten principles for effective teaching. The principles are aligned with current developments on human cognition and learning [7] and have been drawn from our own experience using CAI in seminars, tutorials, and distance education, in courses on Molecular Life Sciences at the undergraduate level, taught to majors in biology or in other subjects (e.g., nutrition, teaching of physics and chemistry, teaching of biology, sports). 
 
The Guide refers to the preparation, presentation, and assessment of CAI. It should be an aid for those who teach CBB with CAI in class, and it is expected to stimulate student motivation and deeper learning in CBB, thus making class time more effective and improving satisfaction of both students and instructors.",2008,PLoS Comput. Biol.
Computational Biology in Costa Rica: The Role of a Small Country in the Global Context of Bioinformatics,"The successful development of high throughput methods for DNA sequencing, transcriptomics, proteomics, and other –omics, has contributed to the emergence of novel possibilities for the examination of complex biological systems through computational analysis. These fields have witnessed unprecedented advances in high income countries. Nevertheless, the role of other nations needs to be examined in order to delineate their contribution within the global context of bioinformatics. Previous articles have focused on the expansion of Computational Biology in Brazil and Mexico [1],[2], two of the largest Latin American countries, and which have shown political commitment to foster their scientific development. Costa Rica is a small Central American country with a population of 4 million, with its territory 164 and 38 times smaller than Brazil and Mexico, respectively. Thus, it is interesting to visualize the possibilities and challenges of this low-income country in the context of the global bioinformatics endeavor. (For author information, see Box 1.) 
 
 
Box 1. Author Biographies 
Edgardo Moreno (EM), Bruno Lomone (BL), and Jose-Maria Gutierrez (JMG) received their B.Sc. degrees in Microbiology and Clinical Chemistry at the University of Costa Rica. They obtained their Ph.D.s at the University of Wisconsin-Madison (EM), the University of Goteborg, Sweden (BL), and Oklahoma State University (JMG). Besides their formal training, the three of them have obtained fellowships to perform research in laboratories in Brazil, Mexico, France, Germany, and the United States, maintaining strong collaborations with valuable foreign research groups. Upon their return to Costa Rica, they were appointed to the staffs of the School of Veterinary Medicine, National University (EM), and the Clodomiro Picado Institute, School of Microbiology, University of Costa Rica (BL and JMG), where they have been teaching Immunology, Cell Biology, Cell Pathology, and Biochemistry, at both graduate and undergraduate levels. 
 
EM's main research interests have been in the understanding the cross-talk between Brucella intracellular pathogens and their animal host cell and the host adaptation and evolution of this endoparasitic bacterium. BL has investigated the structural and functional characteristics of snake venom toxins, contributing to the identification of the molecular regions responsible for the disruption of muscle cell plasma membrane. His interests also include the immunochemical characterization of snake venom components, modeling, and the proteomic analysis of venoms from Central American snakes. JMG's interests have focused mostly on the experimental pathology of snakebite envenomation and regeneration of tissues after snake venom-induced damage. He has also been devoted to the improvement of the technology for antivenom production at the local and global levels, participating in a WHO-lead initiative to improve antivenom production worldwide. 
 
EM, BL, and JMG have actively collaborated together over many years, and have participated in a number of regional projects promoting long-lasting academic and scientific alliances between research groups and institutions from Central America and Sweden, in what is currently known as the NeTropica network. They have also contributed to the consolidation of local and regional graduate programs based in Costa Rican public universities. In addition to their engagement in university activities, the three of them are members of the National Academy of Sciences of Costa Rica.",2008,PLoS Comput. Biol.
Telling ecological networks apart by their structure: A computational challenge,"Ecologists have been compiling ecological networks for over a century, detailing the interactions between species in a variety of ecosystems. To this end, they have built networks for mutualistic (e.g., pollination, seed dispersal) as well as antagonistic (e.g., herbivory, parasitism) interactions. The type of interaction being represented is believed to be reflected in the structure of the network, which would differ substantially between mutualistic and antagonistic networks. Here, we put this notion to the test by attempting to determine the type of interaction represented in a network based solely on its structure. We find that, although it is easy to separate different kinds of nonecological networks, ecological networks display much structural variation, making it difficult to distinguish between mutualistic and antagonistic interactions. We therefore frame the problem as a challenge for the community of scientists interested in computational biology and machine learning. We discuss the features a good solution to this problem should possess and the obstacles that need to be overcome to achieve this goal.",2019,PLoS Comput. Biol.
A Primer on Learning in Bayesian Networks for Computational Biology,"Bayesian networks (BNs) provide a neat and compact representation for expressing joint probability distributions (JPDs) and for inference. They are becoming increasingly important in the biological sciences for the tasks of inferring cellular networks [1], modelling protein signalling pathways [2], systems biology, data integration [3], classification [4], and genetic data analysis [5]. The representation and use of probability theory makes BNs suitable for combining domain knowledge and data, expressing causal relationships, avoiding overfitting a model to training data, and learning from incomplete datasets. The probabilistic formalism provides a natural treatment for the stochastic nature of biological systems and measurements. This primer aims to introduce BNs to the computational biologist, focusing on the concepts behind methods for learning the parameters and structure of models, at a time when they are becoming the machine learning method of choice. 
 
There are many applications in biology where we wish to classify data; for example, gene function prediction. To solve such problems, a set of rules are required that can be used for prediction, but often such knowledge is unavailable, or in practice there turn out to be many exceptions to the rules or so many rules that this approach produces poor results. 
 
Machine learning approaches often produce better results, where a large number of examples (the training set) is used to adapt the parameters of a model that can then be used for performing predictions or classifications on data. There are many different types of models that may be required and many different approaches to training the models, each with its pros and cons. An excellent overview of the topic can be found in [6] and [7]. Neural networks, for example, are often able to learn a model from training data, but it is often difficult to extract information about the model, which with other methods can provide valuable insights into the data or problem being solved. A common problem in machine learning is overfitting, where the learned model is too complex and generalises poorly to unseen data. Increasing the size of the training dataset may reduce this; however, this assumes more training data is readily available, which is often not the case. In addition, often it is important to determine the uncertainty in the learned model parameters or even in the choice of model. This primer focuses on the use of BNs, which offer a solution to these issues. The use of Bayesian probability theory provides mechanisms for describing uncertainty and for adapting the number of parameters to the size of the data. Using a graphical representation provides a simple way to visualise the structure of a model. Inspection of models can provide valuable insights into the properties of the data and allow new models to be produced.",2007,PLoS Comput. Biol.
Combined Model of Intrinsic and Extrinsic Variability for Computational Network Design with Application to Synthetic Biology,"Biological systems are inherently variable, with their dynamics influenced by intrinsic and extrinsic sources. These systems are often only partially characterized, with large uncertainties about specific sources of extrinsic variability and biochemical properties. Moreover, it is not yet well understood how different sources of variability combine and affect biological systems in concert. To successfully design biomedical therapies or synthetic circuits with robust performance, it is crucial to account for uncertainty and effects of variability. Here we introduce an efficient modeling and simulation framework to study systems that are simultaneously subject to multiple sources of variability, and apply it to make design decisions on small genetic networks that play a role of basic design elements of synthetic circuits. Specifically, the framework was used to explore the effect of transcriptional and post-transcriptional autoregulation on fluctuations in protein expression in simple genetic networks. We found that autoregulation could either suppress or increase the output variability, depending on specific noise sources and network parameters. We showed that transcriptional autoregulation was more successful than post-transcriptional in suppressing variability across a wide range of intrinsic and extrinsic magnitudes and sources. We derived the following design principles to guide the design of circuits that best suppress variability: (i) high protein cooperativity and low miRNA cooperativity, (ii) imperfect complementarity between miRNA and mRNA was preferred to perfect complementarity, and (iii) correlated expression of mRNA and miRNA – for example, on the same transcript – was best for suppression of protein variability. Results further showed that correlations in kinetic parameters between cells affected the ability to suppress variability, and that variability in transient states did not necessarily follow the same principles as variability in the steady state. Our model and findings provide a general framework to guide design principles in synthetic biology.",2013,PLoS Comput. Biol.
From Bytes to Bedside: Data Integration and Computational Biology for Translational Cancer Research,"Major advances in genome science and molecular technologies provide new opportunities at the interface between basic biological research and medical practice. The unprecedented completeness, accuracy, and volume of genomic and molecular data necessitate a new kind of computational biology for translational research. Key challenges are standardization of data capture and communication, organization of easily accessible repositories, and algorithms for integrated analysis based on heterogeneous sources of information. Also required are new ways of using complementary clinical and biological data, such as computational methods for predicting disease phenotype from molecular and genetic profiling. New combined experimental and computational methods hold the promise of more accurate diagnosis and prognosis as well as more effective prevention and therapy.",2007,PLoS Comput. Biol.
Genome-Scale Computational Biology and Bioinformatics in Australia,"Australia enjoys a high international reputation for research in experimental genetics, molecular and cell biology, animal and plant sciences, biotechnology, medicine, biodiversity, and ecological modelling. Computational research is broadly established in these domain areas and others relevant to bioscience. Combined with strong traditions in mathematics and statistics, and national and state investment in computational infrastructure, computational biology Down Under is simply too rich and diverse to be done justice in these few pages. Here we (see Box 1 Authors' Biographies) focus more narrowly, attempting to provide a snapshot of bioinformatic and computational genome-scale biology in Australia circa 2008.",2008,PLoS Comput. Biol.
Computational translation of genomic responses from experimental model systems to humans,"The high failure rate of therapeutics showing promise in mouse models to translate to patients is a pressing challenge in biomedical science. Though retrospective studies have examined the fidelity of mouse models to their respective human conditions, approaches for prospective translation of insights from mouse models to patients remain relatively unexplored. Here, we develop a semi-supervised learning approach for inference of disease-associated human differentially expressed genes and pathways from mouse model experiments. We examined 36 transcriptomic case studies where comparable phenotypes were available for mouse and human inflammatory diseases and assessed multiple computational approaches for inferring human biology from mouse datasets. We found that semi-supervised training of a neural network identified significantly more true human biological associations than interpreting mouse experiments directly. Evaluating the experimental design of mouse experiments where our model was most successful revealed principles of experimental design that may improve translational performance. Our study shows that when prospectively evaluating biological associations in mouse studies, semi-supervised learning approaches, combining mouse and human data for biological inference, provide the most accurate assessment of human in vivo disease processes. Finally, we proffer a delineation of four categories of model system-to-human ""Translation Problems"" defined by the resolution and coverage of the datasets available for molecular insight translation and suggest that the task of translating insights from model systems to human disease contexts may be better accomplished by a combination of translation-minded experimental design and computational approaches.",2019,PLoS Comput. Biol.
Outlook on Thailand's Genomics and Computational Biology Research and Development,"With a wealth of biodiversity, a long tradition of agriculture-based industries, and an established medical and biotechnological research and development community, Thailand has become an attractive location for life sciences investment. The large amount of data generated in many areas of life sciences requires visualization, management, and analysis, principally through bioinformatics. To become successful, Thailand's research community should emphasize establishing core technologies, such as genomics and bioinformatics, to boost development of agriculture, food processing, and biomedical research. The Thai government realized the importance of this field and created a national policy to greatly increase Thailand's participation in bioinformatics and genomics, budgeting for specific development goals in research infrastructure, education, and sustainable human resources. 
 
Thailand has not lagged behind in bioinformatics research activity and recognizes the importance of bioinformatics through increased policy awareness, human resources development, and increased research activity involving genomic-scale data generation and computational analyses. Many applications of genomics and bioinformatics to biomedical research and development in Thailand have progressed substantially during the past few years, leading to successful applications in some specific local areas. However, the applications to other important areas, such as agriculture, are hampered by the limited availability of genomic sequence data and the lack of necessary biochemical/physiological information. With the advent of more and more genomic information in public databases, Thailand's research community is striving to adopt comparative genomics to obtain information of direct relevance to the country's health and industrial needs. This article highlights Thailand's contribution to genomics and bioinformatics in the following areas: (1) policy support from the Thai government, (2) capacity building through infrastructure/education/human resources, and (3) research and development in genomics and computational biology. (See Box 1 for Authors' Biographies). 
 
 
Box 1. Authors' Biographies 
Wannipha Tongsima, M.S., obtained her master's degree in Industrial Microbiology from Chulalongkorn University, Thailand. She was involved in founding the Bioinformatics research program in BIOTEC. To reinforce the research activity in this area, she also helped organize the first International Conference on Computational Biology (InCoB), held in Bangkok in 2002. Later, she was appointed to manage one of the first BIOTEC ethnic-specific human genetic variation programs, named the Thailand SNP Discovery Project. She works as a Genomic Medicine program coordinator for the Cluster and Program Management Office (CPMO) of the National Science and Technology Development Agency (NSTDA), which is an umbrella organization of four other national research centers in Thailand, including BIOTEC. 
 
Sissades Tongsima, Ph.D., received his doctoral degree in Computer Science and Engineering from the University of Notre Dame, Indiana, United States. He has worked for the National Electronics and Computer Technology Center on High Performance Computing (HPC) and Computational Grid. During 2002–2004, he cochaired the Asia-Pacific Advanced Network (APAN) Grid Working Group. In 2003, he shifted his research direction from HPC architecture to bioinformatics research, when he started working for BIOTEC, and constructed the ThaiSNP database. His main research interest is in developing algorithms and databases for analyzing various research projects on human genetic variation. He currently heads the Genome Institute biostatistics and informatics laboratory at BIOTEC. 
 
Prasit Palittapongarnpim, M.D., earned his medical degree from Mahidol University, Thailand, and his B.S. in Mathematics from Ramkumhang University, Thailand. He is a Fellow of the Royal College of Pediatricians of Thailand and also an Associate Professor in Microbiology at Mahidol University, where he has conducted research focusing on tuberculosis. While holding a Deputy Director position, he initiated the Bioinformatics research program at BIOTEC in 2002 and led the organization of the first InCoB conference in 2002. He is currently a Vice President of NSTDA.",2008,PLoS Comput. Biol.
Computational Biology in Cuba: An Opportunity to Promote Science in a Developing Country,"Computational biology can be considered a supradisciplinary field of knowledge that merges biology, chemistry, physics, and computer science into a broad-based science that is important to furthering our understanding of the life sciences. Although a relatively new area of research, it is recognized as a crucial field for scientific advancement in developing countries. This Perspective introduces our vision of the role of computational biology in biomedical research and teaching in Cuba. Except where individuals are directly quoted, any opinions expressed herein should be considered those of the authors. (For author information, see Box 1.) The challenge for Cuba was to initiate this new field of research and development without existing expertise. In addition, we had to face the same problems experienced elsewhere in this field—most life scientists were not familiar with the potential of information processing tools, and conversely, computer scientists were unfamiliar with problems facing the life sciences, often brought about by large amounts of new data. Cuba also faced unique problems. As a result of the United States trade embargo, Cuban scientists buy most of their research supplies from Europe. This adds delays and can triple costs, especially for equipment or spare parts that are made only in the US and must be purchased from a third party. In addition, curbs on travel between the US and Cuba isolate Cuban scientists from colleagues and conferences in the United States. Finally, Internet access is also affected by this policy, and Cuban scientists lack fast and efficient access to scientific information from overseas. Despite these drawbacks, Cuban biotech products and activities from biomedical institutes within Cuba have been recognized by other authors [1–4]. Further offsetting these drawbacks, our country has a high educational level, and Cuban universities produce hundreds of new scientists every year. The challenge then is to implement higher education using both the coherence of existing disciplinary education and the means to break down disciplinary walls for students interested in computational biology and other multidisciplinary fields. Since Cuba has scant resources for significant investments in scientific instrumentation, a science such as computational biology that relies mostly on computers, network connectivity, and human resources offers an excellent opportunity. The current challenges facing computational biology research in Cuba are: (1) getting results of direct interest and impact to motivate funding, (2) fostering young leading scientists to sustain present and future research, and (3) using limited infrastructure in the most efficient and productive way. Computational sciences in Cuba were first applied to physics and chemistry in the late 1960s at the University of Havana, with the use of quantum mechanics to model sugar cane derivative molecules [5]. Such an early application of this technology, which was entirely absent in the majority of the world, indicated a new approach to scientific development in a Latin American country. Another important effort began in the early 1970s at the National Center for Scientific Research in the field of computational neurosciences, mostly with Cuban-designed and Cubanmanufactured hardware [6]. Since then, after a heavy effort to build up human scientific potential, mostly with the influence and support of Eastern European countries, the problem of Cuba’s limited computing potential was overcome and Cuban scientists gained valuable experience with microcomputers in the 1980s. Limitations to high-tech development during the 1970s originated from the obsolescence of devices built in the former Soviet bloc and a strong dependence on components from the same market, which hindered Cuba’s national hardware production. Cuba was well-positioned to take advantage of the benefits of the microcomputer revolution of the 1980s. Applications of these new devices to the physical and biological sciences were frequent in that time and several computer programs were written or adapted to allow such applications [7,8]. During the economic crisis of the 1990s, these developments continued in the established groups and appeared in new teams at the Cuban Center for Genetic Engineering and Biotechnology (CIGB) [9].",2007,PLoS Comput. Biol.
Universally Sloppy Parameter Sensitivities in Systems Biology Models,"Quantitative computational models play an increasingly important role in modern biology. Such models typically involve many free parameters, and assigning their values is often a substantial obstacle to model development. Directly measuring in vivo biochemical parameters is difficult, and collectively fitting them to other experimental data often yields large parameter uncertainties. Nevertheless, in earlier work we showed in a growth-factor-signaling model that collective fitting could yield well-constrained predictions, even when it left individual parameters very poorly constrained. We also showed that the model had a “sloppy” spectrum of parameter sensitivities, with eigenvalues roughly evenly distributed over many decades. Here we use a collection of models from the literature to test whether such sloppy spectra are common in systems biology. Strikingly, we find that every model we examine has a sloppy spectrum of sensitivities. We also test several consequences of this sloppiness for building predictive models. In particular, sloppiness suggests that collective fits to even large amounts of ideal time-series data will often leave many parameters poorly constrained. Tests over our model collection are consistent with this suggestion. This difficulty with collective fits may seem to argue for direct parameter measurements, but sloppiness also implies that such measurements must be formidably precise and complete to usefully constrain many model predictions. We confirm this implication in our growth-factor-signaling model. Our results suggest that sloppy sensitivity spectra are universal in systems biology models. The prevalence of sloppiness highlights the power of collective fits and suggests that modelers should focus on predictions rather than on parameters.",2007,PLoS Comput. Biol.
Computational Biology in Argentina,"Computational biology is an interdisciplinary science bred from fields as disparate as mathematics, chemistry, statistics, physics, biology, and computer science. Although the exact definition of computational biology is far from being precise and unambiguous, it is a fact that hundreds of scientists around the world have been increasingly using skills from the above-mentioned fields to approach different biological questions. It is not our aim to elucidate here the definition of computational biology and its differences from related fields such as bioinformatics. However, to review the state of this discipline in Argentina, we need at least a working definition. In this sense, any interdisciplinary research in which the main interest is in studying biological problems and where the working hypothesis can be tested by means of simulation and computational modeling will be considered as belonging to the computational biology field or at least as employing a computational biology approach. 
 
It is interesting to note that computational biology research can be implemented in two different ways depending on how the “interdisciplinary” nature of this field is assembled. On one hand, a scientist with formal training in a given field (for example, a molecular biologist) could acquire other skills (such as programming or mathematical training) in his attempt to answer a given biological question. Alternatively, working teams made up of members specializing in different fields may work together to reach a scientific explanation of a problem. We think that the first approach is more common among the scientific community because it depends entirely on the scientist's desire to discover an explanation for their problems and on their capacity to explore different areas of science. The second approach is generally dependent on the availability of research and development programs, funded by public or private resources, that favor collaborations between different research institutions to form multidisciplinary teams. We will see that the first approach is more common in Argentina. 
 
This paper summarizes the state of the art of computational biology in Argentina. Our aim is to offer as broad a view as possible of the different groups of scientists and their main research interests. Also, we present a brief review of educational, research, and development policies related to the field. We hope that this review may encourage overseas researchers to contact and collaborate with Argentinean teams, as well as organizing and facilitating the exchange of information between researchers in our country. However, this review will probably be far from complete, due to the lack of centralized information on computational biology, and for this reason we apologize for any potential omissions. (For author information, see Box 1.) 
 
 
Box 1. Authors' Biographies 
 
Gustavo Parisi, Ph.D., is the Structural Bioinformatics Group leader and professor at the Universidad Nacional de Quilmes, Buenos Aires, Argentina. He is also a researcher at the Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET). For his Ph.D. thesis, he developed a model of protein molecular evolution which takes into account protein structure to simulate sequence divergence. His principal research interest is focused on the introduction of evolutionary information in the development of bioinformatics tools to study protein structure. 
 
Virginia Gonzalez, a graduate student in biotechnology, is part of the Structural Bioinformatics Group at the Universidad Nacional de Quilmes, Buenos Aires, Argentina. She is a member of the DNALinux developer team and is currently working on her doctorate in Sequential, Structural, and Dynamic Characterization of Allergenic Proteins. She is also part of the Latin America Solanaceae Genome Project (LAT-SOL) working on the sequence annotation of the tomato mitochondrial genome. 
 
Sebastian Bassi worked for five years as a genome database curator at Advanta Seeds, a plant biotechnology company. He was in charge of the molecular marker database for all crops in the enterprise. He is the leader of the DNALinux bioinformatics distribution and has developed several Web-based utilities for bioinformatics analysis. He is part of the developer team for Biopython (http://www.biopython.org/), and is currently writing a book on Python for Bioinformatics and doing bioinformatics support for the tomato mitochondrial genome sequencing project, which is part of the Latin America Solanaceae Genome Project (LAT-SOL).",2007,PLoS Comput. Biol.
ISMB/ECCB 2007: The Premier Conference on Computational Biology,"The International Society for Computational Biology (ISCB) presents ISMB/ECCB 2007, the Fifteenth International Conference on Intelligent Systems for Molecular Biology (ISMB 2007), held jointly with the Sixth European Conference on Computational Biology (ECCB 2007) in Vienna, Austria, July 21–25, 2007 (http://www.iscb.org/ismbeccb2007). Now in the final phases of selecting papers, presentations, demonstrations, and posters, the organizers are preparing what will likely be recognized as the premier conference on computational biology in 2007. ISMB/ECCB 2007 has expanded in ways to specifically encourage increased participation from previously underrepresented disciplines of computational biology. This conference will feature the best of the computer and life sciences through a variety of new and core sessions running in multiple parallel tracks, along with an increase in keynote presentations, posters on display throughout the duration of the conference, and an extensive industry exhibition. Special interest group meetings, a satellite meeting, and tutorials all will precede the main conference dates.",2007,PLoS Comput. Biol.
A Computational Framework for 3D Mechanical Modeling of Plant Morphogenesis with Cellular Resolution,"The link between genetic regulation and the definition of form and size during morphogenesis remains largely an open question in both plant and animal biology. This is partially due to the complexity of the process, involving extensive molecular networks, multiple feedbacks between different scales of organization and physical forces operating at multiple levels. Here we present a conceptual and modeling framework aimed at generating an integrated understanding of morphogenesis in plants. This framework is based on the biophysical properties of plant cells, which are under high internal turgor pressure, and are prevented from bursting because of the presence of a rigid cell wall. To control cell growth, the underlying molecular networks must interfere locally with the elastic and/or plastic extensibility of this cell wall. We present a model in the form of a three dimensional (3D) virtual tissue, where growth depends on the local modulation of wall mechanical properties and turgor pressure. The model shows how forces generated by turgor-pressure can act both cell autonomously and non-cell autonomously to drive growth in different directions. We use simulations to explore lateral organ formation at the shoot apical meristem. Although different scenarios lead to similar shape changes, they are not equivalent and lead to different, testable predictions regarding the mechanical and geometrical properties of the growing lateral organs. Using flower development as an example, we further show how a limited number of gene activities can explain the complex shape changes that accompany organ outgrowth.",2015,PLoS Comput. Biol.
Script of Scripts: A pragmatic workflow system for daily computational research,"Computationally intensive disciplines such as computational biology often require use of a variety of tools implemented in different scripting languages and analysis of large data sets using high-performance computing systems. Although scientific workflow systems can powerfully organize and execute large-scale data-analysis processes, creating and maintaining such workflows usually comes with nontrivial learning curves and engineering overhead, making them cumbersome to use for everyday data exploration and prototyping. To bridge the gap between interactive analysis and workflow systems, we developed Script of Scripts (SoS), an interactive data-analysis platform and workflow system with a strong emphasis on readability, practicality, and reproducibility in daily computational research. For exploratory analysis, SoS has a multilanguage scripting format that centralizes otherwise-scattered scripts and creates dynamic reports for publication and sharing. As a workflow engine, SoS provides an intuitive syntax for creating workflows in process-oriented, outcome-oriented, and mixed styles, as well as a unified interface for executing and managing tasks on a variety of computing platforms with automatic synchronization of files among isolated file systems. As illustrated herein by real-world examples, SoS is both an interactive analysis tool and pipeline platform suitable for different stages of method development and data-analysis projects. In particular, SoS can be easily adopted in existing data analysis routines to substantially improve organization, readability, and cross-platform computation management of research projects.",2019,PLoS Comput. Biol.
Ten Simple Rules for a Computational Biologist’s Laboratory Notebook,"One of the major hurdles I face as the head of a computational biology laboratory is convincing my research team—particularly those pursuing exclusively mathematical and computational modeling—that they need to keep a laboratory notebook. There seems to be a misconception in the computational biology community that a lab notebook is only useful for recording experimental protocols and their results. A lab notebook is much more than that. It is an organizational tool and memory aid, which serves as the primary record of scientific research and activity for all scientists. It also serves as a legal record of ownership of the ideas and results obtained by a scientist. Here, I present the best practices (summarized as ten rules) for keeping a lab notebook in computational biology, for scientists pursuing exclusively “dry” research.",2015,PLoS Comput. Biol.
Computational Modelling of Metastasis Development in Renal Cell Carcinoma,"The biology of the metastatic colonization process remains a poorly understood phenomenon. To improve our knowledge of its dynamics, we conducted a modelling study based on multi-modal data from an orthotopic murine experimental system of metastatic renal cell carcinoma. The standard theory of metastatic colonization usually assumes that secondary tumours, once established at a distant site, grow independently from each other and from the primary tumour. Using a mathematical model that translates this assumption into equations, we challenged this theory against our data that included: 1) dynamics of primary tumour cells in the kidney and metastatic cells in the lungs, retrieved by green fluorescent protein tracking, and 2) magnetic resonance images (MRI) informing on the number and size of macroscopic lesions. Critically, when calibrated on the growth of the primary tumour and total metastatic burden, the predicted theoretical size distributions were not in agreement with the MRI observations. Moreover, tumour expansion only based on proliferation was not able to explain the volume increase of the metastatic lesions. These findings strongly suggested rejection of the standard theory, demonstrating that the time development of the size distribution of metastases could not be explained by independent growth of metastatic foci. This led us to investigate the effect of spatial interactions between merging metastatic tumours on the dynamics of the global metastatic burden. We derived a mathematical model of spatial tumour growth, confronted it with experimental data of single metastatic tumour growth, and used it to provide insights on the dynamics of multiple tumours growing in close vicinity. Together, our results have implications for theories of the metastatic process and suggest that global dynamics of metastasis development is dependent on spatial interactions between metastatic lesions.",2015,PLoS Comput. Biol.
Why Are Computational Neuroscience and Systems Biology So Separate?,"Despite similar computational approaches, there is surprisingly little interaction between the computational neuroscience and the systems biology research communities. In this review I reconstruct the history of the two disciplines and show that this may explain why they grew up apart. The separation is a pity, as both fields can learn quite a bit from each other. Several examples are given, covering sociological, software technical, and methodological aspects. Systems biology is a better organized community which is very effective at sharing resources, while computational neuroscience has more experience in multiscale modeling and the analysis of information processing by biological systems. Finally, I speculate about how the relationship between the two fields may evolve in the near future.",2008,PLoS Comput. Biol.
Unbiased Rare Event Sampling in Spatial Stochastic Systems Biology Models Using a Weighted Ensemble of Trajectories,"The long-term goal of connecting scales in biological simulation can be facilitated by scale-agnostic methods. We demonstrate that the weighted ensemble (WE) strategy, initially developed for molecular simulations, applies effectively to spatially resolved cell-scale simulations. The WE approach runs an ensemble of parallel trajectories with assigned weights and uses a statistical resampling strategy of replicating and pruning trajectories to focus computational effort on difficult-to-sample regions. The method can also generate unbiased estimates of non-equilibrium and equilibrium observables, sometimes with significantly less aggregate computing time than would be possible using standard parallelization. Here, we use WE to orchestrate particle-based kinetic Monte Carlo simulations, which include spatial geometry (e.g., of organelles, plasma membrane) and biochemical interactions among mobile molecular species. We study a series of models exhibiting spatial, temporal and biochemical complexity and show that although WE has important limitations, it can achieve performance significantly exceeding standard parallel simulation—by orders of magnitude for some observables.",2016,PLoS Comput. Biol.
iRegulon: From a Gene List to a Gene Regulatory Network Using Large Motif and Track Collections,"Identifying master regulators of biological processes and mapping their downstream gene networks are key challenges in systems biology. We developed a computational method, called iRegulon, to reverse-engineer the transcriptional regulatory network underlying a co-expressed gene set using cis-regulatory sequence analysis. iRegulon implements a genome-wide ranking-and-recovery approach to detect enriched transcription factor motifs and their optimal sets of direct targets. We increase the accuracy of network inference by using very large motif collections of up to ten thousand position weight matrices collected from various species, and linking these to candidate human TFs via a motif2TF procedure. We validate iRegulon on gene sets derived from ENCODE ChIP-seq data with increasing levels of noise, and we compare iRegulon with existing motif discovery methods. Next, we use iRegulon on more challenging types of gene lists, including microRNA target sets, protein-protein interaction networks, and genetic perturbation data. In particular, we over-activate p53 in breast cancer cells, followed by RNA-seq and ChIP-seq, and could identify an extensive up-regulated network controlled directly by p53. Similarly we map a repressive network with no indication of direct p53 regulation but rather an indirect effect via E2F and NFY. Finally, we generalize our computational framework to include regulatory tracks such as ChIP-seq data and show how motif and track discovery can be combined to map functional regulatory interactions among co-expressed genes. iRegulon is available as a Cytoscape plugin from http://iregulon.aertslab.org.",2014,PLoS Comput. Biol.
A cyber-linked undergraduate research experience in computational biomolecular structure prediction and design,"Computational biology is an interdisciplinary field, and many computational biology research projects involve distributed teams of scientists. To accomplish their work, these teams must overcome both disciplinary and geographic barriers. Introducing new training paradigms is one way to facilitate research progress in computational biology. Here, we describe a new undergraduate program in biomolecular structure prediction and design in which students conduct research at labs located at geographically-distributed institutions while remaining connected through an online community. This 10-week summer program begins with one week of training on computational biology methods development, transitions to eight weeks of research, and culminates in one week at the Rosetta annual conference. To date, two cohorts of students have participated, tackling research topics including vaccine design, enzyme design, protein-based materials, glycoprotein modeling, crowd-sourced science, RNA processing, hydrogen bond networks, and amyloid formation. Students in the program report outcomes comparable to students who participate in similar in-person programs. These outcomes include the development of a sense of community and increases in their scientific self-efficacy, scientific identity, and science values, all predictors of continuing in a science research career. Furthermore, the program attracted students from diverse backgrounds, which demonstrates the potential of this approach to broaden the participation of young scientists from backgrounds traditionally underrepresented in computational biology.",2017,PLoS Comput. Biol.
Why Are Computational Neuroscience and Systems Biology So Separate?,"Despite similar computational approaches, there is surprisingly little interaction between the computational neuroscience and the systems biology research communities. In this review I reconstruct the history of the two disciplines and show that this may explain why they grew up apart. The separation is a pity, as both fields can learn quite a bit from each other. Several examples are given, covering sociological, software technical, and methodological aspects. Systems biology is a better organized community which is very effective at sharing resources, while computational neuroscience has more experience in multiscale modeling and the analysis of information processing by biological systems. Finally, I speculate about how the relationship between the two fields may evolve in the near future.",2008,PLoS Computational Biology
Ten Simple Rules for Effective Computational Research,"In order to attempt to understand the complexity inherent in nature, mathematical, statistical and computational techniques are increasingly being employed in the life sciences. In particular, the use and development of software tools is becoming vital for investigating scientific hypotheses, and a wide range of scientists are finding software development playing a more central role in their day-to-day research. In fields such as biology and ecology, there has been a noticeable trend towards the use of quantitative methods for both making sense of ever-increasing amounts of data [1] and building or selecting models [2]. 
 
As Research Fellows of the “2020 Science” project (http://www.2020science.net), funded jointly by the EPSRC (Engineering and Physical Sciences Research Council) and Microsoft Research, we have firsthand experience of the challenges associated with carrying out multidisciplinary computation-based science [3]–[5]. In this paper we offer a jargon-free guide to best practice when developing and using software for scientific research. While many guides to software development exist, they are often aimed at computer scientists [6] or concentrate on large open-source projects [7]; the present guide is aimed specifically at the vast majority of scientific researchers: those without formal training in computer science. We present our ten simple rules with the aim of enabling scientists to be more effective in undertaking research and therefore maximise the impact of this research within the scientific community. While these rules are described individually, collectively they form a single vision for how to approach the practical side of computational science. 
 
Our rules are presented in roughly the chronological order in which they should be undertaken, beginning with things that, as a computational scientist, you should do before you even think about writing any code. For each rule, guides on getting started, links to relevant tutorials, and further reading are provided in the supplementary material (Text S1).",2014,PLoS Comput. Biol.
Geometric Interpretation of Gene Coexpression Network Analysis,"The merging of network theory and microarray data analysis techniques has spawned a new field: gene coexpression network analysis. While network methods are increasingly used in biology, the network vocabulary of computational biologists tends to be far more limited than that of, say, social network theorists. Here we review and propose several potentially useful network concepts. We take advantage of the relationship between network theory and the field of microarray data analysis to clarify the meaning of and the relationship among network concepts in gene coexpression networks. Network theory offers a wealth of intuitive concepts for describing the pairwise relationships among genes, which are depicted in cluster trees and heat maps. Conversely, microarray data analysis techniques (singular value decomposition, tests of differential expression) can also be used to address difficult problems in network theory. We describe conditions when a close relationship exists between network analysis and microarray data analysis techniques, and provide a rough dictionary for translating between the two fields. Using the angular interpretation of correlations, we provide a geometric interpretation of network theoretic concepts and derive unexpected relationships among them. We use the singular value decomposition of module expression data to characterize approximately factorizable gene coexpression networks, i.e., adjacency matrices that factor into node specific contributions. High and low level views of coexpression networks allow us to study the relationships among modules and among module genes, respectively. We characterize coexpression networks where hub genes are significant with respect to a microarray sample trait and show that the network concept of intramodular connectivity can be interpreted as a fuzzy measure of module membership. We illustrate our results using human, mouse, and yeast microarray gene expression data. The unification of coexpression network methods with traditional data mining methods can inform the application and development of systems biologic methods.",2008,PLoS Comput. Biol.
Computational Methods for Exploration and Analysis of Macromolecular Structure and Dynamics,"All processes that maintain and replicate a living cell involve fluctuating biological macromolecules. As computational biologists, our aim is to discern the behavior of macromolecules in a way that experimental biology is not able to achieve. No single technique—experimental or computational—can capture all the relevant scales of cellular functional behavior. In principle, computations are the tools that can integrate different kinds of experimental and computational characterizations at different resolutions to obtain a more complete description of the processes of life. Computer simulations can act as a bridge between the microscopic length and time scales, and the macroscopic world of the laboratory. They can start from a macroscopic experiment-based guess of interactions between molecules, and obtain “exact” predictions of bulk and detailed properties subject to limitations. They are able to test a theory by constructing and simulating the model, and comparing the results with experimental measurements; and they are able to provide models that experiments can test. Computations can provide leads by processing large sets of data, predicting molecular behaviors, and supplying the mechanistic underpinning that experiments alone may not be able to achieve. 
 
Macromolecules play a vital role in countless biological processes, including DNA replication, transcription, genome reorganization in development and in disease, protein synthesis, protein folding, and active transport with molecular motors. Cell signaling, a multistep pathway on length scales from nanometers to micrometers, provides another inclusive example, incorporating all of the above over time and space. Signals are relayed from the extracellular space to the nucleus through dynamic shifts of molecular ensembles. Macromolecular fluctuations underlie signal amplification; they result in a large number of activated molecules across the cell, creating multiple reactions and producing a major cellular response. Changes in fluctuations through binding second messengers can regulate catalysis, and dynamic shifts in conformational ensembles can also take place through binding to membrane lipids. Key hub proteins that govern cell behaviors are often membrane-anchored. Helped by experimental data, computations can model the components of the systems and their transient interactions to provide a useful, integrated view of the flow of information, its regulation, and its deregulation. 
 
Ultimately, we want to make direct quantitative comparisons with experimental data. We would like to reduce the amount of fitting and guesswork; but at the same time we may also be interested in phenomena of a generic nature, or in discriminating between good and bad theories. Doing this well is challenging. To understand the dynamic interplay across multiple scales, to link it to the atomic-scale physicochemical basis of the conformational behavior of single molecules and their interactions, and, ultimately, to relate it to cellular function, we need efficient and reliable methods to sample the macromolecular fluctuations and identify the biological and disease-related states and their transitions. Inspiration may come from a combination of biology and other fields that model dynamic systems. 
 
Macromolecules move, and their movements are needed for a complete picture of life. Computational biology, with concepts imported from physics and chemistry, increasingly plays a major role, which has recently been recognized by the Nobel Committee [1]. The energy landscape underscores the inherent nature of biomolecules, which are dynamical objects that are always interconverting between structures with varying energies. It affirms that biomolecules must be described statistically, not statically. Macromolecules are not static objects; rather, they populate ensembles of conformations. The transitions between these states occur on length scales from tenths of an Angstrom to nanometers, and time scales that can vary from nanoseconds to seconds. These are linked to functionally relevant phenomena such as allosteric signaling and enzyme catalysis. 
 
Computational methods also include those for molecular modeling and refinement of three-dimensional structures, de novo design of proteins, prediction and modeling of protein-ligand interactions and development of docking protocols, and prediction of macromolecular interactions at varying spatial resolutions and timescales. They further encompass methods of ligand screening in drug development and protein-protein docking, methods for assessing sequence-structure-function relationships and prediction of macromolecular function, protocols for molecular visualization and annotation, and geometric and topological characterization of proteins and polynucleotides. This list is still far from complete. 
 
To celebrate its tenth anniversary [2], PLOS Computational Biology presents a special collection of manuscripts focusing on methods exploring macromolecular structure and dynamics. This collection does not aim to cover all methods; it does, however, aim to provide a taste of currently available approaches and strategies toward these aims. Altogether, the collection covers a broad ground: from sampling, detection of rare events, and exposing hidden alternative backbone conformations in X-ray crystallography, to multi-scale visualization of molecular architecture using real-time ambient occlusion; from discrimination between obligatory and non-obligatory protein-protein interactions based on the dynamics of the complex to binding free energies of inhibitors, to predicting the effect of mutations on protein-protein interactions by exploiting interface profiles; from a virtual mixture approach to the study of multistate equilibrium, to identification of misfolded intermediates; from multiscale estimation of binding kinetics using a combination of Brownian dynamics, molecular dynamics, and mile-stoning, to mapping the protein fold universe. This collection underscores the breadth of computational methods in structural biology, and only some of them made it into this special PLOS Computational Biology collection. 
 
Computational structural biology has made tremendous progress over the last two decades. Computational methods were developed for protein structure prediction, macromolecular function and protein design, as well as for drug discovery. It has also undertaken computational challenges related to experimental approaches in structural biology. Along with new experimental tools, higher resolution, and the rising efficiency of experimental approaches leading to huge amounts of accumulating data, computational biology is pushing the frontiers to meet its challenges. We expect that, in the future, the focus of our methods and tools may shift and more integrative tools will be developed, along with methodological adaptation to massively larger quantities of data. 
 
Experimentally, the biological and chemical sciences are now attempting to push boundaries in drug discovery. We may expect that a translational direction will also prevail in computational structural biology. This, however, does not mean only direct drug discovery; for these efforts to be successful, the underlying mechanistic basis of diseases needs to be understood as well. In addition, we expect even stronger emphasis on the human microbiome and its relationship to human health. PLOS Computational Biology aims to meet this challenge and place a larger focus on this important and certain-to-become-central area in the biological sciences. Clearly, many challenges remain for computational biologists in the coming years. 
 
PLOS Computational Biology, along with the computational biology community and the International Society for Computational Biology (ISCB), are poised to take on this challenge. We view this collection as the first in this direction, helping the community toward this aim.",2015,PLoS Comput. Biol.
Conformal prediction for uncertainty quantification in dynamic biological systems,"Uncertainty quantification (UQ) is the process of systematically determining and characterizing the degree of confidence in computational model predictions. In systems biology, and particularly with dynamic models, UQ is critical due to the nonlinearities and parameter sensitivities that influence the behavior of complex biological systems. Addressing these issues through robust UQ enables a deeper understanding of system dynamics and more reliable extrapolation beyond observed conditions. Many state-of-the-art UQ approaches in this field are grounded in Bayesian statistical methods. While these frameworks naturally incorporate uncertainty quantification, they often require the specification of parameter distributions as priors and may impose parametric assumptions that do not always reflect biological reality. Additionally, Bayesian methods can be computationally expensive, posing significant challenges when dealing with large-scale models and seeking rapid, reliable uncertainty calibration. As an alternative, we propose using conformal predictions methods and introduce two novel algorithms designed for dynamic biological systems. These approaches can provide non-asymptotic guarantees, improving robustness and scalability across various applications, even when the predictive models are misspecified. Through several illustrative scenarios, we demonstrate that these conformal algorithms can serve as powerful complements—or even alternatives—to conventional Bayesian methods, delivering effective uncertainty quantification for predictive tasks in systems biology.",2025,PLoS Computational Biology
A Scalable Computational Framework for Establishing Long-Term Behavior of Stochastic Reaction Networks,"Reaction networks are systems in which the populations of a finite number of species evolve through predefined interactions. Such networks are found as modeling tools in many biological disciplines such as biochemistry, ecology, epidemiology, immunology, systems biology and synthetic biology. It is now well-established that, for small population sizes, stochastic models for biochemical reaction networks are necessary to capture randomness in the interactions. The tools for analyzing such models, however, still lag far behind their deterministic counterparts. In this paper, we bridge this gap by developing a constructive framework for examining the long-term behavior and stability properties of the reaction dynamics in a stochastic setting. In particular, we address the problems of determining ergodicity of the reaction dynamics, which is analogous to having a globally attracting fixed point for deterministic dynamics. We also examine when the statistical moments of the underlying process remain bounded with time and when they converge to their steady state values. The framework we develop relies on a blend of ideas from probability theory, linear algebra and optimization theory. We demonstrate that the stability properties of a wide class of biological networks can be assessed from our sufficient theoretical conditions that can be recast as efficient and scalable linear programs, well-known for their tractability. It is notably shown that the computational complexity is often linear in the number of species. We illustrate the validity, the efficiency and the wide applicability of our results on several reaction networks arising in biochemistry, systems biology, epidemiology and ecology. The biological implications of the results as well as an example of a non-ergodic biological network are also discussed.",2013,PLoS Comput. Biol.
Urgent need for consistent standards in functional enrichment analysis,"Gene set enrichment tests (a.k.a. functional enrichment analysis) are among the most frequently used methods in computational biology. Despite this popularity, there are concerns that these methods are being applied incorrectly and the results of some peer-reviewed publications are unreliable. These problems include the use of inappropriate background gene lists, lack of false discovery rate correction and lack of methodological detail. To ascertain the frequency of these issues in the literature, we performed a screen of 186 open-access research articles describing functional enrichment results. We find that 95% of analyses using over-representation tests did not implement an appropriate background gene list or did not describe this in the methods. Failure to perform p-value correction for multiple tests was identified in 43% of analyses. Many studies lacked detail in the methods section about the tools and gene sets used. An extension of this survey showed that these problems are not associated with journal or article level bibliometrics. Using seven independent RNA-seq datasets, we show misuse of enrichment tools alters results substantially. In conclusion, most published functional enrichment studies suffered from one or more major flaws, highlighting the need for stronger standards for enrichment analysis.",2022,PLoS Comput. Biol.
"Diffusion, Crowding & Protein Stability in a Dynamic Molecular Model of the Bacterial Cytoplasm","A longstanding question in molecular biology is the extent to which the behavior of macromolecules observed in vitro accurately reflects their behavior in vivo. A number of sophisticated experimental techniques now allow the behavior of individual types of macromolecule to be studied directly in vivo; none, however, allow a wide range of molecule types to be observed simultaneously. In order to tackle this issue we have adopted a computational perspective, and, having selected the model prokaryote Escherichia coli as a test system, have assembled an atomically detailed model of its cytoplasmic environment that includes 50 of the most abundant types of macromolecules at experimentally measured concentrations. Brownian dynamics (BD) simulations of the cytoplasm model have been calibrated to reproduce the translational diffusion coefficients of Green Fluorescent Protein (GFP) observed in vivo, and “snapshots” of the simulation trajectories have been used to compute the cytoplasm's effects on the thermodynamics of protein folding, association and aggregation events. The simulation model successfully describes the relative thermodynamic stabilities of proteins measured in E. coli, and shows that effects additional to the commonly cited “crowding” effect must be included in attempts to understand macromolecular behavior in vivo.",2010,PLoS Comput. Biol.
Perturbation Biology: Inferring Signaling Networks in Cellular Systems,"We present a powerful experimental-computational technology for inferring network models that predict the response of cells to perturbations, and that may be useful in the design of combinatorial therapy against cancer. The experiments are systematic series of perturbations of cancer cell lines by targeted drugs, singly or in combination. The response to perturbation is quantified in terms of relative changes in the measured levels of proteins, phospho-proteins and cellular phenotypes such as viability. Computational network models are derived de novo, i.e., without prior knowledge of signaling pathways, and are based on simple non-linear differential equations. The prohibitively large solution space of all possible network models is explored efficiently using a probabilistic algorithm, Belief Propagation (BP), which is three orders of magnitude faster than standard Monte Carlo methods. Explicit executable models are derived for a set of perturbation experiments in SKMEL-133 melanoma cell lines, which are resistant to the therapeutically important inhibitor of RAF kinase. The resulting network models reproduce and extend known pathway biology. They empower potential discoveries of new molecular interactions and predict efficacious novel drug perturbations, such as the inhibition of PLK1, which is verified experimentally. This technology is suitable for application to larger systems in diverse areas of molecular biology.",2013,PLoS Comput. Biol.
The Roots of Bioinformatics in Theoretical Biology,"From the late 1980s onward, the term “bioinformatics” mostly has been used to refer to computational methods for comparative analysis of genome data. However, the term was originally more widely defined as the study of informatic processes in biotic systems. In this essay, I will trace this early history (from a personal point of view) and I will argue that the original meaning of the term is re-emerging.",2011,PLoS Comput. Biol.
Nine quick tips for pathway enrichment analysis,"Pathway enrichment analysis (PEA) is a computational biology method that identifies biological functions that are overrepresented in a group of genes more than would be expected by chance and ranks these functions by relevance. The relative abundance of genes pertinent to specific pathways is measured through statistical methods, and associated functional pathways are retrieved from online bioinformatics databases. In the last decade, along with the spread of the internet, higher availability of computational resources made PEA software tools easy to access and to use for bioinformatics practitioners worldwide. Although it became easier to use these tools, it also became easier to make mistakes that could generate inflated or misleading results, especially for beginners and inexperienced computational biologists. With this article, we propose nine quick tips to avoid common mistakes and to out a complete, sound, thorough PEA, which can produce relevant and robust results. We describe our nine guidelines in a simple way, so that they can be understood and used by anyone, including students and beginners. Some tips explain what to do before starting a PEA, others are suggestions of how to correctly generate meaningful results, and some final guidelines indicate some useful steps to properly interpret PEA results. Our nine tips can help users perform better pathway enrichment analyses and eventually contribute to a better understanding of current biology.",2022,PLoS Comput. Biol.
Machine and deep learning meet genome-scale metabolic modeling,"Omic data analysis is steadily growing as a driver of basic and applied molecular biology research. Core to the interpretation of complex and heterogeneous biological phenotypes are computational approaches in the fields of statistics and machine learning. In parallel, constraint-based metabolic modeling has established itself as the main tool to investigate large-scale relationships between genotype, phenotype, and environment. The development and application of these methodological frameworks have occurred independently for the most part, whereas the potential of their integration for biological, biomedical, and biotechnological research is less known. Here, we describe how machine learning and constraint-based modeling can be combined, reviewing recent works at the intersection of both domains and discussing the mathematical and practical aspects involved. We overlap systematic classifications from both frameworks, making them accessible to nonexperts. Finally, we delineate potential future scenarios, propose new joint theoretical frameworks, and suggest concrete points of investigation for this joint subfield. A multiview approach merging experimental and knowledge-driven omic data through machine learning methods can incorporate key mechanistic information in an otherwise biologically-agnostic learning process.",2019,PLoS Comput. Biol.
From time-series transcriptomics to gene regulatory networks: A review on inference methods,"Inference of gene regulatory networks has been an active area of research for around 20 years, leading to the development of sophisticated inference algorithms based on a variety of assumptions and approaches. With the ever increasing demand for more accurate and powerful models, the inference problem remains of broad scientific interest. The abstract representation of biological systems through gene regulatory networks represents a powerful method to study such systems, encoding different amounts and types of information. In this review, we summarize the different types of inference algorithms specifically based on time-series transcriptomics, giving an overview of the main applications of gene regulatory networks in computational biology. This review is intended to give an updated reference of regulatory networks inference tools to biologists and researchers new to the topic and guide them in selecting the appropriate inference method that best fits their questions, aims, and experimental data.",2022,PLoS Comput. Biol.
Hands-on training about overfitting,"Overfitting is one of the critical problems in developing models by machine learning. With machine learning becoming an essential technology in computational biology, we must include training about overfitting in all courses that introduce this technology to students and practitioners. We here propose a hands-on training for overfitting that is suitable for introductory level courses and can be carried out on its own or embedded within any data science course. We use workflow-based design of machine learning pipelines, experimentation-based teaching, and hands-on approach that focuses on concepts rather than underlying mathematics. We here detail the data analysis workflows we use in training and motivate them from the viewpoint of teaching goals. Our proposed approach relies on Orange, an open-source data science toolbox that combines data visualization and machine learning, and that is tailored for education in machine learning and explorative data analysis.",2021,PLoS Comput. Biol.
Drug Discovery Using Chemical Systems Biology: Weak Inhibition of Multiple Kinases May Contribute to the Anti-Cancer Effect of Nelfinavir,"Nelfinavir is a potent HIV-protease inhibitor with pleiotropic effects in cancer cells. Experimental studies connect its anti-cancer effects to the suppression of the Akt signaling pathway, but the actual molecular targets remain unknown. Using a structural proteome-wide off-target pipeline, which integrates molecular dynamics simulation and MM/GBSA free energy calculations with ligand binding site comparison and biological network analysis, we identified putative human off-targets of Nelfinavir and analyzed the impact on the associated biological processes. Our results suggest that Nelfinavir is able to inhibit multiple members of the protein kinase-like superfamily, which are involved in the regulation of cellular processes vital for carcinogenesis and metastasis. The computational predictions are supported by kinase activity assays and are consistent with existing experimental and clinical evidence. This finding provides a molecular basis to explain the broad-spectrum anti-cancer effect of Nelfinavir and presents opportunities to optimize the drug as a targeted polypharmacology agent.",2011,PLoS Comput. Biol.
Practical Guidelines for the Comprehensive Analysis of ChIP-seq Data,"Mapping the chromosomal locations of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, and polymerases is one of the key tasks of modern biology, as evidenced by the Encyclopedia of DNA Elements (ENCODE) Project. To this end, chromatin immunoprecipitation followed by high-throughput sequencing (ChIP-seq) is the standard methodology. Mapping such protein-DNA interactions in vivo using ChIP-seq presents multiple challenges not only in sample preparation and sequencing but also for computational analysis. Here, we present step-by-step guidelines for the computational analysis of ChIP-seq data. We address all the major steps in the analysis of ChIP-seq data: sequencing depth selection, quality checking, mapping, data normalization, assessment of reproducibility, peak calling, differential binding analysis, controlling the false discovery rate, peak annotation, visualization, and motif analysis. At each step in our guidelines we discuss some of the software tools most frequently used. We also highlight the challenges and problems associated with each step in ChIP-seq data analysis. We present a concise workflow for the analysis of ChIP-seq data in Figure 1 that complements and expands on the recommendations of the ENCODE and modENCODE projects. Each step in the workflow is described in detail in the following sections.",2013,PLoS Comput. Biol.
Drug Discovery Using Chemical Systems Biology: Identification of the Protein-Ligand Binding Network To Explain the Side Effects of CETP Inhibitors,"Systematic identification of protein-drug interaction networks is crucial to correlate complex modes of drug action to clinical indications. We introduce a novel computational strategy to identify protein-ligand binding profiles on a genome-wide scale and apply it to elucidating the molecular mechanisms associated with the adverse drug effects of Cholesteryl Ester Transfer Protein (CETP) inhibitors. CETP inhibitors are a new class of preventive therapies for the treatment of cardiovascular disease. However, clinical studies indicated that one CETP inhibitor, Torcetrapib, has deadly off-target effects as a result of hypertension, and hence it has been withdrawn from phase III clinical trials. We have identified a panel of off-targets for Torcetrapib and other CETP inhibitors from the human structural genome and map those targets to biological pathways via the literature. The predicted protein-ligand network is consistent with experimental results from multiple sources and reveals that the side-effect of CETP inhibitors is modulated through the combinatorial control of multiple interconnected pathways. Given that combinatorial control is a common phenomenon observed in many biological processes, our findings suggest that adverse drug effects might be minimized by fine-tuning multiple off-target interactions using single or multiple therapies. This work extends the scope of chemogenomics approaches and exemplifies the role that systems biology has in the future of drug discovery.",2009,PLoS Comput. Biol.
150 Years of the Mass Action Law,"This year we celebrate the 150th anniversary of the law of mass action. This law is often assumed to have been “there” forever, but it has its own history, background, and a definite starting point. The law has had an impact on chemistry, biochemistry, biomathematics, and systems biology that is difficult to overestimate. It is easily recognized that it is the direct basis for computational enzyme kinetics, ecological systems models, and models for the spread of diseases. The article reviews the explicit and implicit role of the law of mass action in systems biology and reveals how the original, more general formulation of the law emerged one hundred years later ab initio as a very general, canonical representation of biological processes.",2015,PLoS Comput. Biol.
A Computational Clonal Analysis of the Developing Mouse Limb Bud,"A comprehensive spatio-temporal description of the tissue movements underlying organogenesis would be an extremely useful resource to developmental biology. Clonal analysis and fate mappings are popular experiments to study tissue movement during morphogenesis. Such experiments allow cell populations to be labeled at an early stage of development and to follow their spatial evolution over time. However, disentangling the cumulative effects of the multiple events responsible for the expansion of the labeled cell population is not always straightforward. To overcome this problem, we develop a novel computational method that combines accurate quantification of 2D limb bud morphologies and growth modeling to analyze mouse clonal data of early limb development. Firstly, we explore various tissue movements that match experimental limb bud shape changes. Secondly, by comparing computational clones with newly generated mouse clonal data we are able to choose and characterize the tissue movement map that better matches experimental data. Our computational analysis produces for the first time a two dimensional model of limb growth based on experimental data that can be used to better characterize limb tissue movement in space and time. The model shows that the distribution and shapes of clones can be described as a combination of anisotropic growth with isotropic cell mixing, without the need for lineage compartmentalization along the AP and PD axis. Lastly, we show that this comprehensive description can be used to reassess spatio-temporal gene regulations taking tissue movement into account and to investigate PD patterning hypothesis.",2011,PLoS Comput. Biol.
Discovering Sequence Motifs with Arbitrary Insertions and Deletions,"Biology is encoded in molecular sequences: deciphering this encoding remains a grand scientific challenge. Functional regions of DNA, RNA, and protein sequences often exhibit characteristic but subtle motifs; thus, computational discovery of motifs in sequences is a fundamental and much-studied problem. However, most current algorithms do not allow for insertions or deletions (indels) within motifs, and the few that do have other limitations. We present a method, GLAM2 (Gapped Local Alignment of Motifs), for discovering motifs allowing indels in a fully general manner, and a companion method GLAM2SCAN for searching sequence databases using such motifs. glam2 is a generalization of the gapless Gibbs sampling algorithm. It re-discovers variable-width protein motifs from the PROSITE database significantly more accurately than the alternative methods PRATT and SAM-T2K. Furthermore, it usefully refines protein motifs from the ELM database: in some cases, the refined motifs make orders of magnitude fewer overpredictions than the original ELM regular expressions. GLAM2 performs respectably on the BAliBASE multiple alignment benchmark, and may be superior to leading multiple alignment methods for “motif-like” alignments with N- and C-terminal extensions. Finally, we demonstrate the use of GLAM2 to discover protein kinase substrate motifs and a gapped DNA motif for the LIM-only transcriptional regulatory complex: using GLAM2SCAN, we identify promising targets for the latter. GLAM2 is especially promising for short protein motifs, and it should improve our ability to identify the protein cleavage sites, interaction sites, post-translational modification attachment sites, etc., that underlie much of biology. It may be equally useful for arbitrarily gapped motifs in DNA and RNA, although fewer examples of such motifs are known at present. GLAM2 is public domain software, available for download at http://bioinformatics.org.au/glam2.",2008,PLoS Comput. Biol.
Agent-based modeling of morphogenetic systems: Advantages and challenges,"The complexity of morphogenesis poses a fundamental challenge to understanding the mechanisms governing the formation of biological patterns and structures. Over the past century, numerous processes have been identified as critically contributing to morphogenetic events, but the interplay between the various components and aspects of pattern formation have been much harder to grasp. The combination of traditional biology with mathematical and computational methods has had a profound effect on our current understanding of morphogenesis and led to significant insights and advancements in the field. In particular, the theoretical concepts of reaction–diffusion systems and positional information, proposed by Alan Turing and Lewis Wolpert, respectively, dramatically influenced our general view of morphogenesis, although typically in isolation from one another. In recent years, agent-based modeling has been emerging as a consolidation and implementation of the two theories within a single framework. Agent-based models (ABMs) are unique in their ability to integrate combinations of heterogeneous processes and investigate their respective dynamics, especially in the context of spatial phenomena. In this review, we highlight the benefits and technical challenges associated with ABMs as tools for examining morphogenetic events. These models display unparalleled flexibility for studying various morphogenetic phenomena at multiple levels and have the important advantage of informing future experimental work, including the targeted engineering of tissues and organs.",2019,PLoS Comput. Biol.
The development and application of bioinformatics core competencies to improve bioinformatics training and education,"Bioinformatics is recognized as part of the essential knowledge base of numerous career paths in biomedical research and healthcare. However, there is little agreement in the field over what that knowledge entails or how best to provide it. These disagreements are compounded by the wide range of populations in need of bioinformatics training, with divergent prior backgrounds and intended application areas. The Curriculum Task Force of the International Society of Computational Biology (ISCB) Education Committee has sought to provide a framework for training needs and curricula in terms of a set of bioinformatics core competencies that cut across many user personas and training programs. The initial competencies developed based on surveys of employers and training programs have since been refined through a multiyear process of community engagement. This report describes the current status of the competencies and presents a series of use cases illustrating how they are being applied in diverse training contexts. These use cases are intended to demonstrate how others can make use of the competencies and engage in the process of their continuing refinement and application. The report concludes with a consideration of remaining challenges and future plans.",2018,PLoS Comput. Biol.
Computational Methods for Protein Identification from Mass Spectrometry Data,"Protein identification using mass spectrometry is an indispensable computational tool in the life sciences. A dramatic increase in the use of proteomic strategies to understand the biology of living systems generates an ongoing need for more effective, efficient, and accurate computational methods for protein identification. A wide range of computational methods, each with various implementations, are available to complement different proteomic approaches. A solid knowledge of the range of algorithms available and, more critically, the accuracy and effectiveness of these techniques is essential to ensure as many of the proteins as possible, within any particular experiment, are correctly identified. Here, we undertake a systematic review of the currently available methods and algorithms for interpreting, managing, and analyzing biological data associated with protein identification. We summarize the advances in computational solutions as they have responded to corresponding advances in mass spectrometry hardware. The evolution of scoring algorithms and metrics for automated protein identification are also discussed with a focus on the relative performance of different techniques. We also consider the relative advantages and limitations of different techniques in particular biological contexts. Finally, we present our perspective on future developments in the area of computational protein identification by considering the most recent literature on new and promising approaches to the problem as well as identifying areas yet to be explored and the potential application of methods from other areas of computational biology.",2008,PLoS Comput. Biol.
Reproducibility efforts as a teaching tool: A pilot study,"The “replication crisis” is a methodological problem in which many scientific research findings have been difficult or impossible to replicate. Because the reproducibility of empirical results is an essential aspect of the scientific method, such failures endanger the credibility of theories based on them and possibly significant portions of scientific knowledge. An instance of the replication crisis, analytic replication, pertains to reproducing published results through computational reanalysis of the authors’ original data. However, direct replications are costly, time-consuming, and unrewarded in today’s publishing standards. We propose that bioinformatics and computational biology students replicate recent discoveries as part of their curriculum. Considering the above, we performed a pilot study in one of the graduate-level courses we developed and taught at our University. The course is entitled Intro to R Programming and is meant for students in our Master’s and PhD programs who have little to no programming skills. As the course emphasized real-world data analysis, we thought it would be an appropriate setting to carry out this study. The primary objective was to expose the students to real biological data analysis problems. These include locating and downloading the needed datasets, understanding any underlying conventions and annotations, understanding the analytical methods, and regenerating multiple graphs from their assigned article. The secondary goal was to determine whether the assigned articles contained sufficient information for a graduate-level student to replicate its figures. Overall, the students successfully reproduced 39% of the figures. The main obstacles were the need for more advanced programming skills and the incomplete documentation of the applied methods. Students were engaged, enthusiastic, and focused throughout the semester. We believe that this teaching approach will allow students to make fundamental scientific contributions under appropriate supervision. It will teach them about the scientific process, the importance of reporting standards, and the importance of openness.",2022,PLoS Comput. Biol.
Combining Experiments and Simulations Using the Maximum Entropy Principle,"A key component of computational biology is to compare the results of computer modelling with experimental measurements. Despite substantial progress in the models and algorithms used in many areas of computational biology, such comparisons sometimes reveal that the computations are not in quantitative agreement with experimental data. The principle of maximum entropy is a general procedure for constructing probability distributions in the light of new data, making it a natural tool in cases when an initial model provides results that are at odds with experiments. The number of maximum entropy applications in our field has grown steadily in recent years, in areas as diverse as sequence analysis, structural modelling, and neurobiology. In this Perspectives article, we give a broad introduction to the method, in an attempt to encourage its further adoption. The general procedure is explained in the context of a simple example, after which we proceed with a real-world application in the field of molecular simulations, where the maximum entropy procedure has recently provided new insight. Given the limited accuracy of force fields, macromolecular simulations sometimes produce results that are at not in complete and quantitative accordance with experiments. A common solution to this problem is to explicitly ensure agreement between the two by perturbing the potential energy function towards the experimental data. So far, a general consensus for how such perturbations should be implemented has been lacking. Three very recent papers have explored this problem using the maximum entropy approach, providing both new theoretical and practical insights to the problem. We highlight each of these contributions in turn and conclude with a discussion on remaining challenges.",2014,PLoS Comput. Biol.
Nutritional Systems Biology Modeling: From Molecular Mechanisms to Physiology,"The use of computational modeling and simulation has increased in many biological fields, but despite their potential these techniques are only marginally applied in nutritional sciences. Nevertheless, recent applications of modeling have been instrumental in answering important nutritional questions from the cellular up to the physiological levels. Capturing the complexity of today's important nutritional research questions poses a challenge for modeling to become truly integrative in the consideration and interpretation of experimental data at widely differing scales of space and time. In this review, we discuss a selection of available modeling approaches and applications relevant for nutrition. We then put these models into perspective by categorizing them according to their space and time domain. Through this categorization process, we identified a dearth of models that consider processes occurring between the microscopic and macroscopic scale. We propose a “middle-out” strategy to develop the required full-scale, multilevel computational models. Exhaustive and accurate phenotyping, the use of the virtual patient concept, and the development of biomarkers from “-omics” signatures are identified as key elements of a successful systems biology modeling approach in nutrition research—one that integrates physiological mechanisms and data at multiple space and time scales.",2009,PLoS Comput. Biol.
Information and Efficiency in the Nervous System—A Synthesis,"In systems biology, questions concerning the molecular and cellular makeup of an organism are of utmost importance, especially when trying to understand how unreliable components—like genetic circuits, biochemical cascades, and ion channels, among others—enable reliable and adaptive behaviour. The repertoire and speed of biological computations are limited by thermodynamic or metabolic constraints: an example can be found in neurons, where fluctuations in biophysical states limit the information they can encode—with almost 20–60% of the total energy allocated for the brain used for signalling purposes, either via action potentials or by synaptic transmission. Here, we consider the imperatives for neurons to optimise computational and metabolic efficiency, wherein benefits and costs trade-off against each other in the context of self-organised and adaptive behaviour. In particular, we try to link information theoretic (variational) and thermodynamic (Helmholtz) free-energy formulations of neuronal processing and show how they are related in a fundamental way through a complexity minimisation lemma.",2013,PLoS Comput. Biol.
Determinants of combination GM-CSF immunotherapy and oncolytic virotherapy success identified through in silico treatment personalization,"Oncolytic virotherapies, including the modified herpes simplex virus talimogene laherparepvec (T-VEC), have shown great promise as potent instigators of anti-tumour immune effects. The OPTiM trial, in particular, demonstrated the superior anti-cancer effects of T-VEC as compared to systemic immunotherapy treatment using exogenous administration of granulocyte-macrophage colony-stimulating factor (GM-CSF). Theoretically, a combined approach leveraging exogenous cytokine immunotherapy and oncolytic virotherapy would elicit an even greater immune response and improve patient outcomes. However, regimen scheduling of combination immunostimulation and T-VEC therapy has yet to be established. Here, we calibrate a computational biology model of sensitive and resistant tumour cells and immune interactions for implementation into an in silico clinical trial to test and individualize combination immuno- and virotherapy. By personalizing and optimizing combination oncolytic virotherapy and immunostimulatory therapy, we show improved simulated patient outcomes for individuals with late-stage melanoma. More crucially, through evaluation of individualized regimens, we identified determinants of combination GM-CSF and T-VEC therapy that can be translated into clinically-actionable dosing strategies without further personalization. Our results serve as a proof-of-concept for interdisciplinary approaches to determining combination therapy, and suggest promising avenues of investigation towards tailored combination immunotherapy/oncolytic virotherapy.",2019,PLoS Comput. Biol.
Combined Evidence Annotation of Transposable Elements in Genome Sequences,"Transposable elements (TEs) are mobile, repetitive sequences that make up significant fractions of metazoan genomes. Despite their near ubiquity and importance in genome and chromosome biology, most efforts to annotate TEs in genome sequences rely on the results of a single computational program, RepeatMasker. In contrast, recent advances in gene annotation indicate that high-quality gene models can be produced from combining multiple independent sources of computational evidence. To elevate the quality of TE annotations to a level comparable to that of gene models, we have developed a combined evidence-model TE annotation pipeline, analogous to systems used for gene annotation, by integrating results from multiple homology-based and de novo TE identification methods. As proof of principle, we have annotated “TE models” in Drosophila melanogaster Release 4 genomic sequences using the combined computational evidence derived from RepeatMasker, BLASTER, TBLASTX, all-by-all BLASTN, RECON, TE-HMM and the previous Release 3.1 annotation. Our system is designed for use with the Apollo genome annotation tool, allowing automatic results to be curated manually to produce reliable annotations. The euchromatic TE fraction of D. melanogaster is now estimated at 5.3% (cf. 3.86% in Release 3.1), and we found a substantially higher number of TEs (n = 6,013) than previously identified (n = 1,572). Most of the new TEs derive from small fragments of a few hundred nucleotides long and highly abundant families not previously annotated (e.g., INE-1). We also estimated that 518 TE copies (8.6%) are inserted into at least one other TE, forming a nest of elements. The pipeline allows rapid and thorough annotation of even the most complex TE models, including highly deleted and/or nested elements such as those often found in heterochromatic sequences. Our pipeline can be easily adapted to other genome sequences, such as those of the D. melanogaster heterochromatin or other species in the genus Drosophila.",2005,PLoS Comput. Biol.
Strong Inference for Systems Biology,"Platt's 1964 essay on strong inference [1] illuminates a rational approach to scientific inquiry that integrates seamlessly with current investigations on the operation of complex biological systems. Yet in re-examining the 1964 essay in light of current trends, it is apparent that the groundbreaking approach has failed to become universal. Here it is argued that both the opportunity and the need to follow Platt's advice are now greater than ever. A revised method of strong inference for systems biology is presented and applied to analyze longstanding questions in cardiac energy metabolism. It is shown how this logical framework combined with computational-based hypothesis testing illuminates unresolved questions regarding how the energetic state of the heart is maintained in response to changes in the rate of ATP hydrolysis.",2009,PLoS Comput. Biol.
"Structural Biology by NMR: Structure, Dynamics, and Interactions","The function of bio-macromolecules is determined by both their 3D structure and conformational dynamics. These molecules are inherently flexible systems displaying a broad range of dynamics on time-scales from picoseconds to seconds. Nuclear Magnetic Resonance (NMR) spectroscopy has emerged as the method of choice for studying both protein structure and dynamics in solution. Typically, NMR experiments are sensitive both to structural features and to dynamics, and hence the measured data contain information on both. Despite major progress in both experimental approaches and computational methods, obtaining a consistent view of structure and dynamics from experimental NMR data remains a challenge. Molecular dynamics simulations have emerged as an indispensable tool in the analysis of NMR data.",2008,PLoS Comput. Biol.
Bioinformatics Curriculum Guidelines: Toward a Definition of Core Competencies,"Rapid advances in the life sciences and in related information technologies necessitate the ongoing refinement of bioinformatics educational programs in order to maintain their relevance. As the discipline of bioinformatics and computational biology expands and matures, it is important to characterize the elements that contribute to the success of professionals in this field. These individuals work in a wide variety of settings, including bioinformatics core facilities, biological and medical research laboratories, software development organizations, pharmaceutical and instrument development companies, and institutions that provide education, service, and training. In response to this need, the Curriculum Task Force of the International Society for Computational Biology (ISCB) Education Committee seeks to define curricular guidelines for those who train and educate bioinformaticians. The previous report of the task force summarized a survey that was conducted to gather input regarding the skill set needed by bioinformaticians [1]. The current article details a subsequent effort, wherein the task force broadened its perspectives by examining bioinformatics career opportunities, surveying directors of bioinformatics core facilities, and reviewing bioinformatics education programs. 
 
The bioinformatics literature provides valuable perspectives on bioinformatics education by defining skill sets needed by bioinformaticians, presenting approaches for providing informatics training to biologists, and discussing the roles of bioinformatics core facilities in training and education. 
 
The skill sets required for success in the field of bioinformatics are considered by several authors: Altman [2] defines five broad areas of competency and lists key technologies; Ranganathan [3] presents highlights from the Workshops on Education in Bioinformatics, discussing challenges and possible solutions; Yale's interdepartmental PhD program in computational biology and bioinformatics is described in [4], which lists the general areas of knowledge of bioinformatics; in a related article, a graduate of Yale's PhD program reflects on the skills needed by a bioinformatician [5]; Altman and Klein [6] describe the Stanford Biomedical Informatics (BMI) Training Program, presenting observed trends among BMI students; the American Medical Informatics Association defines competencies in the related field of biomedical informatics in [7]; and the approaches used in several German universities to implement bioinformatics education are described in [8]. 
 
Several approaches to providing bioinformatics training for biologists are described in the literature. Tan et al. [9] report on workshops conducted to identify a minimum skill set for biologists to be able to address the informatics challenges of the “-omics” era. They define a requisite skill set by analyzing responses to questions about the knowledge, skills, and abilities that biologists should possess. The authors in [10] present examples of strategies and methods for incorporating bioinformatics content into undergraduate life sciences curricula. Pevzner and Shamir [11] propose that undergraduate biology curricula should contain an additional course, “Algorithmic, Mathematical, and Statistical Concepts in Biology.” Wingren and Botstein [12] present a graduate course in quantitative biology that is based on original, pathbreaking papers in diverse areas of biology. Johnson and Friedman [13] evaluate the effectiveness of incorporating biological informatics into a clinical informatics program. The results reported are based on interviews of four students and informal assessments of bioinformatics faculty. 
 
The challenges and opportunities relevant to training and education in the context of bioinformatics core facilities are discussed by Lewitter et al. [14]. Relatedly, Lewitter and Rebhan [15] provide guidance regarding the role of a bioinformatics core facility in hiring biologists and in furthering their education in bioinformatics. Richter and Sexton [16] describe a need for highly trained bioinformaticians in core facilities and provide a list of requisite skills. Similarly, Kallioniemi et al. [17] highlight the roles of bioinformatics core units in education and training. 
 
This manuscript expands the body of knowledge pertaining to bioinformatics curriculum guidelines by presenting the results from a broad set of surveys (of core facility directors, of career opportunities, and of existing curricula). Although there is some overlap in the findings of the surveys, they are reported separately, in order to avoid masking the unique aspects of each of the perspectives and to demonstrate that the same themes arise, even when different perspectives are considered. The authors derive from their surveys an initial set of core competencies and relate the competencies to three different categories of professions that have a need for bioinformatics training.",2014,PLoS Comput. Biol.
An Introduction to Programming for Bioscientists: A Python-Based Primer,"Computing has revolutionized the biological sciences over the past several decades, such that virtually all contemporary research in molecular biology, biochemistry, and other biosciences utilizes computer programs. The computational advances have come on many fronts, spurred by fundamental developments in hardware, software, and algorithms. These advances have influenced, and even engendered, a phenomenal array of bioscience fields, including molecular evolution and bioinformatics; genome-, proteome-, transcriptome- and metabolome-wide experimental studies; structural genomics; and atomistic simulations of cellular-scale molecular assemblies as large as ribosomes and intact viruses. In short, much of post-genomic biology is increasingly becoming a form of computational biology. The ability to design and write computer programs is among the most indispensable skills that a modern researcher can cultivate. Python has become a popular programming language in the biosciences, largely because (i) its straightforward semantics and clean syntax make it a readily accessible first language; (ii) it is expressive and well-suited to object-oriented programming, as well as other modern paradigms; and (iii) the many available libraries and third-party toolkits extend the functionality of the core language into virtually every biological domain (sequence and structure analyses, phylogenomics, workflow management systems, etc.). This primer offers a basic introduction to coding, via Python, and it includes concrete examples and exercises to illustrate the language’s usage and capabilities; the main text culminates with a final project in structural bioinformatics. A suite of Supplemental Chapters is also provided. Starting with basic concepts, such as that of a “variable,” the Chapters methodically advance the reader to the point of writing a graphical user interface to compute the Hamming distance between two DNA sequences.",2016,PLoS Comput. Biol.
Ten simple rules for biologists learning to program,"As big data and multi-omics analyses are becoming mainstream, computational proficiency and literacy are essential skills in a biologist’s tool kit. All “omics” studies require computational biology: the implementation of analyses requires programming skills, while experimental design and interpretation require a solid understanding of the analytical approach. While academic cores, commercial services, and collaborations can aid in the implementation of analyses, the computational literacy required to design and interpret omics studies cannot be replaced or supplemented. However, many biologists are only trained in experimental techniques. We write these 10 simple rules for traditionally trained biologists, particularly graduate students interested in acquiring a computational skill set.",2018,PLoS Comput. Biol.
The What and Where of Adding Channel Noise to the Hodgkin-Huxley Equations,"Conductance-based equations for electrically active cells form one of the most widely studied mathematical frameworks in computational biology. This framework, as expressed through a set of differential equations by Hodgkin and Huxley, synthesizes the impact of ionic currents on a cell's voltage—and the highly nonlinear impact of that voltage back on the currents themselves—into the rapid push and pull of the action potential. Later studies confirmed that these cellular dynamics are orchestrated by individual ion channels, whose conformational changes regulate the conductance of each ionic current. Thus, kinetic equations familiar from physical chemistry are the natural setting for describing conductances; for small-to-moderate numbers of channels, these will predict fluctuations in conductances and stochasticity in the resulting action potentials. At first glance, the kinetic equations provide a far more complex (and higher-dimensional) description than the original Hodgkin-Huxley equations or their counterparts. This has prompted more than a decade of efforts to capture channel fluctuations with noise terms added to the equations of Hodgkin-Huxley type. Many of these approaches, while intuitively appealing, produce quantitative errors when compared to kinetic equations; others, as only very recently demonstrated, are both accurate and relatively simple. We review what works, what doesn't, and why, seeking to build a bridge to well-established results for the deterministic equations of Hodgkin-Huxley type as well as to more modern models of ion channel dynamics. As such, we hope that this review will speed emerging studies of how channel noise modulates electrophysiological dynamics and function. We supply user-friendly MATLAB simulation code of these stochastic versions of the Hodgkin-Huxley equations on the ModelDB website (accession number 138950) and http://www.amath.washington.edu/~etsb/tutorials.html.",2011,PLoS Comput. Biol.
Disentangling Direct from Indirect Co-Evolution of Residues in Protein Alignments,"Predicting protein structure from primary sequence is one of the ultimate challenges in computational biology. Given the large amount of available sequence data, the analysis of co-evolution, i.e., statistical dependency, between columns in multiple alignments of protein domain sequences remains one of the most promising avenues for predicting residues that are contacting in the structure. A key impediment to this approach is that strong statistical dependencies are also observed for many residue pairs that are distal in the structure. Using a comprehensive analysis of protein domains with available three-dimensional structures we show that co-evolving contacts very commonly form chains that percolate through the protein structure, inducing indirect statistical dependencies between many distal pairs of residues. We characterize the distributions of length and spatial distance traveled by these co-evolving contact chains and show that they explain a large fraction of observed statistical dependencies between structurally distal pairs. We adapt a recently developed Bayesian network model into a rigorous procedure for disentangling direct from indirect statistical dependencies, and we demonstrate that this method not only successfully accomplishes this task, but also allows contacts with weak statistical dependency to be detected. To illustrate how additional information can be incorporated into our method, we incorporate a phylogenetic correction, and we develop an informative prior that takes into account that the probability for a pair of residues to contact depends strongly on their primary-sequence distance and the amount of conservation that the corresponding columns in the multiple alignment exhibit. We show that our model including these extensions dramatically improves the accuracy of contact prediction from multiple sequence alignments.",2010,PLoS Comput. Biol.
Ten Simple Rules for a Successful Cross-Disciplinary Collaboration,"Cross-disciplinary collaborations have become an increasingly important part of science. They are seen as key if we are to find solutions to pressing, global-scale societal challenges, including green technologies, sustainable food production, and drug development. Regulators and policy-makers have realized the power of such collaborations, for example, in the 80 billion Euro ""Horizon 2020"" EU Framework Programme for Research and Innovation. This programme puts special emphasis on “breaking down barriers to create a genuine single market for knowledge, research and innovation” (http://ec.europa.eu/programmes/horizon2020/en/what-horizon-2020). 
 
Cross-disciplinary collaborations are key to all partners in computational biology. On the one hand, for scientists working in theoretical fields such as computer science, mathematics, or statistics, validation of predictions against experimental data is of the utmost importance. On the other hand, experimentalists, such as molecular biologists, geneticists, or clinicians, often want to reduce the number of experiments needed to achieve a certain scientific aim, to obtain insight into processes that are inaccessible using current experimental techniques, or to handle large volumes of data, which are far beyond any human analysis skills. 
 
The synergistic and skilfulcombining ofdifferent disciplines can achieve insight beyond current borders and thereby generate novel solutions to complex problems. The combination of methods and data from different fields can achieve more than the sum of the individual parts could do alone. This applies not only to computational biology but also tomany other academic disciplines. 
 
Initiating and successfully maintaining cross-disciplinary collaborations can be challenging but highly rewarding. In a previous publication in this series, ten simple rules for a successful collaboration were proposed [1]. In the present guide, we go one step further and focus on the specific challenges associated with cross-disciplinary research, from the perspective of the theoretician in particular. As research fellows of the 2020 Science project (http://www.2020science.net) and collaboration partners, we bring broad experience of developing interdisciplinary collaborations. We intend this guide to be for early career computational researchers as well as more senior scientists who are entering a cross-disciplinary setting for the first time. We describe the key benefits, as well as some possible pitfalls, arising from collaborations between scientists with very different backgrounds. 
 
Rule 1: Enjoy Entering a Completely New Field of Research 
Collaborating with scientists from other disciplines is an opportunity to learn about cutting-edge science directly from experts. Make the most of being the novice. No one expects you to know everything about the new field. In particular, there is no pressure to understand everything immediately, so ask the “stupid” questions. Demonstrating your interest and enthusiasm is of much higher value than pretending to know everything already. An interested audience makes information sharing much easier for all partners in a collaboration. 
 
You should prepare for a deluge of new ideas and approaches. It is a good practice to read relevant textbooks and review papers, which your collaborators should be able to recommend, in order to quickly grasp the vocabulary (see Rule 3) and key ideas of the new field. This will make it easier for you to establish a common parlance between you and your collaborators, and allow you to build from there. 
 
You should try to discuss your work with a range of scientists from complementary fields. As well as getting feedback, this can help you identify new collaborative opportunities. Remember that contacts that do not lead directly to collaborations can still prove useful later in your career.",2015,PLoS Comput. Biol.
A Systematic Framework for Molecular Dynamics Simulations of Protein Post-Translational Modifications,"By directly affecting structure, dynamics and interaction networks of their targets, post-translational modifications (PTMs) of proteins play a key role in different cellular processes ranging from enzymatic activation to regulation of signal transduction to cell-cycle control. Despite the great importance of understanding how PTMs affect proteins at the atomistic level, a systematic framework for treating post-translationally modified amino acids by molecular dynamics (MD) simulations, a premier high-resolution computational biology tool, has never been developed. Here, we report and validate force field parameters (GROMOS 45a3 and 54a7) required to run and analyze MD simulations of more than 250 different types of enzymatic and non-enzymatic PTMs. The newly developed GROMOS 54a7 parameters in particular exhibit near chemical accuracy in matching experimentally measured hydration free energies (RMSE = 4.2 kJ/mol over the validation set). Using this tool, we quantitatively show that the majority of PTMs greatly alter the hydrophobicity and other physico-chemical properties of target amino acids, with the extent of change in many cases being comparable to the complete range spanned by native amino acids.",2013,PLoS Comput. Biol.
Training Signaling Pathway Maps to Biochemical Data with Constrained Fuzzy Logic: Quantitative Analysis of Liver Cell Responses to Inflammatory Stimuli,"Predictive understanding of cell signaling network operation based on general prior knowledge but consistent with empirical data in a specific environmental context is a current challenge in computational biology. Recent work has demonstrated that Boolean logic can be used to create context-specific network models by training proteomic pathway maps to dedicated biochemical data; however, the Boolean formalism is restricted to characterizing protein species as either fully active or inactive. To advance beyond this limitation, we propose a novel form of fuzzy logic sufficiently flexible to model quantitative data but also sufficiently simple to efficiently construct models by training pathway maps on dedicated experimental measurements. Our new approach, termed constrained fuzzy logic (cFL), converts a prior knowledge network (obtained from literature or interactome databases) into a computable model that describes graded values of protein activation across multiple pathways. We train a cFL-converted network to experimental data describing hepatocytic protein activation by inflammatory cytokines and demonstrate the application of the resultant trained models for three important purposes: (a) generating experimentally testable biological hypotheses concerning pathway crosstalk, (b) establishing capability for quantitative prediction of protein activity, and (c) prediction and understanding of the cytokine release phenotypic response. Our methodology systematically and quantitatively trains a protein pathway map summarizing curated literature to context-specific biochemical data. This process generates a computable model yielding successful prediction of new test data and offering biological insight into complex datasets that are difficult to fully analyze by intuition alone.",2011,PLoS Comput. Biol.
Large-Scale Chemical Similarity Networks for Target Profiling of Compounds Identified in Cell-Based Chemical Screens,"Target identification is one of the most critical steps following cell-based phenotypic chemical screens aimed at identifying compounds with potential uses in cell biology and for developing novel disease therapies. Current in silico target identification methods, including chemical similarity database searches, are limited to single or sequential ligand analysis that have limited capabilities for accurate deconvolution of a large number of compounds with diverse chemical structures. Here, we present CSNAP (Chemical Similarity Network Analysis Pulldown), a new computational target identification method that utilizes chemical similarity networks for large-scale chemotype (consensus chemical pattern) recognition and drug target profiling. Our benchmark study showed that CSNAP can achieve an overall higher accuracy (>80%) of target prediction with respect to representative chemotypes in large (>200) compound sets, in comparison to the SEA approach (60–70%). Additionally, CSNAP is capable of integrating with biological knowledge-based databases (Uniprot, GO) and high-throughput biology platforms (proteomic, genetic, etc) for system-wise drug target validation. To demonstrate the utility of the CSNAP approach, we combined CSNAP's target prediction with experimental ligand evaluation to identify the major mitotic targets of hit compounds from a cell-based chemical screen and we highlight novel compounds targeting microtubules, an important cancer therapeutic target. The CSNAP method is freely available and can be accessed from the CSNAP web server (http://services.mbi.ucla.edu/CSNAP/).",2015,PLoS Comput. Biol.
"Mapping the Conformation Space of Wildtype and Mutant H-Ras with a Memetic, Cellular, and Multiscale Evolutionary Algorithm","An important goal in molecular biology is to understand functional changes upon single-point mutations in proteins. Doing so through a detailed characterization of structure spaces and underlying energy landscapes is desirable but continues to challenge methods based on Molecular Dynamics. In this paper we propose a novel algorithm, SIfTER, which is based instead on stochastic optimization to circumvent the computational challenge of exploring the breadth of a protein’s structure space. SIfTER is a data-driven evolutionary algorithm, leveraging experimentally-available structures of wildtype and variant sequences of a protein to define a reduced search space from where to efficiently draw samples corresponding to novel structures not directly observed in the wet laboratory. The main advantage of SIfTER is its ability to rapidly generate conformational ensembles, thus allowing mapping and juxtaposing landscapes of variant sequences and relating observed differences to functional changes. We apply SIfTER to variant sequences of the H-Ras catalytic domain, due to the prominent role of the Ras protein in signaling pathways that control cell proliferation, its well-studied conformational switching, and abundance of documented mutations in several human tumors. Many Ras mutations are oncogenic, but detailed energy landscapes have not been reported until now. Analysis of SIfTER-computed energy landscapes for the wildtype and two oncogenic variants, G12V and Q61L, suggests that these mutations cause constitutive activation through two different mechanisms. G12V directly affects binding specificity while leaving the energy landscape largely unchanged, whereas Q61L has pronounced, starker effects on the landscape. An implementation of SIfTER is made available at http://www.cs.gmu.edu/~ashehu/?q=OurTools. We believe SIfTER is useful to the community to answer the question of how sequence mutations affect the function of a protein, when there is an abundance of experimental structures that can be exploited to reconstruct an energy landscape that would be computationally impractical to do via Molecular Dynamics.",2015,PLoS Comput. Biol.
Meet-U: Educating through research immersion,"We present a new educational initiative called Meet-U that aims to train students for collaborative work in computational biology and to bridge the gap between education and research. Meet-U mimics the setup of collaborative research projects and takes advantage of the most popular tools for collaborative work and of cloud computing. Students are grouped in teams of 4–5 people and have to realize a project from A to Z that answers a challenging question in biology. Meet-U promotes ""coopetition,"" as the students collaborate within and across the teams and are also in competition with each other to develop the best final product. Meet-U fosters interactions between different actors of education and research through the organization of a meeting day, open to everyone, where the students present their work to a jury of researchers and jury members give research seminars. This very unique combination of education and research is strongly motivating for the students and provides a formidable opportunity for a scientific community to unite and increase its visibility. We report on our experience with Meet-U in two French universities with master’s students in bioinformatics and modeling, with protein–protein docking as the subject of the course. Meet-U is easy to implement and can be straightforwardly transferred to other fields and/or universities. All the information and data are available at www.meet-u.org.",2018,PLoS Comput. Biol.
Generation of Diverse Biological Forms through Combinatorial Interactions between Tissue Polarity and Growth,"A major problem in biology is to understand how complex tissue shapes may arise through growth. In many cases this process involves preferential growth along particular orientations raising the question of how these orientations are specified. One view is that orientations are specified through stresses in the tissue (axiality-based system). Another possibility is that orientations can be specified independently of stresses through molecular signalling (polarity-based system). The axiality-based system has recently been explored through computational modelling. Here we develop and apply a polarity-based system which we call the Growing Polarised Tissue (GPT) framework. Tissue is treated as a continuous material within which regionally expressed factors under genetic control may interact and propagate. Polarity is established by signals that propagate through the tissue and is anchored in regions termed tissue polarity organisers that are also under genetic control. Rates of growth parallel or perpendicular to the local polarity may then be specified through a regulatory network. The resulting growth depends on how specified growth patterns interact within the constraints of mechanically connected tissue. This constraint leads to the emergence of features such as curvature that were not directly specified by the regulatory networks. Resultant growth feeds back to influence spatial arrangements and local orientations of tissue, allowing complex shapes to emerge from simple rules. Moreover, asymmetries may emerge through interactions between polarity fields. We illustrate the value of the GPT-framework for understanding morphogenesis by applying it to a growing Snapdragon flower and indicate how the underlying hypotheses may be tested by computational simulation. We propose that combinatorial intractions between orientations and rates of growth, which are a key feature of polarity-based systems, have been exploited during evolution to generate a range of observed biological shapes.",2011,PLoS Comput. Biol.
Speeding Up Ecological and Evolutionary Computations in R; Essentials of High Performance Computing for Biologists,"Computation has become a critical component of research in biology. A risk has emerged that computational and programming challenges may limit research scope, depth, and quality. We review various solutions to common computational efficiency problems in ecological and evolutionary research. Our review pulls together material that is currently scattered across many sources and emphasizes those techniques that are especially effective for typical ecological and environmental problems. We demonstrate how straightforward it can be to write efficient code and implement techniques such as profiling or parallel computing. We supply a newly developed R package (aprof) that helps to identify computational bottlenecks in R code and determine whether optimization can be effective. Our review is complemented by a practical set of examples and detailed Supporting Information material (S1–S3 Texts) that demonstrate large improvements in computational speed (ranging from 10.5 times to 14,000 times faster). By improving computational efficiency, biologists can feasibly solve more complex tasks, ask more ambitious questions, and include more sophisticated analyses in their research.",2015,PLoS Comput. Biol.
Ten simple rules for drawing scientific comics,"1 Computational Biology and Bioinformatics, Pacific Northwest National Laboratory, Richland, Washington, United States of America, 2 Department of Molecular Microbiology and Immunology, Oregon Health & Sciences University, Portland, Oregon, United States of America, 3 Engineering Photonics, Cranfield University, Cranfield, Bedfordshire, United Kingdom, 4 Department of Biochemistry and Microbiology, Rutgers University, New Brunswick, New Jersey, United States of America, 5 Institute for Advanced Study, Technische Universität München, Garching, Germany",2018,PLoS Comput. Biol.
BioStar: An Online Question & Answer Resource for the Bioinformatics Community,"Although the era of big data has produced many bioinformatics tools and databases, using them effectively often requires specialized knowledge. Many groups lack bioinformatics expertise, and frequently find that software documentation is inadequate while local colleagues may be overburdened or unfamiliar with specific applications. Too often, such problems create data analysis bottlenecks that hinder the progress of biological research. In order to help address this deficiency, we present BioStar, a forum based on the Stack Exchange platform where experts and those seeking solutions to problems of computational biology exchange ideas. The main strengths of BioStar are its large and active group of knowledgeable users, rapid response times, clear organization of questions and responses that limit discussion to the topic at hand, and ranking of questions and answers that help identify their usefulness. These rankings, based on community votes, also contribute to a reputation score for each user, which serves to keep expert contributors engaged. The BioStar community has helped to answer over 2,300 questions from over 1,400 users (as of June 10, 2011), and has played a critical role in enabling and expediting many research projects. BioStar can be accessed at http://www.biostars.org/.",2011,PLoS Comput. Biol.
Physiological models of the lateral superior olive,"In computational biology, modeling is a fundamental tool for formulating, analyzing and predicting complex phenomena. Most neuron models, however, are designed to reproduce certain small sets of empirical data. Hence their outcome is usually not compatible or comparable with other models or datasets, making it unclear how widely applicable such models are. In this study, we investigate these aspects of modeling, namely credibility and generalizability, with a specific focus on auditory neurons involved in the localization of sound sources. The primary cues for binaural sound localization are comprised of interaural time and level differences (ITD/ILD), which are the timing and intensity differences of the sound waves arriving at the two ears. The lateral superior olive (LSO) in the auditory brainstem is one of the locations where such acoustic information is first computed. An LSO neuron receives temporally structured excitatory and inhibitory synaptic inputs that are driven by ipsi- and contralateral sound stimuli, respectively, and changes its spike rate according to binaural acoustic differences. Here we examine seven contemporary models of LSO neurons with different levels of biophysical complexity, from predominantly functional ones (‘shot-noise’ models) to those with more detailed physiological components (variations of integrate-and-fire and Hodgkin-Huxley-type). These models, calibrated to reproduce known monaural and binaural characteristics of LSO, generate largely similar results to each other in simulating ITD and ILD coding. Our comparisons of physiological detail, computational efficiency, predictive performances, and further expandability of the models demonstrate (1) that the simplistic, functional LSO models are suitable for applications where low computational costs and mathematical transparency are needed, (2) that more complex models with detailed membrane potential dynamics are necessary for simulation studies where sub-neuronal nonlinear processes play important roles, and (3) that, for general purposes, intermediate models might be a reasonable compromise between simplicity and biological plausibility.",2017,PLoS Comput. Biol.
Regression Analysis for Constraining Free Parameters in Electrophysiological Models of Cardiac Cells,"A major challenge in computational biology is constraining free parameters in mathematical models. Adjusting a parameter to make a given model output more realistic sometimes has unexpected and undesirable effects on other model behaviors. Here, we extend a regression-based method for parameter sensitivity analysis and show that a straightforward procedure can uniquely define most ionic conductances in a well-known model of the human ventricular myocyte. The model's parameter sensitivity was analyzed by randomizing ionic conductances, running repeated simulations to measure physiological outputs, then collecting the randomized parameters and simulation results as “input” and “output” matrices, respectively. Multivariable regression derived a matrix whose elements indicate how changes in conductances influence model outputs. We show here that if the number of linearly-independent outputs equals the number of inputs, the regression matrix can be inverted. This is significant, because it implies that the inverted matrix can specify the ionic conductances that are required to generate a particular combination of model outputs. Applying this idea to the myocyte model tested, we found that most ionic conductances could be specified with precision (R2 > 0.77 for 12 out of 16 parameters). We also applied this method to a test case of changes in electrophysiology caused by heart failure and found that changes in most parameters could be well predicted. We complemented our findings using a Bayesian approach to demonstrate that model parameters cannot be specified using limited outputs, but they can be successfully constrained if multiple outputs are considered. Our results place on a solid mathematical footing the intuition-based procedure simultaneously matching a model's output to several data sets. More generally, this method shows promise as a tool to define model parameters, in electrophysiology and in other biological fields.",2009,PLoS Comput. Biol.
Unraveling Protein Networks with Power Graph Analysis,"Networks play a crucial role in computational biology, yet their analysis and representation is still an open problem. Power Graph Analysis is a lossless transformation of biological networks into a compact, less redundant representation, exploiting the abundance of cliques and bicliques as elementary topological motifs. We demonstrate with five examples the advantages of Power Graph Analysis. Investigating protein-protein interaction networks, we show how the catalytic subunits of the casein kinase II complex are distinguishable from the regulatory subunits, how interaction profiles and sequence phylogeny of SH3 domains correlate, and how false positive interactions among high-throughput interactions are spotted. Additionally, we demonstrate the generality of Power Graph Analysis by applying it to two other types of networks. We show how power graphs induce a clustering of both transcription factors and target genes in bipartite transcription networks, and how the erosion of a phosphatase domain in type 22 non-receptor tyrosine phosphatases is detected. We apply Power Graph Analysis to high-throughput protein interaction networks and show that up to 85% (56% on average) of the information is redundant. Experimental networks are more compressible than rewired ones of same degree distribution, indicating that experimental networks are rich in cliques and bicliques. Power Graphs are a novel representation of networks, which reduces network complexity by explicitly representing re-occurring network motifs. Power Graphs compress up to 85% of the edges in protein interaction networks and are applicable to all types of networks such as protein interactions, regulatory networks, or homology networks.",2008,PLoS Comput. Biol.
Thermodynamic State Ensemble Models of cis-Regulation,"A major goal in computational biology is to develop models that accurately predict a gene's expression from its surrounding regulatory DNA. Here we present one class of such models, thermodynamic state ensemble models. We describe the biochemical derivation of the thermodynamic framework in simple terms, and lay out the mathematical components that comprise each model. These components include (1) the possible states of a promoter, where a state is defined as a particular arrangement of transcription factors bound to a DNA promoter, (2) the binding constants that describe the affinity of the protein–protein and protein–DNA interactions that occur in each state, and (3) whether each state is capable of transcribing. Using these components, we demonstrate how to compute a cis-regulatory function that encodes the probability of a promoter being active. Our intention is to provide enough detail so that readers with little background in thermodynamics can compose their own cis-regulatory functions. To facilitate this goal, we also describe a matrix form of the model that can be easily coded in any programming language. This formalism has great flexibility, which we show by illustrating how phenomena such as competition between transcription factors and cooperativity are readily incorporated into these models. Using this framework, we also demonstrate that Michaelis-like functions, another class of cis-regulatory models, are a subset of the thermodynamic framework with specific assumptions. By recasting Michaelis-like functions as thermodynamic functions, we emphasize the relationship between these models and delineate the specific circumstances representable by each approach. Application of thermodynamic state ensemble models is likely to be an important tool in unraveling the physical basis of combinatorial cis-regulation and in generating formalisms that accurately predict gene expression from DNA sequence.",2012,PLoS Comput. Biol.
Modeling Planarian Regeneration: A Primer for Reverse-Engineering the Worm,"A mechanistic understanding of robust self-assembly and repair capabilities of complex systems would have enormous implications for basic evolutionary developmental biology as well as for transformative applications in regenerative biomedicine and the engineering of highly fault-tolerant cybernetic systems. Molecular biologists are working to identify the pathways underlying the remarkable regenerative abilities of model species that perfectly regenerate limbs, brains, and other complex body parts. However, a profound disconnect remains between the deluge of high-resolution genetic and protein data on pathways required for regeneration, and the desired spatial, algorithmic models that show how self-monitoring and growth control arise from the synthesis of cellular activities. This barrier to progress in the understanding of morphogenetic controls may be breached by powerful techniques from the computational sciences—using non-traditional modeling approaches to reverse-engineer systems such as planaria: flatworms with a complex bodyplan and nervous system that are able to regenerate any body part after traumatic injury. Currently, the involvement of experts from outside of molecular genetics is hampered by the specialist literature of molecular developmental biology: impactful collaborations across such different fields require that review literature be available that presents the key functional capabilities of important biological model systems while abstracting away from the often irrelevant and confusing details of specific genes and proteins. To facilitate modeling efforts by computer scientists, physicists, engineers, and mathematicians, we present a different kind of review of planarian regeneration. Focusing on the main patterning properties of this system, we review what is known about the signal exchanges that occur during regenerative repair in planaria and the cellular mechanisms that are thought to underlie them. By establishing an engineering-like style for reviews of the molecular developmental biology of biomedically important model systems, significant fresh insights and quantitative computational models will be developed by new collaborations between biology and the information sciences.",2012,PLoS Comput. Biol.
Rise and Demise of Bioinformatics? Promise and Progress,"The field of bioinformatics and computational biology has gone through a number of transformations during the past 15 years, establishing itself as a key component of new biology. This spectacular growth has been challenged by a number of disruptive changes in science and technology. Despite the apparent fatigue of the linguistic use of the term itself, bioinformatics has grown perhaps to a point beyond recognition. We explore both historical aspects and future trends and argue that as the field expands, key questions remain unanswered and acquire new meaning while at the same time the range of applications is widening to cover an ever increasing number of biological disciplines. These trends appear to be pointing to a redefinition of certain objectives, milestones, and possibly the field itself.",2012,PLoS Comput. Biol.
Parameter uncertainty quantification using surrogate models applied to a spatial model of yeast mating polarization,"A common challenge in systems biology is quantifying the effects of unknown parameters and estimating parameter values from data. For many systems, this task is computationally intractable due to expensive model evaluations and large numbers of parameters. In this work, we investigate a new method for performing sensitivity analysis and parameter estimation of complex biological models using techniques from uncertainty quantification. The primary advance is a significant improvement in computational efficiency from the replacement of model simulation by evaluation of a polynomial surrogate model. We demonstrate the method on two models of mating in budding yeast: a smaller ODE model of the heterotrimeric G-protein cycle, and a larger spatial model of pheromone-induced cell polarization. A small number of model simulations are used to fit the polynomial surrogates, which are then used to calculate global parameter sensitivities. The surrogate models also allow rapid Bayesian inference of the parameters via Markov chain Monte Carlo (MCMC) by eliminating model simulations at each step. Application to the ODE model shows results consistent with published single-point estimates for the model and data, with the added benefit of calculating the correlations between pairs of parameters. On the larger PDE model, the surrogate models allowed convergence for the distribution of 15 parameters, which otherwise would have been computationally prohibitive using simulations at each MCMC step. We inferred parameter distributions that in certain cases peaked at values different from published values, and showed that a wide range of parameters would permit polarization in the model. Strikingly our results suggested different diffusion constants for active versus inactive Cdc42 to achieve good polarization, which is consistent with experimental observations in another yeast species S. pombe.",2018,PLoS Comput. Biol.
Efficient Reverse-Engineering of a Developmental Gene Regulatory Network,"Understanding the complex regulatory networks underlying development and evolution of multi-cellular organisms is a major problem in biology. Computational models can be used as tools to extract the regulatory structure and dynamics of such networks from gene expression data. This approach is called reverse engineering. It has been successfully applied to many gene networks in various biological systems. However, to reconstitute the structure and non-linear dynamics of a developmental gene network in its spatial context remains a considerable challenge. Here, we address this challenge using a case study: the gap gene network involved in segment determination during early development of Drosophila melanogaster. A major problem for reverse-engineering pattern-forming networks is the significant amount of time and effort required to acquire and quantify spatial gene expression data. We have developed a simplified data processing pipeline that considerably increases the throughput of the method, but results in data of reduced accuracy compared to those previously used for gap gene network inference. We demonstrate that we can infer the correct network structure using our reduced data set, and investigate minimal data requirements for successful reverse engineering. Our results show that timing and position of expression domain boundaries are the crucial features for determining regulatory network structure from data, while it is less important to precisely measure expression levels. Based on this, we define minimal data requirements for gap gene network inference. Our results demonstrate the feasibility of reverse-engineering with much reduced experimental effort. This enables more widespread use of the method in different developmental contexts and organisms. Such systematic application of data-driven models to real-world networks has enormous potential. Only the quantitative investigation of a large number of developmental gene regulatory networks will allow us to discover whether there are rules or regularities governing development and evolution of complex multi-cellular organisms.",2012,PLoS Comput. Biol.
Fostering bioinformatics education through skill development of professors: Big Genomic Data Skills Training for Professors,"Bioinformatics has become an indispensable part of life science over the past 2 decades. However, bioinformatics education is not well integrated at the undergraduate level, especially in liberal arts colleges and regional universities in the United States. One significant obstacle pointed out by the Network for Integrating Bioinformatics into Life Sciences Education is the lack of faculty in the bioinformatics area. Most current life science professors did not acquire bioinformatics analysis skills during their own training. Consequently, a great number of undergraduate and graduate students do not get the chance to learn bioinformatics or computational biology skills within a structured curriculum during their education. To address this gap, we developed a module-based, week-long short course to train small college and regional university professors with essential bioinformatics skills. The bioinformatics modules were built to be adapted by the professor-trainees afterward and used in their own classes. All the course materials can be accessed at https://github.com/TheJacksonLaboratory/JAXBD2K-ShortCourse.",2019,PLoS Comput. Biol.
"The Rough Guide to In Silico Function Prediction, or How To Use Sequence and Structure Information To Predict Protein Function","Choosing the right function prediction tools 
The vast majority of known proteins have not yet been characterized experimentally, and there is very little that is known about their function. New unannotated sequences are added to the databases at a pace that far exceeds the one in which they are annotated in the lab. Computational biology offers tools that can provide insight into the function of proteins based on their sequence, their structure, their evolutionary history, and their association with other proteins. In this contribution, we attempt to provide a framework that will enable biologists and computational biologists to decide which type of computational tool is appropriate for the analysis of their protein of interest, and what kind of insights into its function these tools can provide. In particular, we describe computational methods for predicting protein function directly from sequence or structure, focusing mainly on methods for predicting molecular function. We do not discuss methods that rely on sources of information that are beyond the protein itself, such as genomic context [1], protein–protein interaction networks [2], or membership in biochemical pathways [3]. When choosing a tool for function prediction, one would typically want to identify the best performing tool. However, a quantitative comparison of different tools is a tricky task. While most developers report their own assessment of their tool, in most cases there are no standard datasets and generally agreed-upon measures and criteria for benchmarking function prediction methods. In the absence of independent benchmarks, comparing the figures reported by the developers is almost always comparing oranges and apples (for discussion of this problem see [4]). Therefore, we refrain from reporting numerical assessments of specific methods. For those cases in which independent assessment of performance is available, we refer the reader to the original publications. Finally, we discuss only methods that are either accessible as Web servers or freely available for download (relevant Web links can be found in Table S1).",2008,PLoS Comput. Biol.
Ten simple rules for biologists initiating a collaboration with computer scientists,"Biology is increasingly digital, and scientists are generating huge amounts of data daily, turning molecules into sequences and text files. As a biologist, you might need help analyzing all these data and have considered collaborating with a computer scientist for the first, second, or third time. This person might have some training in computational biology, but their main focus has always been computer science (CS), and here is the challenge: how do you talk to them? They might be able to write efficient code, but they often do not know some of the basics of biology. When they look at your molecules, some of them might see text files before biology. Also, if explaining things takes so much time, is it worth it? Should you be analyzing your own data instead? Or perhaps you have noticed that all those big, shiny papers of today represent a smart blend of biology and CS. You have found a collaborator and want to learn how to engage them. These 10 simple rules aim to help.",2020,PLoS Comput. Biol.
The Mycobacterium tuberculosis Drugome and Its Polypharmacological Implications,"We report a computational approach that integrates structural bioinformatics, molecular modelling and systems biology to construct a drug-target network on a structural proteome-wide scale. The approach has been applied to the genome of Mycobacterium tuberculosis (M.tb), the causative agent of one of today's most widely spread infectious diseases. The resulting drug-target interaction network for all structurally characterized approved drugs bound to putative M.tb receptors, we refer to as the ‘TB-drugome’. The TB-drugome reveals that approximately one-third of the drugs examined have the potential to be repositioned to treat tuberculosis and that many currently unexploited M.tb receptors may be chemically druggable and could serve as novel anti-tubercular targets. Furthermore, a detailed analysis of the TB-drugome has shed new light on the controversial issues surrounding drug-target networks [1]–[3]. Indeed, our results support the idea that drug-target networks are inherently modular, and further that any observed randomness is mainly caused by biased target coverage. The TB-drugome (http://funsite.sdsc.edu/drugome/TB) has the potential to be a valuable resource in the development of safe and efficient anti-tubercular drugs. More generally the methodology may be applied to other pathogens of interest with results improving as more of their structural proteomes are determined through the continued efforts of structural biology/genomics.",2010,PLoS Comput. Biol.
Efficient Characterization of Parametric Uncertainty of Complex (Bio)chemical Networks,"Parametric uncertainty is a particularly challenging and relevant aspect of systems analysis in domains such as systems biology where, both for inference and for assessing prediction uncertainties, it is essential to characterize the system behavior globally in the parameter space. However, current methods based on local approximations or on Monte-Carlo sampling cope only insufficiently with high-dimensional parameter spaces associated with complex network models. Here, we propose an alternative deterministic methodology that relies on sparse polynomial approximations. We propose a deterministic computational interpolation scheme which identifies most significant expansion coefficients adaptively. We present its performance in kinetic model equations from computational systems biology with several hundred parameters and state variables, leading to numerical approximations of the parametric solution on the entire parameter space. The scheme is based on adaptive Smolyak interpolation of the parametric solution at judiciously and adaptively chosen points in parameter space. As Monte-Carlo sampling, it is “non-intrusive” and well-suited for massively parallel implementation, but affords higher convergence rates. This opens up new avenues for large-scale dynamic network analysis by enabling scaling for many applications, including parameter estimation, uncertainty quantification, and systems design.",2015,PLoS Comput. Biol.
The Quantitative Methods Boot Camp: Teaching Quantitative Thinking and Computing Skills to Graduate Students in the Life Sciences,"The past decade has seen a rapid increase in the ability of biologists to collect large amounts of data. It is therefore vital that research biologists acquire the necessary skills during their training to visualize, analyze, and interpret such data. To begin to meet this need, we have developed a “boot camp” in quantitative methods for biology graduate students at Harvard Medical School. The goal of this short, intensive course is to enable students to use computational tools to visualize and analyze data, to strengthen their computational thinking skills, and to simulate and thus extend their intuition about the behavior of complex biological systems. The boot camp teaches basic programming using biological examples from statistics, image processing, and data analysis. This integrative approach to teaching programming and quantitative reasoning motivates students’ engagement by demonstrating the relevance of these skills to their work in life science laboratories. Students also have the opportunity to analyze their own data or explore a topic of interest in more detail. The class is taught with a mixture of short lectures, Socratic discussion, and in-class exercises. Students spend approximately 40% of their class time working through both short and long problems. A high instructor-to-student ratio allows students to get assistance or additional challenges when needed, thus enhancing the experience for students at all levels of mastery. Data collected from end-of-course surveys from the last five offerings of the course (between 2012 and 2014) show that students report high learning gains and feel that the course prepares them for solving quantitative and computational problems they will encounter in their research. We outline our course here which, together with the course materials freely available online under a Creative Commons License, should help to facilitate similar efforts by others.",2015,PLoS Comput. Biol.
The Signaling Petri Net-Based Simulator: A Non-Parametric Strategy for Characterizing the Dynamics of Cell-Specific Signaling Networks,"Reconstructing cellular signaling networks and understanding how they work are major endeavors in cell biology. The scale and complexity of these networks, however, render their analysis using experimental biology approaches alone very challenging. As a result, computational methods have been developed and combined with experimental biology approaches, producing powerful tools for the analysis of these networks. These computational methods mostly fall on either end of a spectrum of model parameterization. On one end is a class of structural network analysis methods; these typically use the network connectivity alone to generate hypotheses about global properties. On the other end is a class of dynamic network analysis methods; these use, in addition to the connectivity, kinetic parameters of the biochemical reactions to predict the network's dynamic behavior. These predictions provide detailed insights into the properties that determine aspects of the network's structure and behavior. However, the difficulty of obtaining numerical values of kinetic parameters is widely recognized to limit the applicability of this latter class of methods. Several researchers have observed that the connectivity of a network alone can provide significant insights into its dynamics. Motivated by this fundamental observation, we present the signaling Petri net, a non-parametric model of cellular signaling networks, and the signaling Petri net-based simulator, a Petri net execution strategy for characterizing the dynamics of signal flow through a signaling network using token distribution and sampling. The result is a very fast method, which can analyze large-scale networks, and provide insights into the trends of molecules' activity-levels in response to an external stimulus, based solely on the network's connectivity. We have implemented the signaling Petri net-based simulator in the PathwayOracle toolkit, which is publicly available at http://bioinfo.cs.rice.edu/pathwayoracle. Using this method, we studied a MAPK1,2 and AKT signaling network downstream from EGFR in two breast tumor cell lines. We analyzed, both experimentally and computationally, the activity level of several molecules in response to a targeted manipulation of TSC2 and mTOR-Raptor. The results from our method agreed with experimental results in greater than 90% of the cases considered, and in those where they did not agree, our approach provided valuable insights into discrepancies between known network connectivities and experimental observations.",2008,PLoS Comput. Biol.
De-Novo Discovery of Differentially Abundant Transcription Factor Binding Sites Including Their Positional Preference,"Transcription factors are a main component of gene regulation as they activate or repress gene expression by binding to specific binding sites in promoters. The de-novo discovery of transcription factor binding sites in target regions obtained by wet-lab experiments is a challenging problem in computational biology, which has not been fully solved yet. Here, we present a de-novo motif discovery tool called Dispom for finding differentially abundant transcription factor binding sites that models existing positional preferences of binding sites and adjusts the length of the motif in the learning process. Evaluating Dispom, we find that its prediction performance is superior to existing tools for de-novo motif discovery for 18 benchmark data sets with planted binding sites, and for a metazoan compendium based on experimental data from micro-array, ChIP-chip, ChIP-DSL, and DamID as well as Gene Ontology data. Finally, we apply Dispom to find binding sites differentially abundant in promoters of auxin-responsive genes extracted from Arabidopsis thaliana microarray data, and we find a motif that can be interpreted as a refined auxin responsive element predominately positioned in the 250-bp region upstream of the transcription start site. Using an independent data set of auxin-responsive genes, we find in genome-wide predictions that the refined motif is more specific for auxin-responsive genes than the canonical auxin-responsive element. In general, Dispom can be used to find differentially abundant motifs in sequences of any origin. However, the positional distribution learned by Dispom is especially beneficial if all sequences are aligned to some anchor point like the transcription start site in case of promoter sequences. We demonstrate that the combination of searching for differentially abundant motifs and inferring a position distribution from the data is beneficial for de-novo motif discovery. Hence, we make the tool freely available as a component of the open-source Java framework Jstacs and as a stand-alone application at http://www.jstacs.de/index.php/Dispom.",2011,PLoS Comput. Biol.
Soft Skills: An Important Asset Acquired from Organizing Regional Student Group Activities,"Contributing to a student organization, such as the International Society for Computational Biology Student Council (ISCB-SC) and its Regional Student Group (RSG) program, takes time and energy. Both are scarce commodities, especially when you are trying to find your place in the world of computational biology as a graduate student. It comes as no surprise that organizing ISCB-SC-related activities sometimes interferes with day-to-day research and shakes up your priority list. However, we unanimously agree that the rewards, both in the short as well as the long term, make the time spent on these extracurricular activities more than worth it. In this article, we will explain what makes this so worthwhile: soft skills.",2014,PLoS Comput. Biol.
Modeling Somatic Evolution in Tumorigenesis,"Tumorigenesis in humans is thought to be a multistep process where certain mutations confer a selective advantage, allowing lineages derived from the mutated cell to outcompete other cells. Although molecular cell biology has substantially advanced cancer research, our understanding of the evolutionary dynamics that govern tumorigenesis is limited. This paper analyzes the computational implications of cancer progression presented by Hanahan and Weinberg in The Hallmarks of Cancer. We model the complexities of tumor progression as a small set of underlying rules that govern the transformation of normal cells to tumor cells. The rules are implemented in a stochastic multistep model. The model predicts that (i) early-onset cancers proceed through a different sequence of mutation acquisition than late-onset cancers; (ii) tumor heterogeneity varies with acquisition of genetic instability, mutation pathway, and selective pressures during tumorigenesis; (iii) there exists an optimal initial telomere length which lowers cancer incidence and raises time of cancer onset; and (iv) the ability to initiate angiogenesis is an important stage-setting mutation, which is often exploited by other cells. The model offers insight into how the sequence of acquired mutations affects the timing and cellular makeup of the resulting tumor and how the cellular-level population dynamics drive neoplastic evolution.",2006,PLoS Comput. Biol.
Modeling Structure-Function Relationships in Synthetic DNA Sequences using Attribute Grammars,"Recognizing that certain biological functions can be associated with specific DNA sequences has led various fields of biology to adopt the notion of the genetic part. This concept provides a finer level of granularity than the traditional notion of the gene. However, a method of formally relating how a set of parts relates to a function has not yet emerged. Synthetic biology both demands such a formalism and provides an ideal setting for testing hypotheses about relationships between DNA sequences and phenotypes beyond the gene-centric methods used in genetics. Attribute grammars are used in computer science to translate the text of a program source code into the computational operations it represents. By associating attributes with parts, modifying the value of these attributes using rules that describe the structure of DNA sequences, and using a multi-pass compilation process, it is possible to translate DNA sequences into molecular interaction network models. These capabilities are illustrated by simple example grammars expressing how gene expression rates are dependent upon single or multiple parts. The translation process is validated by systematically generating, translating, and simulating the phenotype of all the sequences in the design space generated by a small library of genetic parts. Attribute grammars represent a flexible framework connecting parts with models of biological function. They will be instrumental for building mathematical models of libraries of genetic constructs synthesized to characterize the function of genetic parts. This formalism is also expected to provide a solid foundation for the development of computer assisted design applications for synthetic biology.",2009,PLoS Comput. Biol.
"A demonstration of modularity, reuse, reproducibility, portability and scalability for modeling and simulation of cardiac electrophysiology using Kepler Workflows","Multi-scale computational modeling is a major branch of computational biology as evidenced by the US federal interagency Multi-Scale Modeling Consortium and major international projects. It invariably involves specific and detailed sequences of data analysis and simulation, often with multiple tools and datasets, and the community recognizes improved modularity, reuse, reproducibility, portability and scalability as critical unmet needs in this area. Scientific workflows are a well-recognized strategy for addressing these needs in scientific computing. While there are good examples if the use of scientific workflows in bioinformatics, medical informatics, biomedical imaging and data analysis, there are fewer examples in multi-scale computational modeling in general and cardiac electrophysiology in particular. Cardiac electrophysiology simulation is a mature area of multi-scale computational biology that serves as an excellent use case for developing and testing new scientific workflows. In this article, we develop, describe and test a computational workflow that serves as a proof of concept of a platform for the robust integration and implementation of a reusable and reproducible multi-scale cardiac cell and tissue model that is expandable, modular and portable. The workflow described leverages Python and Kepler-Python actor for plotting and pre/post-processing. During all stages of the workflow design, we rely on freely available open-source tools, to make our workflow freely usable by scientists.",2019,PLoS Comput. Biol.
A Quick Guide to Genomics and Bioinformatics Training for Clinical and Public Audiences,"Traditionally, bioinformatics tools and training programs have focused on life science audiences. Though heterogeneous, their needs are at least fairly well understood. Driven by the impact of technology in diverse areas, bioinformatics is becoming increasingly interdisciplinary, and, in parallel, so too are the audiences seeking bioinformatics training. Audiences as disparate as physicians and lawyers, industry, and even the general public, previously without real need of bioinformatics skills or awareness, are now pursuing an understanding of and skill sets in bioinformatics. 
 
These audiences represent a new and exciting challenge for bioinformatics training programs. A recent workshop at ISMB/ECCB (Intelligent Systems for Molecular Biology/European Conference on Computational Biology) 2013, “Workshop on Education in Bioinformatics 2013” (WEB2013), discussed opportunities and bioinformatics training strategies for emerging clinical and public audiences [1]. The aim of this Quick Guide is to share our guidelines for core bioinformatics skills and training requirements with bioinformatics educators and trainers who are already involved in or are thinking about developing and delivering bioinformatics programs to these audiences.",2014,PLoS Comput. Biol.
Time to Organize the Bioinformatics Resourceome,"T he field of bioinformatics has blossomed in the last ten years, and as a result, there is a large and increasing number of researchers generating computational tools for solving problems relevant to biology. Because the number of artifacts has increased greatly, it is impossible for many bioinformatics researchers to track tools, databases, and methods in the field—or even perhaps within their own specialty area. More critically, however, biologist users and scientists approaching the field do not have a comprehensive index of bioinformatics algorithms, databases, and literature annotated with information about their context and appropriate use. We suggest that the full set of bioinformatics resources—the ‘‘resourceome’’—should be explicitly characterized and organized. A hierarchical and machine-understandable organization of the field, along with rich cross-links (an ontology!) would be a useful start. It is likely that a distributed development approach would be required so that those with focused expertise can classify resources in their area, while providing the metadata that would allow easier access to useful existing resources. The growth of bioinformatics can be quantified in many ways. The Intelligent Systems for Molecular Biology Meeting began in 1993, and numerous other meetings have been established. The International Society for Computational Biology (ISCB) was formed in 1995, and recent membership numbers have reached 2,000. The field has gone from having one or two journals to having more than a dozen—if one considers ‘‘-omics’’ (i.e., subjects relating to high-throughput functional genomics, where computation plays a central role) and the emerging field of systems biology. Because bioinformatics has a strong element of engineering, the creation and maintenance of tools provide value only insofar as they are used. These tools may be databases that hold biological data, or they may be algorithms that act on this data to draw inferences. Access to these artifacts is currently uneven. Of course, the published literature is the archival resting place for the initial description of these innovations, but it only contains a snapshot of most tools early in their lifetime. The literature does not use any standard classification system to describe tools, so the sensitivity of searches for specific functions is not generally high. Indeed, the bibliome itself is idiosyncratically organized, and finding the right article is often like searching for a needle in a haystack [2]. Finally, the published literature does not contain reliable references to the location and to the availability of most bioinformatics resources [3,4]. One could also argue that Google (http:// www.google.com) provides adequate access to tools based on keyword searching [5]. However, the lack of standard terms makes sensitive and specific searches difficult. In addition, most search hits confound papers, Web sites, tools, departments, and people in a manner that makes extracting useful information very difficult. Recognizing this limitation, there have been some grassroots attempts to organize the bioinformatics resourceome. Among the most famous are the ‘‘archaeological’’ Pedro’s List—a list of computer tools for molecular biologists (http://www.public.iastate.edu/;pedro/ research_tools.html)—and the Expasy Life Sciences Directory, formerly known as the Amos’s WWW links page (http://www.expasy.org/links.html). The Bioinformatics Links Directory (http://www.bioinformatics.ubc.ca/resources/ links_directory/) today contains more than 700 curated links to bioinformatics resources, organized into eleven main categories, including all the databases and Web servers yearly listed in the dedicated Nucleic Acids Research special issues [6]. The National Center for Biotechnology Institute has tried to make access to its suite of tools transparent, with moderate success. Many Web sites can be found listing ‘‘useful sites,’’ especially concerning special interest or limited topics (e.g., microarrays, text mining, and gene regulation). But all of these efforts are limited by the difficulty in maintaining currency and by the lack of a uniformly recognized classification scheme. Yet our colleagues in bioinformatics and biology are constantly asking about the availability of tools or databases with certain characteristics. The lack of a useful index, thus, routinely costs time and opportunities. In addition, there is no ‘‘peer-review’’ system for bioinformatics tools so that the most useful ones can be highlighted by happy users. A secure and reliable system for rating (similar to that used by Amazon.com, for example) would also be an important prerequisite. An ‘‘ontology’’ is a specification of a conceptual space, often used by computer programs. The field of ontology",2005,PLoS Comput. Biol.
A Novel Bayesian DNA Motif Comparison Method for Clustering and Retrieval,"Characterizing the DNA-binding specificities of transcription factors is a key problem in computational biology that has been addressed by multiple algorithms. These usually take as input sequences that are putatively bound by the same factor and output one or more DNA motifs. A common practice is to apply several such algorithms simultaneously to improve coverage at the price of redundancy. In interpreting such results, two tasks are crucial: clustering of redundant motifs, and attributing the motifs to transcription factors by retrieval of similar motifs from previously characterized motif libraries. Both tasks inherently involve motif comparison. Here we present a novel method for comparing and merging motifs, based on Bayesian probabilistic principles. This method takes into account both the similarity in positional nucleotide distributions of the two motifs and their dissimilarity to the background distribution. We demonstrate the use of the new comparison method as a basis for motif clustering and retrieval procedures, and compare it to several commonly used alternatives. Our results show that the new method outperforms other available methods in accuracy and sensitivity. We incorporated the resulting motif clustering and retrieval procedures in a large-scale automated pipeline for analyzing DNA motifs. This pipeline integrates the results of various DNA motif discovery algorithms and automatically merges redundant motifs from multiple training sets into a coherent annotated library of motifs. Application of this pipeline to recent genome-wide transcription factor location data in S. cerevisiae successfully identified DNA motifs in a manner that is as good as semi-automated analysis reported in the literature. Moreover, we show how this analysis elucidates the mechanisms of condition-specific preferences of transcription factors.",2008,PLoS Comput. Biol.
Ten Simple Rules for Getting Published,The student council (http://www. iscbsc.org/) of the International Society for Computational Biology asked me to present my thoughts on getting published in the field of computational biology at the Intelligent Systems in Molecular Biology conference held in Detroit in late June of 2005. Close to 200 bright young souls (and a few not so young) crammed into a small room for what proved to be a wonderful interchange among a group of whom approximately one-half had yet to publish their first paper. The advice I gave that day I have modified and present as ten rules for getting published.,2005,PLoS Comput. Biol.
Genome Landscapes of Disease: Strategies to Predict the Phenotypic Consequences of Human Germline and Somatic Variation,"Computational biology can marry disciplines to help solve some of the most pressing problems in medical research. When built on fundamental evolutionary, biological, and/or physics principles and provided with large quantities of diverse experimental data, computing power, and rigorous statistics tools, efficient and effective computational strategies can help unlock the secrets of the genome to cure disease. Computational biology has undertaken this challenge, spearheading efforts to decode the genetic blueprints encrypted in the human germline and in somatic variations to decipher complex phenotypic consequences. It devises new and creative approaches to sift through the immense—and rapidly growing—assembled human genetic material, its products, the proteome and the metabolome, and their possible clinical ramifications. 
 
The cell is the basic unit of life; the nucleus houses the genetic material that is believed to have provided a distinct advantage to the evolving cell. The organization of the genome varies; it depends on cell type, stage of development, differentiation, disease status, and more. The higher-order spatial and temporal organization of genomes—which itself is a function of conditions and environment—is a driver of biological function in differentiation, development, and disease. Studies of genomics, epigenetics, big data analysis, imaging, and clinical cell and molecular biology all benefit from rigorous computational biology analyses and modeling. They profit from the testable hypotheses that computational biology provides. These link the genetic material to the physiological cell state; however, in-depth understanding of—and predicting—the phenotypic relationship to genome landscapes also requires effective algorithms to unravel the outcomes of small- and large-scale alterations on DNA, RNA, and protein molecules. 
 
Diseases can be viewed as perturbed states of molecular systems. They can be of different types, including single-gene (monogenic) diseases and multifactorial diseases, such as cancers, immune system diseases, neurodegenerative diseases, cardiovascular diseases, and metabolic diseases. They may involve genetic alterations or be more complex. They can also be infectious diseases where interacting molecular networks of both pathogens and humans are involved, with the pathogen protein subverting the cell’s machinery. 
 
The rapid development of next-generation methods for whole-genome, whole-exome, and targeted sequencing, complemented by earlier microarray technologies, including comparative genomic hybridization and single nucleotide polymorphism (SNP) genotyping arrays, has generated an unprecedented amount of data for analysis. The unraveling of the more-frequent-than-expected scope of submicroscopic copy-number variations (CNVs) and their link to structural variation and disease has yielded discoveries of novel phenotype associations. It is now well known that CNVs, as well as SNPs, are responsible for human evolution, genetic diversity, and susceptibility to genomic disorders. Some genomic disorders result from structural changes of the human genome that convey traits or susceptibility to traits. Such rearrangements are believed to occur because of architectural features of the genome that abet genome instability. Genome-wide association studies have revealed common variation, and targeted sequencing has provided fine mapping of the genomic regions surrounding common variants. New methods have been developed to detect and assess rare variant associations, enhancing the understanding of the genetic architecture of disease. 
 
The most common form of germline intra-species variation is the single nucleotide variant (SNV). Recent sequencing of whole human genomes has identified the number of SNVs in each individual to be in the range of 3–5 million. Although most SNVs are likely to be benign, some SNVs have a pathogenic effect and thus directly contribute to disease susceptibilities and drug sensitivities. Discovering these pathogenic SNVs is one of the main goals of modern genetics and genomics studies. Large-scale sequencing of cancer genomes has further uncovered thousands of somatic DNA alterations present in the tumor cells but not in the germline DNA of sequenced individuals. The implications of these alterations, which include SNVs, copy number changes, and structural rearrangements, on patient diagnosis, prognosis, and treatment regimens have become one of the central issues in 21st century cancer biology and medicine. 
 
The number of genomic alterations discovered by next-generation sequencing methods is too large to be directly assessed by wet bench methods, such as cell culture experiments and animal models, or by statistical associations in case-control or family-based study designs. Computational biology is increasingly playing a role in prioritizing genomic alterations most likely to be pathogenic or of clinical relevance in cancer because computer algorithms can perform high-throughput analysis of large datasets. Computer models can also integrate genomic data with information about gene expression, methylation, regulatory and protein interaction networks, as well as clinical and imaging data. 
 
There is continuous increase in the scale of genomic datasets as well as in the difficulty of identifying functionally and clinically relevant genetic variation. Among the many challenges posed to computational biology is to push the boundaries of what we might be able to learn from the genome alone while considering its cell/tissue environment, physiological state, and occurrence in disease. If significant strides are made and specific associations and linkages are identified, progress will be made toward sufficiently robust predictions with rewarding prognostic medical implications. 
 
PLOS Computational Biology has assembled a Focus Feature on the genome landscapes of disease: strategies to predict the phenotypic consequences of human germline and somatic variation. International efforts to sequence large patient cohorts and healthy control populations are generating exponentially increasing amounts of genomic data. Without the ability to interpret the phenotypic consequences of germline and somatic variation, this data collection will have limited value. We hope that this Focus Feature will inform the readership of PLOS Computational Biology about current state-of-the-art of computational methods to predict these consequences, provide perspective on key successes and limitations in this area to date, and point to areas where progress is urgently needed. 
 
The Focus Feature includes three papers. In the first, Yoo-Ah Kim, Dong-Yeon Cho, and Teresa Przytycka point to genotype-phenotype effects in cancer, which can be revealed via network approaches. The authors argue that cancer is a complex and heterogeneous disease and no two cancer cases are identical. Capturing the similarities as well as the differences is essential for a better understanding of the disease and its treatment. The authors review network-based approaches to cancer data analysis, focusing on their roles in modeling inter-tumor heterogeneity. 
 
Jonas Reeb, Maximilian Hecht, Yannick Mahlich, Yana Bromberg, and Burkhard Rost describe predictions of molecular effects of sequence variants to bridge the gap from the micro level of molecular function to the systems/macro level of disease. David Masica and Rachel Karchin emphasize the importance of increasing the clinical relevance of in silico methods to predict pathogenic missense variants. They highlight the rapid accumulation of variants of unknown clinical significance, most of which cause amino acid substitutions (cSNVs). To interpret these variants, there is an urgent need to develop better in silico bioinformatic methods. They critically review the current state of the field, pointing to the progress and shortcomings in the development of bioinformatics missense variant classifiers. In particular, they advocate the increased use of endophenotypes, which are quantitative measurements that are correlated with phenotypes via shared genetic causes. 
 
Computational biology can tackle cancer through analysis of massive quantities of data. These analyses may allow correlating the data with phenotypic outcomes. Voluminous compilations permit reaping robust statistical trends, which can be exploited for predictions. Computation can also contribute to cancer research by unraveling the mechanisms through which particular genetic or acquired aberrations actually work. Molecular structures can contribute greatly to understanding of disease mechanisms and the design and computational screening of drugs. By combining data from genomics and molecular structure, computational biology can enable interpretation of genetic and epigenetic data to tailor disease treatments to individual patients. Together with experiments, the physical sciences, genetics, statistical “big data,” and clinical analysis, computational biology can help to lay the foundation for new paradigms in the translational sciences. We look forward to the time when these tools can elucidate the basis of cancer and provide predictive blueprints and combinatorial drug therapies. However, the challenge facing us is daunting.",2016,PLoS Comput. Biol.
The 5th ISCB Wikipedia Competition: Coming to a Classroom Near You?,"The International Society for Computational Biology (ISCB) is pleased to announce the 5th ISCB Wikipedia Competition. The competition has been run annually since 2012 and awards students and trainees for the best contributions to computational biology-related articles [1]. ISCB runs the competition in collaboration with WikiProject Computational Biology, a group of around 130 editors overseeing the roughly 1,300 computational biology-related articles on Wikipedia. Entry to the competition is open internationally to students and trainees of any level, both as individuals and as groups. Further information about the competition can be found here: https://en.wikipedia.org/wiki/WP:ISCB2016. The 2016 competition began on 13 July 2016, coinciding with the Intelligent Systems for Molecular Biology (ISMB) conference, and will end on 31 December 2016. Each article entered in the competition will be reviewed by students nominated by the ISCB Student Council, and a shortlist of entries will be examined by a judging panel. The winners will be presented with their awards at ISMB/ECCB 2017 in Prague. As of the 2015 competition, entries in languages other than English are welcomed; the 2015 competition saw the creation and expansion of several articles in Spanish, and the newly-created Spanish Wikipedia entry for BioJava was awarded second prize. The 2015 competition was also the first to include a Wikidata component. Similar to Wikipedia, Wikidata is a collaboratively edited knowledge base, intended to provide a common source of data to be used primarily by Wikipedia but also the general public. Launched in 2012, Wikidata plays an increasingly important role in communicating all types of science to the public, and the inclusion of a Wikidata component in the competition aims to improve data content related to computational biology. In the 2015 competition, prizes were awarded for contributions to the Vienna RNA Package and Docking Wikidata items. We hope to see an increased number of contributions in other languages and to Wikidata content in this year’s competition. As the 2016 competition begins, the ISCB and WikiProject Computational Biology remain keen to grow the quality and depth of computational biology coverage over all Wikimedia projects and wish to encourage the widest possible range of students and trainees to participate. We particularly encourage teachers, tutors, and lecturers to use the competition in their courses, with contributions to relevant Wikipedia or Wikidata content as part of a class assignment. Wikipedia provides information and resources for running editing projects as part of school and university classes (see https://en.wikipedia.org/wiki/WP:SUP), and PLOS Computational Biology has also published tips for Wikipedia beginners [2]. As described previously, unlike traditional class assignments, contributing to Wikipedia means that students’ work will become publicly available to future researchers. Several courses already contain such a component; a 2015 course on Biological Clocks at Washington University in St. Louis, Missouri added a significant amount of information on the topic of chronobiology, including new biography articles on notable researchers in the field. Besides the",2016,PLoS Comput. Biol.
A Scalable Algorithm to Explore the Gibbs Energy Landscape of Genome-Scale Metabolic Networks,"The integration of various types of genomic data into predictive models of biological networks is one of the main challenges currently faced by computational biology. Constraint-based models in particular play a key role in the attempt to obtain a quantitative understanding of cellular metabolism at genome scale. In essence, their goal is to frame the metabolic capabilities of an organism based on minimal assumptions that describe the steady states of the underlying reaction network via suitable stoichiometric constraints, specifically mass balance and energy balance (i.e. thermodynamic feasibility). The implementation of these requirements to generate viable configurations of reaction fluxes and/or to test given flux profiles for thermodynamic feasibility can however prove to be computationally intensive. We propose here a fast and scalable stoichiometry-based method to explore the Gibbs energy landscape of a biochemical network at steady state. The method is applied to the problem of reconstructing the Gibbs energy landscape underlying metabolic activity in the human red blood cell, and to that of identifying and removing thermodynamically infeasible reaction cycles in the Escherichia coli metabolic network (iAF1260). In the former case, we produce consistent predictions for chemical potentials (or log-concentrations) of intracellular metabolites; in the latter, we identify a restricted set of loops (23 in total) in the periplasmic and cytoplasmic core as the origin of thermodynamic infeasibility in a large sample () of flux configurations generated randomly and compatibly with the prior information available on reaction reversibility.",2012,PLoS Comput. Biol.
Deciphering Diseases and Biological Targets for Environmental Chemicals using Toxicogenomics Networks,"Exposure to environmental chemicals and drugs may have a negative effect on human health. A better understanding of the molecular mechanism of such compounds is needed to determine the risk. We present a high confidence human protein-protein association network built upon the integration of chemical toxicology and systems biology. This computational systems chemical biology model reveals uncharacterized connections between compounds and diseases, thus predicting which compounds may be risk factors for human health. Additionally, the network can be used to identify unexpected potential associations between chemicals and proteins. Examples are shown for chemicals associated with breast cancer, lung cancer and necrosis, and potential protein targets for di-ethylhexyl-phthalate, 2,3,7,8-tetrachlorodibenzo-p-dioxin, pirinixic acid and permethrine. The chemical-protein associations are supported through recent published studies, which illustrate the power of our approach that integrates toxicogenomics data with other data types.",2010,PLoS Comput. Biol.
Getting Started in Probabilistic Graphical Models,"Probabilistic graphical models offer a common conceptual architecture where biological and mathematical objects can be expressed with a common, intuitive formalism. This enables effective communication between scientists across the mathematical divide by fostering substantive debate in the context of a scientific problem, and ultimately facilitates the joint development of statistical and computational tools for quantitative data analysis. A number of success stories have appeared over the years [1–4]. Today, probabilistic graphical models promise to play a major role in the resolution of many intriguing conundrums in the biological sciences. The goal of this short article is to be a dense, informative introduction to the language of probabilistic graphical models, for beginners, with pointers to successful applications in selected areas of biology. The exposition introduces the essential concepts involved in PGMs in the context of the various stages of a typical collaboration between natural and computational scientists, and discusses the aspects to which each scientist should contribute to carry out the data analysis successfully using PGMs. 
 
Let us start by considering a specific problem in transcriptional regulation. Given measurements about the abundance of gene transcripts in retinal cells across stages of development, we would like to discover which functional processes are relevant for development, and reveal which ones are most important at which stage. To develop a PGM to address this problem, we begin by identifying the biological objects that would appear in a cartoon model of how cellular development impacts transcription. In this illustrative example, we have genes and functional processes/contexts. It is reasonable to assume that each gene will participate in multiple functional processes, although typically in a small number of them, and that not all functional processes will be important at all stages of development. We then assess what aspects of the problem we can probe directly, with experimental techniques, and what aspects we cannot. In the example, while an abundance of gene transcripts can be obtained, for instance, via SAGE (serial analysis of gene expression), it is harder to measure functional processes. However, the latter could be operationally defined as sets of genes that share a similar temporal regulation pattern; this definition has the advantage of creating a connection between membership of genes to functional processes (i.e., an unobservable mapping) and similarity of the temporal expression profiles (i.e., observable quantities). The establishment of connections between those biological objects that we can probe and those that we cannot ends a first conceptual effort. 
 
A cartoon model of how cellular development impacts transcription is now specified in terms of genes and their abundance, functional processes, and membership of genes to functional processes. Next we translate the biological players and the connections we established among them into mathematical quantities (i.e., random variables) and connections among them (i.e., statistical dependencies). This translation specifies the model structure. At this stage, we rely on biological intuitions to fine-tune the model, for instance, by deciding which sources of variability in the measurements carry information about the latent variables and which do not—if the temporal expression profiles of genes A and B are similar on a relative scale, but their absolute abundance is quite different, should we believe that they both participate in the same functional processes? Last, we assign numerical values to those quantities that are unknown in the final model specifications (i.e., we fit the model to the data) and we use them to develop biological intuitions in the context of the original problem. (Functional aspects of retinal development, in mouse, are fully addressed in [5].) 
 
In the following, we briefly introduce the basic mathematical quantities that enable the translation of a cartoon model of biology into a PGM, and we review strategies to assign numerical values to the unknown quantities underlying any PGM that are most likely given the observations. We conclude with an overview of selected applications, complete with pointers to published work.",2007,PLoS Comput. Biol.
An Introduction to Biomolecular Graphics,"Biological function arises from detailed molecular structure, making it difficult to overemphasize the role of structural visualization and biomolecular graphics in shaping our current understanding of the molecular nature of biological systems. Indeed, one need only compare the number of three-dimensional (3D) structures illustrated in the first (1990) and fourth (2010) editions of Voet & Voet's Biochemistry in order to appreciate the profound communicative value of molecular graphics in modern biosciences, ranging from medicine and physiology to drug design and computational biology. Faced with a deluge of structural genomics results over the past decade, the cliche about a picture being worth a thousand words is quite poignant: The information “content” of carefully constructed molecular graphics can be immense. Because computer-based molecular visualization (MolVis) is such an effective means for exploring and analyzing structural data, this guide introduces the science and art of biomolecular graphics, both in principle and as practiced in structural and computational biology. 
 
This guide is built around a series of practical case-studies, emphasizing the creation of biomolecular graphics for publication figures and animations. Intended primarily for those embarking upon their first illustrations, intermediate-level examples are also provided in order to facilitate the transition from novice user to advanced practitioner. For enhanced pedagogical value, the exact methods used to create each figure are provided to the reader in the form of heavily annotated computer scripts. Because the PyMOL [1] software package was used to create these illustrations, all materials (images, animations, scripts, etc.) have been made freely available as a dedicated section of the PyMOL wiki site (http://pymolwiki.org/PLoS). Additional background material on MolVis, including a detailed review of the underlying principles (Box 1), is provided as supporting information (Text S1). Further information can be found in the recent treatment by Bottomley and Helmerhorst [2], and in several reviews covering either small-molecule [3] or macromolecular visualization [4]–[6]. 
 
 
Box 1. MolVis Concepts and Terminology 
Raster, vector: Two different ways to structure images, either as combinations of simple geometric objects such as points, lines, curves (vector graphics), or as a discrete 2D array of colored pixels (raster/bitmap). Vector graphics are arbitrarily scalable, whereas the fixed array of pixels in a bitmap leads to graininess (“pixelization”) upon zooming-in of raster graphics; see Box S1 in Text S1 and ref. [2] for further information. 
 
Graphics primitives: Low-level geometric entities that are readily described in mathematical terms (lines, spheres, tetrahedra, etc.), and from which any complex shape, such as protein surfaces, can be constructed via solid geometry. Scenes are built from primitives, along with associated lighting, shading, and texturing properties; thus, primitives are how a scene is discretized for computer representation and manipulation. As an example, increasing PyMOL's “sphere_quality” beyond the default value of 1 yields smoother spheres (more triangles), while decreasing to 0 exposes the individual triangular primitives used to render spheres. 
 
Scene geometry, matrices: Several matrices are used to transform a molecular scene (atomic coordinate-based) into the image (pixel-based) shown on the actual 2D display. Along with all the primitives that represent molecular properties (atoms, bonds, surfaces, etc.), many other scene data must also be carried through these transforms, including materials, colors, lighting, shading, clipping, and depth (z) buffer data—in other words, all the attributes that define a scene. In being mapped onto the viewing plane, a scene can be rendered in either a perspective (skewed viewing matrix) or orthoscopic (orthonormal viewing matrix) projection mode; the PyMOL settings “orthoscopic” and “field_of_view” adjust this behavior, and the viewing matrix can be retrieved/modified via the “get_view” / “set_view” pair of commands. 
 
Clipping planes: The boundaries of a scene define a rectangular pyramid, with an apex at the camera(/eye), and the faces defined by top/bottom and right/left pairs of planes. In addition, far/near clipping planes can be defined behind/in front of a region of interest in this rectangular pyramid. Clipping plane geometry and behavior is adjustable; for instance, the PyMOL command “clip slab, 20” sets the slab thickness to 20 A. 
 
Ray tracing: A method to render photorealistic images by simulating the path of light rays through a scene, incorporating effects such as light sources, opacity, textures, atmospheric fog, and shading models. Ray tracing is computationally expensive for complex scenes, and more “realistic” (higher resolution) images require a greater density of light rays per pixel of the final image. 
 
Keyframes: Reference markers, either in time (animations) or in space (interpolations), that serve as the end-points that bracket an interpolation stage. For instance, in a sequence of frames consisting of structural snapshots S1→···→S2 ······ Sn, S1 and S2 define the first pair of keyframes. Linearly interpolating the gaps between S1 and S2 is essentially a form of data-smoothening. Most movie-making functionalities incorporate the keyframe concept. 
 
Anti-aliasing: A feature/setting in most MolVis programs (“antialias” in PyMOL) that greatly improves image quality by diminishing the jagged distortions (“aliasing”) of curves and diagonal lines that compose the geometric primitives of a scene.",2010,PLoS Comput. Biol.
Crossing Over…Markov Meets Mendel,"Chromosomal crossover is a biological mechanism to combine parental traits. It is perhaps the first mechanism ever taught in any introductory biology class. The formulation of crossover, and resulting recombination, came about 100 years after Mendel's famous experiments. To a great extent, this formulation is consistent with the basic genetic findings of Mendel. More importantly, it provides a mathematical insight for his two laws (and corrects them). From a mathematical perspective, and while it retains similarities, genetic recombination guarantees diversity so that we do not rapidly converge to the same being. It is this diversity that made the study of biology possible. In particular, the problem of genetic mapping and linkage—one of the first efforts towards a computational approach to biology—relies heavily on the mathematical foundation of crossover and recombination. Nevertheless, as students we often overlook the mathematics of these phenomena. Emphasizing the mathematical aspect of Mendel's laws through crossover and recombination will prepare the students to make an early realization that biology, in addition to being experimental, IS a computational science. This can serve as a first step towards a broader curricular transformation in teaching biological sciences. I will show that a simple and modern treatment of Mendel's laws using a Markov chain will make this step possible, and it will only require basic college-level probability and calculus. My personal teaching experience confirms that students WANT to know Markov chains because they hear about them from bioinformaticists all the time. This entire exposition is based on three homework problems that I designed for a course in computational biology. A typical reader is, therefore, an instructional staff member or a student in a computational field (e.g., computer science, mathematics, statistics, computational biology, bioinformatics). However, other students may easily follow by omitting the mathematically more elaborate parts. I kept those as separate sections in the exposition.",2012,PLoS Comput. Biol.
Automated Discovery of Functional Generality of Human Gene Expression Programs,"An important research problem in computational biology is the identification of expression programs, sets of co-expressed genes orchestrating normal or pathological processes, and the characterization of the functional breadth of these programs. The use of human expression data compendia for discovery of such programs presents several challenges including cellular inhomogeneity within samples, genetic and environmental variation across samples, uncertainty in the numbers of programs and sample populations, and temporal behavior. We developed GeneProgram, a new unsupervised computational framework based on Hierarchical Dirichlet Processes that addresses each of the above challenges. GeneProgram uses expression data to simultaneously organize tissues into groups and genes into overlapping programs with consistent temporal behavior, to produce maps of expression programs, which are sorted by generality scores that exploit the automatically learned groupings. Using synthetic and real gene expression data, we showed that GeneProgram outperformed several popular expression analysis methods. We applied GeneProgram to a compendium of 62 short time-series gene expression datasets exploring the responses of human cells to infectious agents and immune-modulating molecules. GeneProgram produced a map of 104 expression programs, a substantial number of which were significantly enriched for genes involved in key signaling pathways and/or bound by NF-κB transcription factors in genome-wide experiments. Further, GeneProgram discovered expression programs that appear to implicate surprising signaling pathways or receptor types in the response to infection, including Wnt signaling and neurotransmitter receptors. We believe the discovered map of expression programs involved in the response to infection will be useful for guiding future biological experiments; genes from programs with low generality scores might serve as new drug targets that exhibit minimal “cross-talk,” and genes from high generality programs may maintain common physiological responses that go awry in disease states. Further, our method is multipurpose, and can be applied readily to novel compendia of biological data.",2007,PLoS Comput. Biol.
The Regional Student Group Program of the ISCB Student Council: Stories from the Road,"The International Society for Computational Biology (ISCB) Student Council was launched in 2004 to facilitate interaction between young scientists in the fields of bioinformatics and computational biology. Since then, the Student Council has successfully run events and programs to promote the development of the next generation of computational biologists. However, in its early years, the Student Council faced a major challenge, in that students from different geographical regions had different needs; no single activity or event could address the needs of all students. To overcome this challenge, the Student Council created the Regional Student Group (RSG) program. The program consists of locally organised and run student groups that address the specific needs of students in their region. These groups usually encompass a given country, and, via affiliation with the international Student Council, are provided with financial support, organisational support, and the ability to share information with other RSGs. In the last five years, RSGs have been created all over the world and organised activities that have helped develop dynamic bioinformatics student communities. In this article series, we present common themes emerging from RSG initiatives, explain their goals, and highlight the challenges and rewards through specific examples. This article, the first in the series, introduces the Student Council and provides a high-level overview of RSG activities. Our hope is that the article series will be a valuable source of information and inspiration for initiating similar activities in other regions and scientific communities.",2013,PLoS Comput. Biol.
Biocurators: Contributors to the World of Science,"Computational biology is a discipline built upon data (mostly free access), found in biological databases, and knowledge (mostly not free access), found in the literature. So important are these online sources of data that the discipline, and indeed this Journal, simply would not exist without them. Whether we are using the data in “browse mode”—doing a PubMed search, looking up a reaction in an enzymatic pathway, or in “compute mode”—analysis of a large dataset, we usually visit Web sites and download information without a second thought. Since our discipline is so dependent on the availability, extent, and quality of biological data, it is worth taking some time to think about the processes of data accessibility, annotation, and validation. These processes depend very much on biocurators—trained staff who ensure the information you are receiving is as complete and accurate as possible. 
 
 
Biocurators can be considered the museum catalogers of the Internet age: they turn inert and unidentifiable objects (now virtual) into a powerful exhibit from which we can all marvel and learn. That would be a decent enough contribution to the world of science, but the task of the biocurator is even more extensive. Computational biologists do not expect to merely walk through the door, cast a casual eye over the exhibit, and exit wiser (although we frequently do); we also want to add our own data to the exhibit, plus pick and choose pieces of it to take home and create new exhibits of our own. Oh, and we would like to do all these things with minimal effort, please. We can be a pretty exacting bunch of customers, and it takes skills over and above a knowledge of biology to juggle the different needs of data submitters, information seekers, and power players. 
“We pay homage to these special individuals who are dedicated to making our research endeavors a success.” 
 
In this October issue, we pay homage to these special individuals who are dedicated to making our research endeavors a success. We do so through two Perspectives written by biocurators working with different types of biological data. The first is by biocurators from the Research Collaboratory for Structural Bioinformatics Protein Data Bank (PDB), a well-established biological resource of macromolecular structure data used by more than 10,000 individual scientists per day, and the second by biocurators of the Immune Epitope Database and Analysis Resource (IEDB), a new resource detailing known epitopes and their immunological outcomes. The PDB validates the quality and consistency of primary data submitted by structural biologists as a prerequisite to publication. The IEDB curates the published literature, extracting relevant facts about the epitopes discussed therein. As you read these two Perspectives, similarities and differences concerning the approaches will emerge. But more than anything, we hope you are struck by the level of professionalism and dedication that goes into helping to make the quality research articles that you read in this Journal and elsewhere. 
 
These two articles are told from the perspective of the biocurators themselves. It is only two perspectives; we certainly encourage you to send eLetters with your own perspective on biocuration, either as a curator of a different type of information, or as a person whose information has been curated, or as a consumer of information that has been curated. If you are not moved to comment, at least give a thought to the person upon whose efforts your research may well depend.",2006,PLoS Comput. Biol.
Ten Simple Rules for Starting a Regional Student Group,"Student organizations are a great way to network and take a break from the rigors of the classroom. They provide a range of benefits beyond regular coursework and can be critical to having a well-rounded education. Many students are active in organizations at an undergraduate level, but the increased demands of a master's or PhD typically result in reduced participation at a graduate level. However, a student organization can equally provide benefits for a graduate student, especially if it is centered on the student's area of study. In this article, we focus on Regional Student Groups (RSGs). An RSG is a group of like-minded students across a geographical region with a common field of research. The group provides a support network and collaboration opportunities via a collection of individuals who “speak the same language.” The RSG concept was created by the International Society for Computational Biology Student Council to address the needs of students in the field of computational biology in each region. Currently, the RSG program consists of over 20 regional student groups worldwide. In this article, we provide ten simple rules for how to start a regional student group in the hope that others will start up similar groups around the world.",2013,PLoS Comput. Biol.
Maximum-Likelihood Model Averaging To Profile Clustering of Site Types across Discrete Linear Sequences,"A major analytical challenge in computational biology is the detection and description of clusters of specified site types, such as polymorphic or substituted sites within DNA or protein sequences. Progress has been stymied by a lack of suitable methods to detect clusters and to estimate the extent of clustering in discrete linear sequences, particularly when there is no a priori specification of cluster size or cluster count. Here we derive and demonstrate a maximum likelihood method of hierarchical clustering. Our method incorporates a tripartite divide-and-conquer strategy that models sequence heterogeneity, delineates clusters, and yields a profile of the level of clustering associated with each site. The clustering model may be evaluated via model selection using the Akaike Information Criterion, the corrected Akaike Information Criterion, and the Bayesian Information Criterion. Furthermore, model averaging using weighted model likelihoods may be applied to incorporate model uncertainty into the profile of heterogeneity across sites. We evaluated our method by examining its performance on a number of simulated datasets as well as on empirical polymorphism data from diverse natural alleles of the Drosophila alcohol dehydrogenase gene. Our method yielded greater power for the detection of clustered sites across a breadth of parameter ranges, and achieved better accuracy and precision of estimation of clusters, than did the existing empirical cumulative distribution function statistics.",2009,PLoS Comput. Biol.
A Report of the Curriculum Task Force of the ISCB Education Committee,"The International Society for Computational Biology (ISCB) Education Committee (EduComm) promotes worldwide education and training in computational biology and bioinformatics and serves as a resource and advisor to organizations interested in developing educational programs. 
 
The topic of curricula for bioinformatics programs has long been of interest to ISCB and EduComm. Dr. Russ Altman, a founding board member and past president of ISCB, has been associated with one of the first bioinformatics degree programs (at Stanford University) and wrote an article on this topic [1]. Dr. Shoba Ranganathan, as chair of EduComm a decade ago, began organizing a yearly Workshop on Education in Bioinformatics (WEB) at Intelligent Systems for Molecular Biology (ISMB) meetings that generated exchange of information and many productive discussions. Curriculum development was one aspect of bioinformatics education covered in these sessions [2]. 
 
The field of bioinformatics has grown in the past decade. There are many such degree granting programs around the world at the bachelor's, master's, and PhD levels. This article provides a status report of the EduComm's ongoing endeavor to identify a set of core curricular guidelines for bioinformatics education at all levels. As a pilot project, the Curriculum Task Force of the EduComm conducted a survey in the spring of 2011. This initial survey was sent to members of the EduComm, consisting of 50 individuals from various regions of the world, and to the EMBnet community, representing 79 people from more than 30 countries. The response rate was 33%, with 41 individuals completing the survey. Analysis of the survey produced an initial set of recommendations to be used as a discussion point from which to launch a larger effort to develop a working bioinformatics curriculum. With increased input from the larger community, the EduComm will continue to refine its results. Individuals who are interested in contributing to this initiative are encouraged to contact the Chairs of the ISCB EduComm. 
 
The purposes of this article are to further disseminate the survey results and to solicit participation in the initiative. The initial survey results are summarized, the preliminary working curriculum is defined, and the next steps of the EduComm Curriculum Task Force are outlined.",2012,PLoS Comput. Biol.
Bioinformatics in China: A Personal Perspective,"In this personal perspective, we recall the history of bioinformatics and computational biology in China, review current research and education, and discuss future prospects and challenges. The field of bioinformatics in China has grown significantly in the past decade despite a delayed and patchy start at the end of the 1980s by a few scientists from other disciplines, most noticeably physics and mathematics, where China's traditional strength has been. In the late 1990s and early 2000s, rapid expansion of the field was fueled by the Internet boom and genomics boom worldwide and in China. Today bioinformatics research in China is characterized by a great variety of biological questions addressed and the close collaborative efforts between computational scientists and biologists, with a full spectrum of focuses ranging from database building and algorithm development to hypothesis generation and biological discoveries. Although challenges remain, the future of bioinformatics in China is promising thanks to advances in both computing infrastructure and experimental biology research, a steady increase of governmental funding, and most importantly a critical mass of bioinformatics scientists consisting of not only converts from other disciplines but also formally trained overseas returnees and a new generation of domestically trained bioinformatics Ph.D.s.",2008,PLoS Comput. Biol.
Making Biomolecular Simulations Accessible in the Post-Nobel Prize Era,"In 2013, three pioneers of computational biophysics and structural biology, Martin Karplus, Arieh Warshel, and Michael Levitt, were awarded the Nobel Prize in Chemistry. Although the citation focused on their innovative efforts on integrating quantum mechanical and classical mechanical models to study reactive processes in proteins, the award has also been seen by many researchers in the biomolecular simulation field as recognizing the tremendous value of computations for the investigation of biomolecules in general. From the days when proteins were modeled at the picosecond timescale using a united atom representation [1], or even as coarse-grained beads [2], in vacuum, to modern simulations that approach the millisecond timescale for a fully solvated protein [3], the biomolecular simulation field has, indeed, come a long way. Much of the progress has been due to the efforts of the three laureates, their contemporaries, and many others (e.g., their students) who were inspired by their dream of understanding life by studying “the jiggling and wiggling of atoms” [4]. One could only admire the tremendous courage, imagination, and vision that drove these three scientists to start pursuing their dream in an era when theoretical and computational chemistry largely focused on understanding the interactions and reactivity of small molecules. 
 
Just as the Nobel Prize in 1998 to John Pople and Walter Kohn highlighted both the impact and emerging challenges of quantum chemistry, the 2013 Chemistry Prize should also further inspire us to ponder about the future of computational biology. Clearly, developing methodologies that further enhance the quantitative accuracy and/or complexity of computational models are important and being actively pursued by many researchers. On the quantitative aspect, several community-wide blind tests on observables such as solvation free energies, binding affinities, and pKa values are being held. Provided that the results are disseminated in a constructive manner, these blind tests are highly valuable for helping the community converge towards the most robust and efficient computational algorithms and protocols. On the other hand, it is valuable to bear in mind that in many (certainly not necessarily all) investigations, quantitative computations represent a means to validate the model rather than the ultimate goal, which ought to focus on revealing the physical and chemical principles that govern the biological problem at hand. In other words, understanding qualitative trends is equally important. Therefore, building models with different levels of complexity and identifying robust features relevant to the biological problem remains an important research strategy. After all, in many mechanistic studies, whether at the molecular or cellular scale, the ultimate goal is to establish a conceptual framework to guide the development of novel mechanistic hypotheses and to stimulate new experiments to evaluate them. 
 
Another important issue worth emphasizing in this “Post-Nobel Prize era” concerns making high-quality biomolecular simulation protocols available to the bioscience community, especially to young researchers who have just entered the field and perhaps even researchers who are primarily experimentalists. Such efforts will be essential to further enhancing the impact of biomolecular simulations while maintaining a high level of integrity in the result. In this issue of PLOS Computational Biology, Woodcock and coworkers have made a major step in this direction by describing a set of web-based tutorials and tools for the simulation package Chemistry at HARvard Molecular Mechanics (CHARMM) [5]–[7]; the tools are fittingly and playfully referred to as “CHARMMing.” The web-based tools make it straightforward to set up complex biomolecular simulations, including reduction potential computation for proteins and molecular dynamics simulations using a coarse-grained model. For even an expert in biomolecular simulation, it is often cumbersome to set up a new simulation that requires the generation of force field parameters for cofactors; CHARMMing is helpful in this context by providing an easy access to several automated small molecule force field generation services (e.g., the ParamChem web-server, the MATCH toolkit). 
 
Importantly, CHARMMing goes beyond simply facilitating the set-up of biomolecular simulations by including carefully designed lessons on topics that range from basic simulation tutorials to advanced protocols such as quantum mechanical (QM)/molecular mechanical (MM) calculations and enhanced sampling techniques. The graphic interface allows the “students,” who take those lessons, to understand and modify CHARMM input scripts as well as visualize simulation results. Therefore, CHARMMing is valuable not only as a research tool, but also an educational module that can easily be incorporated into curriculum at both the undergraduate and early graduate level. As a result, CHARMMing is complementary to another valuable web-based research tool, CHARMM-GUI [8], which features a number of sophisticated functionalities, such as setting up membrane simulations [9] and absolute ligand binding affinity calculations [10]. We hope that the set of CHARMMing papers will help stimulate additional efforts in bringing advanced simulations, good computational practices, and thorough analysis of simulation results to the broader biological research community. Although pushing the limit of computational research via method development is always essential, an equally important goal is, to paraphrase what Martin Karplus once stated [11], that experimental (structural) biologists, who know their systems better than anyone else, will make increasing use of molecular dynamics simulations for obtaining a deeper understanding of particular biological systems. 
 
This Editorial was first published as a blog post on PLOS Biologue on July 25, 2014.",2014,PLoS Comput. Biol.
ISCB Public Policy Statement on Open Access to Scientific and Technical Research Literature,"The International Society for Computational Biology (ISCB) is dedicated to advancing human knowledge at the intersection of computation and life sciences. On behalf of the ISCB members, this public policy statement expresses strong support for open access, reuse, integration, and distillation of the publicly funded archival scientific and technical research literature, and for the infrastructure to achieve that goal. Knowledge is the fruit of the research endeavor, and the archival scientific and technical research literature is its practical expression and means of communication. Shared knowledge multiplies in utility because every new scientific discovery is built upon previous scientific knowledge. Access to knowledge is access to the power to solve new problems and make informed decisions. Free, open, public, online access to the archival scientific and technical research literature will empower citizens and scientists to solve more problems and make better, more informed decisions. Attribution to the original authors will maintain consistency and accountability within the knowledge base. Computational reuse, integration, and distillation of that literature will produce new and as yet unforeseen knowledge. We strongly encourage open software, data, and databases, issues that are not addressed here. A prior ISCB public policy statement on sharing software provides very clear support for open source/open access (http://www.iscb.org/iscb-policystatements/software_sharing). We support open database access, standards, and interoperability. We also recognize that databases are complex dynamic entities, with ongoing roles and needs that cannot be treated properly within this statement. In contrast, the publicly funded archival research literature, addressed here, is the static historical record of publicly funded research outcomes. ISCB supports many of the principles set forth in other open-access policies and statements, including the ‘‘Budapest Open Access Initiative,’’ the ‘‘Bethesda Declaration on Open Access Publishing,’’ the Bulletin of the World Health Organization ‘‘Equitable Access to Scientific and Technical Information for Health,’’ the US National Academies of Sciences report on ‘‘Sharing Publication-Related Data and Materials: Responsibilities of Authorship in the Life Sciences,’’ the Organisation for Economic Co-Operation and Development ‘‘Principles and Guidelines for Access to Research Data from Public Funding,’’ and the ‘‘Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities.’’ Details on the documents mentioned here may be found in Text S1. Further background material is available in Text S2. The public policy statement (Box 1) put forward here builds upon these principles to elucidate in more detail the public policy position of ISCB and its members on this important issue in scientific dissemination.",2011,PLoS Computational Biology
Correction: De-Novo Discovery of Differentially Abundant Transcription Factor Binding Sites Including Their Positional Preference,"Transcription factors are a main component of gene regulation as they activate or repress gene expression by binding to specific binding sites in promoters. The de-novo discovery of transcription factor binding sites in target regions obtained by wet-lab experiments is a challenging problem in computational biology, which has not been fully solved yet. Here, we present a de-novo motif discovery tool called Dispom for finding differentially abundant transcription factor binding sites that models existing positional preferences of binding sites and adjusts the length of the motif in the learning process. Evaluating Dispom, we find that its prediction performance is superior to existing tools for de-novo motif discovery for 18 benchmark data sets with planted binding sites, and for a metazoan compendium based on experimental data from micro-array, ChIP-chip, ChIP-DSL, and DamID as well as Gene Ontology data. Finally, we apply Dispom to find binding sites differentially abundant in promoters of auxin-responsive genes extracted from Arabidopsis thaliana microarray data, and we find a motif that can be interpreted as a refined auxin responsive element predominately positioned in the 250-bp region upstream of the transcription start site. Using an independent data set of auxin-responsive genes, we find in genome-wide predictions that the refined motif is more specific for auxin-responsive genes than the canonical auxin-responsive element. In general, Dispom can be used to find differentially abundant motifs in sequences of any origin. However, the positional distribution learned by Dispom is especially beneficial if all sequences are aligned to some anchor point like the transcription start site in case of promoter sequences. We demonstrate that the combination of searching for differentially abundant motifs and inferring a position distribution from the data is beneficial for de-novo motif discovery. Hence, we make the tool freely available as a component of the open-source Java framework Jstacs and as a stand-alone application at Initiative of the German Federal, and State Governments (EXC 294). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.",2011,PLoS Computational Biology
On the Estimation of Intron Evolution,"PLoS Computational Biology recently published an article about spliceosomal intron evolution by Nguyen, Yoshihama, and Kenmochi [1]. The authors were unaware of some earlier independent results. Foremostly, the main point of the article—that of estimating the density of potential intron sites—is not novel. It was described more than three months earlier [2]. The numerical results are virtually identical in the two publications, which is not surprising, since they apply the same model to the same data [3]. A recent article points to the model's validity. Raible and coauthors [4] report that introns in the protostome Platynereis dumerilii are almost as abundant as in humans, and many introns are in homologous positions between the two species. The shared positions indicate that at most one-third of human introns were gained in the vertebrate lineage, in agreement with the estimates of [2] and [1]. In contrast, parsimony estimates [3] should change significantly when including P. dumerilii. 
 
 
To estimate ancestral intron losses and gains, Nguyen and coauthors use an exponential-time procedure, which is practical only for a few species. In reality, the estimation can be done in linear time [2], as described briefly below. We are modeling intron presence and absence in homologous sites across organisms related by a known phylogeny. Presence and absence are encoded by 1 and 0, respectively. Introns evolve independently, by a Markov model for a binary character. On branch e, an intron is lost with probability pe(1 → 0) and an intron is gained with probability pe(0 → 1) at every site. Assuming a continuous-time Markov process, 
 
 
 
 
 
where λ, μ > 0 are branch-specific gain and loss rates, and t > 0 is branch length. Introns are observed at the terminal taxa. An all-absent intron site is never observed, and, thus, the number of potential intron sites must be estimated for correct likelihood optimization. The likelihood can be computed by a dynamic programming algorithm [5]. The algorithm calculates the conditional likelihood Lu(x) for every node u and state x ∈ {0,1}: Lu(x) is the probability of the observed states in descendants of u, conditioned on the state x at u. One can further define the upper conditional likelihood Uu(x) for the observed states outside the subtree of u, which can be computed efficiently by dynamic programming even if the underlying process is irreversible [2]. Feslenstein [6] reviews relevant techniques for the reconstruction of ancestral molecular sequences, which are generally assumed to evolve by a reversible process. Now, the posterior probability of the intron state x at every node u can be computed as 
 
 
 
 
 
 
The posterior probability for state change x → y on an edge uv is computed as 
 
 
 
 
 
The expected numbers of gains or losses are obtained by summing the probabilities quv(0 → 1) and quv(1 → 0) over all intron sites, respectively. Nguyen and coauthors consider instead all 2N state labeling of N internal nodes to compute the expected numbers of gains and losses. A Java package implements the more efficient procedure, and is publicly available at http://www.iro.umontreal.ca/~csuros/introns/. 
 
Nguyen et al. [1] reiterate well-known concerns of identifiability. Their Proposition 1 echoes the Pulley Principle for ambiguous root placement [5]. Proposition 2 asserts that there are two possible parameter sets pe(x → y) for every branch, which can be combined to get exponentially many choices that give the same likelihood function. The continuous-time process of Equation 1 implies pe(0 → 1) + pe(1 → 0) < 1. Such constraint leads to unique parametrization (except for the root position), and is more natural than the one proposed by Nguyen and coauthors, which is based on the variance of intron gains and losses. 
 
Nguyen and coauthors discuss an important study by Qiu, Schisler, and Stoltzfus [7]. Qiu and coauthors constructed multiple alignments of ten gene families. The families had 68 sequences and 49 intron sites on average. Using a Bayesian framework, Qiu and coauthors estimated two intron evolution parameters per family, assuming constant rates across sites and branches. The model's adequacy and some of the conclusions can certainly be debated, especially in view of the assumption of constant rates. Nguyen and coauthors, however, speculate that the data were insufficient for valid inference, since there are 268 possible intron presence–absence patterns for the average gene family, but only 49 intron sites. The argument is not sound: the number of patterns has little to do with inference (consider the case of a protein alignment with 20k possible patterns for k sequences). It is the number of parameters that matters.",2006,PLoS Computational Biology
How To Advance Open International Scientific Exchange,"Computational biology is an international collaboration. Open scholarly exchange nurtures the development of our field. And scientists are not the only beneficiaries; international cooperation is a crucial part of any country's diplomatic relations. Our community, by actively engaging governments, needs to promote scientific exchange. 
 
In response to reports of visa difficulties, the International Society for Computational Biology (ISCB; http://www.iscb.org/) recently surveyed its members about the experience of non-US scientists visiting or working in the United States (http://www.iscb.org/US_visa_survey.html). The answers of 50 people of 20 nationalities revealed needless inefficiencies and problems. The results of the survey were shared with the National Academy of Sciences (NAS, http://www.nationalacademies.org/) in advance of a February 7, 2008, Senate hearing on the topic of barriers to scientific exchange (http://science.house.gov/publications/hearings_markups_details.aspx?NewsID=2064). 
 
While ISCB collected information only on US visa problems, enabling international scientific exchange is a matter of importance in all countries. A 2005 European Union (EU) directive designed to make it easier for non-European scientists to get working visas for the EU has been implemented by only some member countries (http://www.euractiv.com/en/science/member-states-hesitant-welcome-foreign-researchers/article-167646, http://wbc-inco.net/news/3285.html). Visa agreements between countries are almost always reciprocal. For example, if China allows only single entry visas to US citizens, then the US allows only single entry visas to Chinese citizens. There is a need for all countries to work out better reciprocity agreements with each other in the name of international exchange and collaboration. 
 
Concerns about terrorism negatively affected the US visa process; one result was that in the two years following the September 11, 2001, attacks, the number of foreign graduate students in the US fell significantly [1]–[3]. After 2003, this trend was reversed as policies were changed, but the situation is still not ideal, as reflected in the ISCB survey. 
 
Too many of our fellow computational biologists have had to put their careers on hold, in some cases for more than 6 months, while waiting for permission to enter the US for study or a job. For some, permission arrives too late; the conference is over, or the research opportunity lost. For others, permission is never granted, with reasons either unknown or incomprehensible. Many non-US citizens living in the US are afraid to leave to visit family or attend a meeting, because getting back into the US can take a long time and is not guaranteed. Most survey respondents complained of the delays and lack of transparency in the visa process. 
 
While many scientists have experienced positive interactions with helpful embassy and border personnel, there are also stories of ill treatment. Our colleagues have reported being spoken to rudely, detained at the border, interrogated, and even shackled. Some are afraid to tell their stories without assurance of anonymity, for fear of negative impact on their careers. 
 
Perhaps most troubling of all are the many thousands of scientific collaborations and personal relationships that never had a chance to develop because scientists did not even attempt to come to the US as a result of these difficulties. The current situation is puzzling and disappointing given the central role that international scientists have played in enriching the US both culturally and economically. 
 
Respondents to the ISCB survey gave recommendations, which we support, for the various stakeholders: government, scientific organizations, and scientists themselves. 
 
Some of the recommendations for the US government are also applicable to other countries: 
 
 
Treat visiting scientists with respect and recognize their contributions. 
 
 
Have well-documented and transparent immigration policies. 
 
 
Make the immigration procedure predictable with fixed time lines. 
 
 
Streamline and simplify the visa process and reduce turnaround times. 
 
 
Make visas valid for a longer time; allow multiple entries. 
 
 
 
Additional recommendations specific to the US government were to: 
 
 
Make it easier for non-US citizens living in the US to leave for a short time for meetings or visits home. 
 
 
Make it possible to extend the J-1, O-1, and other visas from within the US. 
 
 
Remove the 2-year 221(e) restriction. 
 
 
Perform the necessary MANTIS searches before the consular appointments. 
 
 
 
Professional societies and conference organizers should: 
 
 
Educate embassy decisionmakers about our field, to reduce the mistaken tagging of computational biologists as security risks. Give scientists advice about how to explain what they do in such a way to make it clear that they are not a security risk. 
 
 
Notify submitters to conferences earlier of acceptances, to allow enough time to go through the visa process. 
 
 
Give full refunds to attendees not able to make it due to visa problems. 
 
 
Lobby government for changes listed above. 
 
 
Provide direct help to conference attendees in processing their visas, including explaining what documents are necessary. 
 
 
 
What can you as a scientist do? A lot! 
 
You can report US-visa–related issues to the NAS International Visitors Office (http://www7.nationalacademies.org/visas/Visa_Questionnaire.html). This information is used to support international scientific exchange, and in some cases to help address specific cases. 
 
Talk and write to your legislative representatives as well as to all levels of government. Let them know about problems, and about the value of international scientific exchange to your work. Advocate specific improvements to policies, laws, and international agreements. 
 
Work through scientific organizations such as ISCB, NAS, and FASEB (the Federation of American Societies for Experimental Biology, http://opa.faseb.org/). Many of these organizations have been very active in the issue of international scientific exchange. 
 
And finally, share your experience, concerns, and ideas with other scientists. PLoS invites you to use the comments feature associated with this article to share your experience of traveling internationally, and your ideas about how to take action on this topic. 
 
Enabling open scientific exchange is an important issue to the future of our science; your voice is important!",2008,PLoS Comput. Biol.
“Getting Started In…”: A Series Not to Miss,"In recent decades, computational biology has established itself as a critical part of biomedical research and as an ever-growing and highly interdisciplinary field. It seems that every year, new areas of research appear—sequence analysis now includes SNP detection and whole genome alignments, gene expression microarray data can be analyzed in concert with interaction networks, and protein structures can be used to predict potential drug targets. This diversity and development of bioinformatics has led to an increasing number of approaches and techniques that require understanding of areas as diverse as biology and computer science, medicine and statistics, chemistry and applied mathematics. With the knowledge required to explore each area of computational biology, the barrier grows for those trying to enter it. So how can one start learning about tiling array analysis or text mining in biology? What should a new graduate student read before analyzing his first microarray? How can a machine learning researcher find out what interesting problems in bioinformatics she may address with probabilistic graphical models? We hope that help is on the way. 
 
This month, PLoS Computational Biology and the International Society for Computational Biology begin a series of short, practical articles for students and active researchers who want to learn more about new areas of computational biology and are unsure where or how to start. The aim of each article in the “Getting Started in…” series is to introduce the essentials: define the area and what it is about, highlight the debates and issues of relevance, and provide directions to the most relevant books, articles, or Web sites to find out more. The series will not include review articles or detailed tutorials; these are available in the Education section of the Journal. Rather, each “Getting Started in…” article will aim to be a cache of “go to” information for someone for whom the field is completely new. We hope each part of the series, written by experts in areas as diverse as data integration and phylogeny reconstruction, will be as invaluable as receiving an e-mail from a colleague who takes time and thought to offer the best advice and the essential introduction to his or her area of research. 
 
The first expert to inform, motivate, and inspire readers to consider a new direction is Dr. Xiaole Shirley Liu, who introduces tiling microarrays. As the series progresses, we can look forward to learning about text mining and probabilistic graphical models, to name a few topics. We hope you find this new series useful and enjoyable.",2007,PLoS Comput. Biol.
ISMB 2005 Conference Report,"The ISCB held its 13th annual Intelligent Systems for Molecular Biology Conference (ISMB) in Detroit, Michigan, June 25– 29, 2005. Considering this conference has moved to many points on the globe in its 13 years, this location was not so far away geographically from the first ISMB held in Bethesda, Maryland, in 1993, but the increase in numbers in all categories—attendance, presentations, and special intrest group meetings and tutorials—put it in a separate stratosphere altogether (see Table 1). ISMB/ECCB 2004, held in Scotland last year as a joint conference with the European Conference on Computational Biology (ECCB), resulted in the highest attendance of any ISMBor any ECCB on record, with 2,136 total attendees. Given a variety of challenges, and given that the meeting in Detroit was not co-located with ECCB or any other conference in our field, ISMB2005 drew attendees surprisingly well. In fact, ISMB 2005 had the second-highest attendance ever, andwas themost highly attended North American ISMB to date. As in years past, seven special interest group (SIG) meetings preceded this year’s conference, enabling registrants to come for both the SIGs and the main conference, or attend just the SIGs. The SIGs are fast becoming one of the main attractions of ISMB, with topics as varied as the ‘‘Bioinformatics Open Source Conference,’’ ‘‘Alternative Splicing,’’ and ‘‘Bioinformatics and Disease.’’ Some 653 delegates arrived in Detroit early for these oneor twoday focused sessions—485 of whom then stayed on for ISMB. The ‘‘Alternative Splicing’’ SIG had the highest number of attendees, but the beauty of this year’s SIG arrangements was the ability for all registrants to move among SIGs to capture more of a knowledge base on the varied topics. If you’d like to view the full list of options that were available this year, the preconference SIG abstracts are available at http:// www.iscb.org/ismb2005/sigs.html. Also serving as a precursor to the main conference were 14 half-day tutorial sessions that were attended by 548 delegates. New and interesting tutorials are sought for each ISMB, and this year’s tutorial abstracts on the conference Web site (http://www.iscb. org/ismb2005/tutorials.html) include a link to the original proposal selected for one of the limited tutorial slots. If you are interested in presenting a tutorial proposal for 2006, we encourage you to follow the links at http://www.iscb.org/ismb2005/tutorials. html to gather ideas on preparation and submission of tutorials. Then watch for the call for tutorials via Email and on the ISMB 2006 Web site. A very special few were recognized for their outstanding papers with the GlaxoSmithKline Bioinformatics Prizes for best paper. ISCB congratulates the winners (see Box 1). ISCB also bestowed its two highest awards of recognition during the conference. The Overton Prize, for a scientist in the early-to-midcareer stage, went to Ewan Birney, and the Senior Scientist Accomplishment Award went to Janet Thornton, both from the European Bioinformatics Institute (EBI). This was the first time both awards had gone to scientists at the same institution—a remarkable statement about the quality of science being generated from the EBI. Both award winners gave keynote lectures on the final day of ISMB—a very fitting way to end a successful conference. Please commit to joining us for ISMB 2006 in Fortaleza, Cearà, Brazil, August 6–10 (http://www.iscb.org/ismb2006). Program co-chairs Phil Bourne, Editorin-Chief of PLoS Computational Biology and Professor of Pharmacology at University of California at San Diego, and Søren Brunak, Director of the Center for Biological Sequence Analysis of the Technical University of Denmark, are working together with conference chair Goran Neshich of Empresa Brasileira de Pesquisa Agropecuária to Box 1. GlaxoSmithKline Bioinformatics Winners",2005,PLoS Comput. Biol.
On the open-source landscape of PLOS Computational Biology,"Over the past year, I (M.B.) have been investigating the landscape of code-sharing in academic journals across different research fields. At the end of my PhD, I made the choice to share code that reproduces figures from one of my papers [1], and since then, I’ve been involved in several open-source projects (qMRLab and AxonDeepSeg) and initiatives dealing with open science in publishing (NeuroLibre and Canadian Open Neuroscience Platform). Recently, following an editorial by N.S. on reproducibility and the future of MRI research [2], we wrote a blog post presenting an analysis of the open-source landscape for the journal Magnetic Resonance in Medicine (MRM), which broadly focuses on MRI research for medical applications. These findings provided a snapshot of the current state of the open-source landscape for that journal (e.g., most used coding language is still MATLAB) and some insights into new trends (12% of the articles shared code that reproduced figures). In this editorial, we examine the open-source landscape of PLOS Computational Biology. PLOS Computational Biology is inherently different from MRM not only because of the difference in research topics, but also because it’s an openaccess journal that focuses primarily on computational studies. The broad questions that were of interest are the following:",2021,PLoS Comput. Biol.
Submit a Topic Page to PLOS Computational Biology and Wikipedia,"1 Data Science Institute, University of Virginia, Charlottesville, Virginia, United States of America, 2 Vlaams Instituut voor Biotechnologie-Vrije Universiteit Brussel Centre for Structural Biology, Brussels, Belgium, 3 Institute of Computing Science, Poznan University of Technology, Poznan, Poland, 4 European Centre for Bioinformatics and Genomics, Poznan University of Technology, Poznan, Poland, 5 Institute of Bioorganic Chemistry, Polish Academy of Sciences, Poznan, Poland, 6 University College London, London, United Kingdom, 7 University of Lausanne, Lausanne, Switzerland, 8 Swiss Institute of Bioinformatics, Lausanne, Switzerland",2018,PLoS Comput. Biol.
PLOS Computational Biology 2016 Reviewer and Editorial Board Thank You,"PLOS and the PLOS Computational Biology team would like to express our sincere gratitude to everyone who lent us their expertise as an associate editor, as a guest editor, or as a peer reviewer in 2016. We are fully dependent on the voluntary efforts of these experts to publish the journal for the academic community. Over the past year, PLOS Computational Biology relied on 403 editorial board members and guest editors to manage the 1,715 articles submitted to the journal, which were assessed by more than 2,500 reviewers. Their efforts enabled the publication of more than 580 rigorous, Open Access papers. The names of our 2016 editors that handled submitted manuscripts appear in the Supporting Information as S1 Editor List and as S1 Guest Editor List. Our reviewers appear in the Supporting Information as S1 Reviewer List. We are grateful for their dedication, generosity, and support of Open Access science. Thank you all.",2017,PLoS Comput. Biol.
Ten Years of PLoS‡ Computational Biology: A Decade of Appreciation and Innovation,"‡ Since the founding of PLoS Computational Biology, our publisher, the Public Library of Science, has evolved its own gloss from the piquantly camelesque “PLoS” to the modern “PLOS,” matching its equally emphatic journal ONE. As we review the journal, it seemed appropriate to hearken to its uncertain beginnings, embodied even in its quirky typography. 
 
Ten years ago, we founded PLoS Computational Biology to advance the science of computational biology and its practitioners. Our goals were appreciation and innovation. At the remove of no longer being active in day-to-day activities of the journal, it is wonderful to look back over the first ten years of the journal. Here we reflect on the journal’s evolution and offer a few thoughts for the next ten years. 
 
The journal began at a propitious time, as computational biology engaged markedly more researchers and as scientific publication was beginning a tectonic shift. Many researchers noted a serious gap in the publishing firmament: there was no respected venue that focused on biological discovery made by computational methodology. This lack of appreciation had implications at many levels. At pre-eminent journals, computational biology research was often was not reviewed by those with expertise to evaluate its technical content and significance, leading to delays and rejections. The alternative of publishing at the existing specialist journals often led to publications in venues that were not appreciated by the most appropriate audience. This meant that the work was not seen by many suitable readers, and that computational biologists had greater challenges in their career development. 
 
We are pleased that PLoS Computational Biology is now widely appreciated as a treasury of first-class scientific work, offering a respected venue for publication of work fundamental to our field. For researchers at all tiers, but especially trainees, publication venue is too often taken as a proxy for the significance of their research studies. Such individuals have no time for their papers to accrue citations or become well recognized before their achievements are assessed for career advancement. We hope that those who have published in PLoS Computational Biology have met with success comparable to the burnishing their outstanding work has provided for the journal’s reputation. 
 
Even more important, we have been gratified to see researchers in our field have their work treated with respect and insight by reviewers and editors who understand the significance of their research and its approaches. Certainly, in a journal that rejects two-thirds of the submissions, not all prospective authors are happy, and undoubtedly there have been some significant oversights. However, on the whole, we believe that PLoS Computational Biology has succeeded in providing computational biology researchers consistently informed and thoughtful reviews. 
 
In order to meet the needs of our field’s researchers, our intent was to establish a computational journal that was read by experimentalists and computational researchers alike, and would drive biologists’ interest in the application of computational techniques. Another early motivator was also not to encroach on existing bioinformatics journals, hence, the niche of computation requiring new biological insights. To this end, PLoS Computational Biology initially had a scope statement that emphasized the need for biological outcomes. However, as both our journal and bioinformatics journals became more established, the editorial board increasingly felt we should expand the remit to engage pure methods of the greatest significance. The expansion of authors and readers has been clearly enhanced by embracing methods papers. 
 
A pleasant and unexpected surprise was the scientific scope of papers submitted to the journal. For example, we never envisaged that computational neuroscience would become such a vibrant part of the journal. In retrospect, this would seem to be the result of a combination of lack of suitable venues publishing such papers and the hard work of the early neuroscience editors. The scope has continued to expand even as the number of journals competing has multiplied. An imminent challenge will likely be understanding the relationship with biomedical informatics as the line between basic and clinical research blurs in a translational world. A broader challenge will arise as biology becomes ever more a data science: what should the unique role of the journal be as nearly all biologists become computational biologists? 
 
The journal began at a time when open access was still considered by most scientists at best a novelty and at worst a business model doomed to failure. Against this backdrop, many computational biologists rallied behind the cause since they were already accustomed to open and free data and software. A particularly important leap was made by the International Society for Computational Biology (ISCB), which was a key founding partner. Not only did ISCB offer a community supporting the journal, but in doing so, they abandoned the very substantial revenues they had previously garnered from partnership from a journal that was then closed-access. Valuing access over profit is to be highly commended. 
 
In the ten interceding years, open access has evolved and is now a mainstream publishing option, which has undoubtedly increased access to scientific discourse. Importantly, PLoS Computational Biology has the most elevated tier of open access, using a CC-By license. Amongst other benefits, for our community, this offers machine access to the corpus for knowledge discovery, an undertaking that remains at a nascent stage. Let us hope that such value is commonplace and plain for all to see before the next ten-year anniversary. 
 
The front matter has been a wonderful sandbox in which to experiment with scholarly communication. Not all such experiments have been successful, but some clearly have and can be attributed to both filling a need (not necessarily obvious at the outset) and folks who believed in the experiment and who put in many hours of hard work on behalf of the community. The Education section and the Ten Simple Rules professional development series have blossomed beautifully. The jury is still out on the success of other experiments, but they all speak to doing things differently, which has been a hallmark of the journal since day one. 
 
The science of computational biology and the dissemination of that science have changed over the past ten years, and the journal has been there to document and enable those changes, and more disruption will follow. The next ten years will see a further broadening of the scope of what is covered by the field of computational biology, particularly as it relates to the generation and use of a broader range of data types by all investigators, with increased emphasis on the use of clinical data. We hope that more of the research life cycle will be shared and captured as part of a publication. If dreams come true, a journal paper will offer a persistent mechanism for further data exploration and be capable of offering a platform for follow-on sets of experiments. We are sure your community journal will be ready for whatever innovations you can bring. 
 
It was a privilege for us to work with so many talented authors, reviewers, editors, and staff—and an honor to see our initial efforts taken to such wonderful levels. All have contributed both appreciation and innovation and, in doing so, helped create a community larger than the journal itself. All of their efforts have truly elevated the field and supported us.",2015,PLoS Comput. Biol.
How Can PLOS Computational Biology Help the Biological Sciences?,"A year has passed since I took over as the Editor-in-Chief of PLOS Computational Biology, making it a good time to reflect on the journal, reassess the needs of our community, and, broadly, the direction of computational biology within the framework of the biological sciences. The rapid growth in computational power, the masses of data that need to be understood, and the advances in the biological sciences all point to a need to reevaluate directions, merging these with the fast pace of discoveries made by experimental studies. Are we, as computational biologists, contributing as much as we can to the advancement of the sciences in key areas? Since PLOS Computational Biology is a broad, community-based open-access journal, the areas of focus that we deem central to the field may impact the future of computational biology. 
 
The biological sciences increasingly shift to research projects that aim to understand the causes and the mechanisms of diseases and to discover therapeutics. This shift over recent years is fueled both by the desire to alleviate human suffering and by the preferences of funding agencies. Within this framework, computational biology can effectively contribute to the understanding of a broad range of biological processes under normal physiological conditions and in disease; and it can do this across a range of scales, from the molecular and biochemical to the organismal and population levels. As a leading journal in the computational sciences, PLOS Computational Biology can, and, I believe, should, foster a highly stimulating and interactive environment among computational and experimental scientists in our community toward these aims. 
 
Computational biology increasingly gains the scientific center stage. It is an exciting interdisciplinary field that draws scientists from different fields: physics, chemistry, mathematics, engineering, computer science, and biology. Computational biology aims to organize and make sense of huge amounts of data and large-scale cellular processes and pathways, and, at the same time, to understand biological phenomena on the atomic scale. Experiments obtain information at multiple levels. Computational biology is a quantitative field. The goals of computational biology are to distinguish between noise and signals, obtain and quantify trends, and put these together, so that we are able to figure out how the information flows, how the processes are regulated, and what goes wrong in disease. We would like to understand the mechanisms through which mutations lead to cell proliferation, how the signals transmit in the cellular network and how external stimuli translate to cellular differentiation and to turning genes on and off, and how viruses enter cells. However, beyond these, computational biology aims to use the information to make predictions and obtain experimentally testable models. 
 
PLOS Computational Biology embraces all areas of computational biology. However, I believe that to help the advancement of the biological sciences, the journal should consider papers that address biologically relevant questions that are of broad interest and provide new concepts that can guide the design of experiments. This has been in our scope since the inception of the journal, and we shall continue to follow these guidelines. It is emboldening to think of open questions in the biological sciences, particularly those relating to diseases, where computational biology can drive progress. 
 
PLOS Computational Biology is highly regarded by the scientific community. We should aim to not only retain this appreciation but to further it. In response to requests by our community, the first step that I took after taking over as the Editor-in-Chief was to introduce a Methods section, led by Thomas Lengauer. Under Thomas' leadership, this section is flourishing, and is fast becoming highly popular, with an increasing number of submissions. The section strives to publish only the most outstanding methods, aiming to consider only those that are of exceptional quality. We are now also actively engaged in reducing the response times to authors, publication processing, and in simplifying the submission process. 
 
The content of PLOS Computational Biology is important not only for scientific advancement; it also bears on our students who should be exposed to a variety of ways to define, analyze, question, and solve relevant scientific research problems computationally, relying on experimental data. For PLOS Computational Biology to help and inspire the biological sciences, we, as a community, should remain tuned-in to research trends and new data, and take action. These goals are shared with the International Society for Computational Biology (ISCB), with whom we are proud to partner. Together, we hope to make a difference. 
 
Finally, the achievements of PLOS Computational Biology would not have been possible had it not been for the devoted journal staff, who are always there to help, guide, suggest, and follow on initiatives, and our extended Editorial Board. Armed with these, and embracing our community, our open-access PLOS Computational Biology journal will continue to thrive.",2013,PLoS Comput. Biol.
The PLOS Computational Biology Software Section,"As a member of the PLOS family, PLOS Computational Biology promotes open and unrestricted access to scientific publications and the research products that support them. For PLOS Computational Biology, this specifically includes computational methods and the software that implements them. To foster a culture of open exchange and reuse of software, we created a new category of manuscripts called Software Articles. We have been accepting submissions in this category for over a year now, and all articles published in this category are now also available as an online collection (http://www.ploscollections.org/software). 
 
This is a collection of articles spanning a wide range of different topics in computational biology, starting from video image analysis in “Automated Tracking of Whiskers in Videos of Head Fixed Rodents” [1], over to simulation and analysis of biological system models in “Hybrid Models and Biological Model Reduction with PyDSTool” [2]. We have published articles describing genomic tools, such as “Podbat: Genomic Tool for Epigenetic Meta-Analysis” [3] and “Exploring Massive, Genome Scale Datasets with the GenometriCorr Package” [4], computational chemistry, in “AutoClickChem: Click Chemistry in Silico” [5], and protein science, in “ProteinHistorian: Tools for the Comparative Analysis of Eukaryote Protein Origin” [6] and “CAVER 3.0: A Tool for the Analysis of Transport Pathways in Dynamic Protein Structures” [7]. As new Software Articles get published in this journal, they will be added to this collection.",2012,PLoS Comput. Biol.
New Methods Section in PLOS Computational Biology,"PLOS Computational Biology is the Public Library of Science journal that targets new biology that is facilitated by computational methods. Since the inception of the journal, biology has been at the center of the scope of PLOS Computational Biology. Thus, each submission is required to put advances in biology into its focus in a very concrete manner and not only focus on computation. This is in contrast to other, more technically oriented journals in the area of computational biology. 
 
The new Methods section of the journal acknowledges the fact that a major methodical advance can, in itself, be so relevant that it deserves transcending the technological domain and being presented to a broader readership including not only computational biologists, but also biologists, the targeted readership of this journal. For this reason, PLOS Computational Biology has installed a special type of submission, the Methods paper. As the scope statement of the journal spells out, “Methods papers should describe outstanding methods of exceptional importance that have been shown, or have the promise to provide new biological insights. The method must already be widely adopted, or have the promise of wide adoption by a broad community of users. Enhancements to existing published methods will only be considered if those enhancements bring exceptional new capabilities.” 
 
In order to render the processing of Methods papers as effective as possible and to limit the effort required on the part of authors and reviewers, a presubmission inquiry is mandatory for Methods papers. In such an inquiry, the authors are requested to present a concise abstract-like statement on what the manuscript they plan to submit entails and why the authors feel that it fits the Methods section of the journal. Presubmission inquiries are given top priority in the paper handling process; the median processing time should be about a week. Within that time, submission is either encouraged or discouraged. 
 
The Methods section of the journal is handled by Deputy Editor Thomas Lengauer.",2013,PLoS Comput. Biol.
Topic Pages: PLoS Computational Biology Meets Wikipedia,"While there has been much debate about the coverage and quality of Wikipedia (starting with an article in 2005 [1]), there is no doubt about its value (and increasing role) as a reference source and starting point for in-depth research. For example, within the biomedical sciences, there have been recent articles about the accuracy and completeness of drug information in Wikipedia [2], Wikipedia as a source of information in nursing care [3] and mental disorders [4], and making biological databases available through Wikipedia [5].",2012,PLoS Comput. Biol.
Ten quick tips for harnessing the power of ChatGPT in computational biology,This is a PLOS Computational Biology Methods paper,2023,PLoS Comput. Biol.
A Future Vision for PLOS Computational Biology,"With much trepidation I accepted the great honor and responsibility of becoming Editor-in-Chief of PLOS Computational Biology. I am fully aware of how hard it will be to step into the shoes of Philip Bourne, the Editor-in-Chief of the journal for the last seven years, since it was founded by him, Steven Brenner, and Michael Eisen. We are all deeply appreciative and thankful to Phil for his unique role; and we are grateful that he will be continuing his association with the journal in the future in the role of Founding Editor-in-Chief, aiding and inspiring us and the PLOS Computational Biology community. As a Deputy Editor-in-Chief, I became aware of the true family relationship in the broad PLOS organization and of the devoted and gifted editorial force so nicely fostered by Phil in PLOS Computational Biology. These played a crucial role in helping me decide to accept the invitation to become Editor-in-Chief. 
 
I am deeply committed to the underlying principle of free public access to scientific information. In particular, what is truly unique and special about PLOS Computational Biology is that it fulfills this mission while maintaining the highest standards of scientific rigor, originality, and clear biological relevance. As Editor-in-Chief I will do my best to have PLOS Computational Biology continue these traditions. 
 
Computational biology is often perceived as a single field; this however is not the case. Like experimental biology, computational biology is enormously broad; the only distinction from experimental biology is the means. This has disadvantages and advantages: the main disadvantage is that conclusions based on computations are often treated by biologists as less conclusive than those based on experiments; the main advantages are that computations allow researchers to analyze vast amounts of data and make testable predications, and they allow researchers to address problems that current experimental methods may not be able to treat. The high quality of papers published in PLOS Computational Biology indicates that the apparent disadvantage is not necessarily there. While they may still need further experimental and computational validation, conclusions based on rigorous computations applied to carefully assembled and curated data, which are backed up by available experimental results, can be as reliable, insightful, and biologically significant as those based on experiments. 
 
PLOS Computational Biology is broad; it addresses diverse biological problems. We look forward to further expanding its scope through the inclusion of outstanding methods and resource papers, opening up significant new research directions while retaining and enhancing the strong scientific merit of PLOS Computational Biology publications. We further plan on improving the pace of submissions processing. PLOS Computational Biology is recognized by the community as the premier journal in computational biology. We will strive to continue in this tradition. 
 
PLOS Computational Biology is tightly associated with the International Society of Computational Biology (ISCB). We cherish and will continue fostering this association. The Society, its meetings, and the journal all have a common goal: enhancing and promoting excellence in computational biology. Outstanding research with clear biological relevance, which leads to fundamental new insights into important biological problems, is the hallmark of future contributions by our community to biology. As the Editor-in-Chief I shall do my utmost to achieve these goals.",2012,PLoS Comput. Biol.
A Review of 2011 for PLoS Computational Biology,"In 2011, during discussions at various conferences, as well as informally with authors, readers, reviewers, and editors, we were struck by one resonating theme: the view that PLoS Computational Biology has helped to create a sense of community amongst a broad group of scientists and educators. While the journal labels itself as a PLoS “community” journal, if that label has any true meaning, it must come from the community itself. We feel that, after six years, we are indeed serving the community well, but as always you can disagree at any time, either publicly with a comment in response to this article or by email (gro.solp@loibpmocsolp). 
 
That service comes first and foremost from the research we publish, but also from our desire to educate, report on open-source software, provide a history of the field, capture the vision of our editors, and move beyond the boundaries of traditional publishing to inform people within and outside of our community. Before we take a look at developments in each of these areas, and what is to come in 2012, let us first review how we served the community in 2011. 
 
According to Google Analytics, 2011 saw over 553,000 unique visitors to our website and more than two million article views (not including access statistics from PubMed Central). Visitors came from 211 countries/territories, which was undoubtedly helped by the fact that the journal is open access. India, Spain, Russia, and Iran each showed over a 40% increase in visitors from the previous year. From the journal website, the most accessed Research Article was “Effect of Promoter Architecture on the Cell-to-Cell Variability in Gene Expression” by Sanchez et al. [1], published in March 2011 (8,954 views at the time of writing); the most accessed article overall was “Ten Simple Rules for Building and Maintaining a Scientific Reputation” by Bourne and Barbour [2], published in June 2011 (15,255 views at the time of writing). 
 
Also in 2011, 1,623 research articles from 57 countries were submitted, up 16% from 2010, and 384 were published (down 2% from 2010). Receiving more but publishing about the same number in real terms should reflect the increasing quality of our content. We are very grateful to our Associate Editors, Guest Editors, reviewers (a list of Guest Editors and reviewers from 2011 is available in Table S1), and, of course, our Deputy Editors – Patricia Babbitt, Joel Bader, Sebastian Bonhoeffer, Lyle J. Graham, Konrad Kording, Douglas Lauffenburger, Uwe Ohler, Nathan Price, Burkhard Rost, Olaf Sporns, Wyeth Wasserman, and Weixiong Zhang – for helping us to handle this growth. With this growth, we have not met our goal of reducing the times to first decision, even with the addition of new editors, but we will continue to work on this in 2012. Our median decision before review time in 2011 was 8 days, and our median decision after review time was 47 days. 
 
A number of our Research Articles were featured in blogs and the popular press. Notably, Mitra Hartman's paper on the morphology of the rat vibrissal array [3] was covered extensively, including two videos, by National Public Radio and Science Bytes. 
 
Our Software section was launched in August 2011, and we have so far published one article, with six more either accepted or under review. Uptake has been relatively slow, based on, we believe, the open source and stringent documentation requirements we have imposed. We believe it is better to publish only a few, but high-quality, software articles, and that this will highlight the lack of rigor of software otherwise in the field. 
 
Our Education section has continued to flourish, in part because of the journal's relationship with the International Society for Computational Biology (ISCB). This year we introduced a collection, Bioinformatics: Starting Early, which takes the notion of biology as a computational science into secondary schools. We are hoping for more articles from those involved in secondary teaching in 2012. Open science removes all boundaries not only to reading the latest science, but also to contributing to that science. We have even seen secondary school students as authors and expect to see more in the future. 
 
In July 2011 we began the Editors Outlook series, with five published [4]–[8] and more on the way. These mini-reviews already broach subjects from ontologies to genome organization, and from evolution to data and privacy. They speak to the breadth of our field and editorial board, and collectively will form a vision from our many expert editors of what is being, and will be, accomplished in the coming years. 
 
That our journal is fully open access provides opportunities for maximizing the use and reuse of our scholarship; we intend to explore this further in 2012. Early in 2012 we will launch our first Topic Page on circular permutations in proteins. Wikipedia is a valuable resource for knowledge dissemination, yet Wikipedia pages are lacking in coverage of computational biology. In part this is because authors gain little career-based reward for creating Wikipedia pages. We aim to bridge the gap. Topic Page articles, which will be published in the journal and will each receive a PubMed identifier and DOI, will become the copy of record, thereby crediting the author(s). At the same time the Topic Page will be used to seed a Wikipedia article and become a living version of the same material–a viable option thanks to our Creative Commons license. Look for an announcement of this development in the new year, but in the interim if you have ideas for Topic Pages you would like to contribute, please do get in touch for further information (gro.solp@loibpmocsolp). 
 
We are also contemplating a new article type: Data Pages. Data Pages would be brief publications about datasets, in which the data are not already well described in other papers yet are considered of great value to the community. Such brief publications would bring a traditional reward to the producers of these shared datasets. Which is more valuable: a dataset downloaded and used by 100 investigators, who in turn publish research based on these data, or a paper that is cited only by the authors who wrote it? Data Pages would, from our point of view, help to answer this question. 
 
If you want to provide feedback on our plans for Data Pages later in 2012, please do so by commenting on this article. Feel free to comment in public or to us privately on anything we are doing, or ideas that you have for the future of the journal. After all, PLoS Computational Biology is a community journal, and if you have read this far, you should consider yourself an important part of our ever-broadening community.",2012,PLoS Comput. Biol.
PLoS Computational Biology: A New Community Journal,"Citation: Bourne PE, Brenner SE, Eisen MB (2005) PLoS Computational Biology: A new community journal. PLoS Comput Biol 1(1): e4.",2005,PLoS Comput. Biol.
A Review of 2010 for PLoS Computational Biology,"PLoS Computational Biology celebrated its fifth anniversary in 2010, and all in our community, either as readers, authors, or editors, should take pride in what has been accomplished in such a short space of time. In the past year we received 1,403 new Research Articles, a 295% increase from our first year of operation in 2005–2006 and a 17% increase over 2009. Of the articles submitted in 2010, 875 (62%) were rejected, and 70% of these were before review. We have seen growth not only in submissions, but in readership as well. Currently, around 16,000 readers receive the electronic table of contents, a 14% increase over the previous year. We published 392 Research Articles this year, along with 23 “front section” articles (Reviews, Perspectives, Education), down from 33 in the previous year. Eighty Associate Editors handled the combined submissions, with a total of 26 new editors joining this past year and six departing. We are proud to say that virtually every editor we asked to join accepted, a testament to how our community values the journal. These editors worked with more than 180 guest editors and 1,800 reviewers to handle the submissions, and we are of course very grateful for their support (Table S1). 
 
Table 1 provides a list of Research Articles we have published since 2005 through October 2010 that have accrued over 10,000 downloads and shows the diversity of highly accessed papers published by the journal. Note that these are downloads from the PLoS Web site only, and do not include downloads from PubMed Central. Readers are free to review download statistics for all research and non-research articles published across the PLoS journals through the Microsoft Excel spreadsheet that can be found at http://www.ploscompbiol.org/static/plos-alm.zip. Individual article metrics and comments are available from the respective tabs associated with each article. 
 
 
 
Table 1 
 
List of published Research Articles that have accrued over 10,000 downloads since launch. 
 
 
 
In 2010, we launched two new features to enrich the journal: “The Roots of Bioinformatics” and “PLoS Conference Postcards”. The Roots of Bioinformatics was eloquently introduced by the Series Editor, David B. Searls, in June [1] and was followed in July by Russell F. Doolittle’s insightful reflections on the roots of protein evolution, which went back as far as the 1950s when chemistry, rather than computers, ruled [2]. More such reflections will follow in 2011. Conference Postcards act as a counterpoint to the rich roots retrospectives by providing current views of the field of computational biology, as young scientists present crisp perspectives on what they perceive as conference highlights. We published Postcards from January’s Pacific Symposium on Biocomputing (PSB) meeting held in Hawaii [3] and from the Intelligent Systems for Molecular Biology (ISMB) meeting held in Boston in July [4]. At the latter we learnt about various sessions held at ISMB, namely the Highlights session, the ISCB Student Council Symposium’s “speed dating” event, and reports from Satellite meetings. We look forward to digging deeper and receiving Postcards from further afield in 2011. 
 
PLoS Computational Biology continues its strong relationship with the International Society for Computational Biology (ISCB) through postings on its Web site and activities at ISMB. At ISMB 2010 in Boston, PLoS Computational Biology ran a Workshop entitled “Where and How to Get Published” in which we endeavored to make the path to getting published a little less inscrutable. The first half was led by journal co-founders Philip E. Bourne and Steven E. Brenner and provided guidelines on how to write a good paper, and the second half included questions and advice from editors and authors from a range of career stages, which resulted in a broad discussion of what journals want and the state of publishing today. Presenters’ materials from the Workshop are available on the new PLoS Blog (http://blogs.plos.org/plos/2010/10/materials-from-plos%E2%80%99-workshop-at-ismb-2010/). 
 
We have three major goals for 2011. First, to reduce the time to decision for submitted manuscripts, which currently averages 10 days for those papers rejected before review and 40–50 days for those reviewed. Second, to introduce a new section called “Editors’ Outlook”, which are invited mini-reviews from members of our Editorial Board who will provide insights into their respective fields, discussing what is hot and what we can expect going forward. Collectively, these will provide an ongoing and insightful look into the broad and rapidly expanding field of computational biology—a field of endeavor the journal is proud to serve. 
 
Specifically, current experimental techniques are leading to an unprecedented increase in the rate at which data are becoming available. When combined with the vast growth in computational power, we can expect rapid growth in computational papers. Computational biology is the area that helps in organizing the data, in making sense of observations, and in using these to make experimentally testable predictions. Our third goal is to keep abreast of these developments and keep PLoS Computational Biology the number one journal in the field.",2011,PLoS Comput. Biol.
PLoS Computational Biology: A New Community Journal,"Citation: Bourne PE, Brenner SE, Eisen MB (2005) PLoS Computational Biology: A new community journal. PLoS Comput Biol 1(1): e4.",2005,PLoS Comput. Biol.
A Review of 2010 for PLoS Computational Biology,"PLoS Computational Biology celebrated its fifth anniversary in 2010, and all in our community, either as readers, authors, or editors, should take pride in what has been accomplished in such a short space of time. In the past year we received 1,403 new Research Articles, a 295% increase from our first year of operation in 2005–2006 and a 17% increase over 2009. Of the articles submitted in 2010, 875 (62%) were rejected, and 70% of these were before review. We have seen growth not only in submissions, but in readership as well. Currently, around 16,000 readers receive the electronic table of contents, a 14% increase over the previous year. We published 392 Research Articles this year, along with 23 “front section” articles (Reviews, Perspectives, Education), down from 33 in the previous year. Eighty Associate Editors handled the combined submissions, with a total of 26 new editors joining this past year and six departing. We are proud to say that virtually every editor we asked to join accepted, a testament to how our community values the journal. These editors worked with more than 180 guest editors and 1,800 reviewers to handle the submissions, and we are of course very grateful for their support (Table S1). 
 
Table 1 provides a list of Research Articles we have published since 2005 through October 2010 that have accrued over 10,000 downloads and shows the diversity of highly accessed papers published by the journal. Note that these are downloads from the PLoS Web site only, and do not include downloads from PubMed Central. Readers are free to review download statistics for all research and non-research articles published across the PLoS journals through the Microsoft Excel spreadsheet that can be found at http://www.ploscompbiol.org/static/plos-alm.zip. Individual article metrics and comments are available from the respective tabs associated with each article. 
 
 
 
Table 1 
 
List of published Research Articles that have accrued over 10,000 downloads since launch. 
 
 
 
In 2010, we launched two new features to enrich the journal: “The Roots of Bioinformatics” and “PLoS Conference Postcards”. The Roots of Bioinformatics was eloquently introduced by the Series Editor, David B. Searls, in June [1] and was followed in July by Russell F. Doolittle’s insightful reflections on the roots of protein evolution, which went back as far as the 1950s when chemistry, rather than computers, ruled [2]. More such reflections will follow in 2011. Conference Postcards act as a counterpoint to the rich roots retrospectives by providing current views of the field of computational biology, as young scientists present crisp perspectives on what they perceive as conference highlights. We published Postcards from January’s Pacific Symposium on Biocomputing (PSB) meeting held in Hawaii [3] and from the Intelligent Systems for Molecular Biology (ISMB) meeting held in Boston in July [4]. At the latter we learnt about various sessions held at ISMB, namely the Highlights session, the ISCB Student Council Symposium’s “speed dating” event, and reports from Satellite meetings. We look forward to digging deeper and receiving Postcards from further afield in 2011. 
 
PLoS Computational Biology continues its strong relationship with the International Society for Computational Biology (ISCB) through postings on its Web site and activities at ISMB. At ISMB 2010 in Boston, PLoS Computational Biology ran a Workshop entitled “Where and How to Get Published” in which we endeavored to make the path to getting published a little less inscrutable. The first half was led by journal co-founders Philip E. Bourne and Steven E. Brenner and provided guidelines on how to write a good paper, and the second half included questions and advice from editors and authors from a range of career stages, which resulted in a broad discussion of what journals want and the state of publishing today. Presenters’ materials from the Workshop are available on the new PLoS Blog (http://blogs.plos.org/plos/2010/10/materials-from-plos%E2%80%99-workshop-at-ismb-2010/). 
 
We have three major goals for 2011. First, to reduce the time to decision for submitted manuscripts, which currently averages 10 days for those papers rejected before review and 40–50 days for those reviewed. Second, to introduce a new section called “Editors’ Outlook”, which are invited mini-reviews from members of our Editorial Board who will provide insights into their respective fields, discussing what is hot and what we can expect going forward. Collectively, these will provide an ongoing and insightful look into the broad and rapidly expanding field of computational biology—a field of endeavor the journal is proud to serve. 
 
Specifically, current experimental techniques are leading to an unprecedented increase in the rate at which data are becoming available. When combined with the vast growth in computational power, we can expect rapid growth in computational papers. Computational biology is the area that helps in organizing the data, in making sense of observations, and in using these to make experimentally testable predictions. Our third goal is to keep abreast of these developments and keep PLoS Computational Biology the number one journal in the field.",2011,PLoS Comput. Biol.
PLoS Computational Biology Conference Postcards from ISMB/ECCB 2011,"This July, PLoS Computational Biology invited attendees of ISMB/ECCB 2011 (http://www.iscb.org/ismbeccb2011) to send us short reports of conference highlights in the guise of PLoS Conference Postcards. Philip E. Bourne, Editor-in-Chief, selected three Postcards, which we received from Poland, Germany, and the United States of America. If the reports below capture your interest, you can find Postcards from past conferences in our recent collection: http://collections.plos.org/ploscompbiol/conferencepostcards.",2011,PLoS Comput. Biol.
Ten simple rules for defining a computational biology project,a PLOS Computational Biology Methods paper.,2023,PLoS Comput. Biol.
A Review of 2009 for PLoS Computational Biology,"2009 was another strong year for PLoS Computational Biology. As in 2008, we saw growth and development at every turn—our submissions and publishing presence, the level of quality of the work we considered and published, the degree of community engagement with the journal, and our editorial leadership. 
 
As we briefly review the year past and reveal plans for the year to come, one conclusion is undeniable: we are truly a community journal, the achievements of which result from and depend on the contributions of our authors, readers, editors, and reviewers. Our thanks to you all for your continued support, trust, and partnership in advancing the field in which we work. 
 
PLoS Computational Biology's growing position in the field is evidenced by the statistics for 2009. We recorded 1,204 research articles submitted to the journal—an increase of 31% from 2008. Of these submissions, we published a total of 344 (29%), along with 33 Reviews, Perspectives, and Education articles. Of the total full submissions, 41% were rejected without peer review. Such an increase in our submission volume prompted the addition of 11 new editors to our Editorial Board, the recruitment of 128 guest editors, and the support of 1,616 reviewers (see Table S1). And, importantly, our readership grew this year as well. The number of readers receiving our new article alerts now exceeds 14,000, and our press coverage worldwide alerted countless more to the high-quality science we have the privilege to publish. 
 
By traditional measures comprising a variety of “factors,” whether impact, Eigen, or H, the journal is doing very well and represents an important open-access contributor to the field. There are now more important metrics, however, by which to measure the quality of a published paper: the usage statistics and other measures of community response now available at the article level. In the Fall of 2009, PLoS introduced article-level metrics across all journals, making it possible to see, among other metrics, the number of views and downloads each paper receives in real time. The “Metrics” tab on each article presents a summary of all activity post-publication, which has proven to be of great interest to the readers and authors alike. 
 
Also available on our Web site is a summary Excel file of the journal's entire corpus (see http://www.ploscompbiol.org/static/journalStatistics.action#PLoSCompBiol), which provides the opportunity for some very interesting analyses. For example, some highly downloaded articles, like the Ten Simple Rules series, are not likely to be cited frequently but consistently draw readers. This brings the issue of scientific merit and reward sharply into focus, raising the question, What does indeed represent a scientific contribution? Taking a different view of the data, the average number of downloads for any article is over 2,000 with a strong showing in mathematically oriented articles and those relating to genetics and genomics. These divisions are based on author-provided keywords and tell only part of the story of what is “hot” in our field. The summary statistics show clearly that computational neuroscience continues to be a strong area of the journal with modeling of biological systems at various scales a definite sphere of growth. We encourage you to view the data and perform your own analyses—and let us know your results. 
 
Our relationship with the International Society for Computational Biology (ISCB) remains strong. In 2009, we continued to publish Messages from ISCB on a variety of topics and, as in years past, contributed to the Society's annual meeting, Intelligent Systems for Molecular Biology (ISMB) 2009, in Stockholm, Sweden. This year, PLoS Computational Biology organized a special session on “Advances and Challenges in Computational Biology,” chaired by Deputy Editor Barbara Bryant. Three members of the journal's Editorial Board—Abigail Morrison, Adam Arkin, and Donna Slonim—highlighted recent scientific advances made possible by computation and mathematics in the respective fields of computational neuroscience, synthetic biology, and translational medicine in human development. 
 
With 2009 behind us, we look forward to continued strength in another area of the journal—our non-research articles. To add to our popular Editorial, Education, and Review series, we will be introducing some exciting features in 2010 that we hope will appeal to our broad readership. An ongoing “Roots of Bioinformatics…” series, edited by David Searls, will provide insights into how various areas of the discipline developed. These will be personal perspectives from scientists who helped to shape the field and will be compelling and inspirational reading for those entering or thinking of entering this vibrant arena. A “Postcards from” feature, designed to capture the highlights of important computational biology conferences, will provide the opportunity for the younger members of our community to comment and offer a fresh perspective on new developments described in presentations and through dialogue. We welcome your feedback and ideas on this new content as the year progresses. 
 
Another, and different, editorial goal in 2010 is to improve our service to our authors, particularly with regard to reducing the time to first decision for papers that we do not intend to consider for publication. Our 2009 records show an average interval of 45 days from submission to a first decision for peer-reviewed manuscripts—and we aim to do better. We are committed to this task, but are equally committed to ensuring that any steps we take do not compromise the quality of the review. To help us serve you better, we encourage the regular practice of submitting a presubmission inquiry rather than a full submission. This feature allows us to preview the paper and offer a far faster response, in a matter of days, as to the likely suitability of your paper for the journal. If you have other thoughts and comments on the state of the journal and what we should be doing—or doing better—we encourage you to use the commenting feature on an article that prompts your comment or, if you prefer, by contacting the editorial office directly at ploscompbiol[at]plos.org. 
 
Thank you once again for your ongoing support. We wish you all well in your research endeavors during 2010.",2010,PLoS Comput. Biol.
PLoS Computational Biology Conference Postcards from ISMB 2010,"The annual international conference on Intelligent Systems for Molecular Biology (ISMB) is the largest meeting of the International Society for Computational Biology (ISCB). In 2010 it was held in Boston, United States, July 11–13. What follows are four conference postcards that reflect different activities considered exciting and important by younger attendees. Postcards, as the name suggests, are brief reports on the talks and other events that interested attendees. You can read more about the idea of conference postcards at http://www.ploscompbiol.org/doi/pcbi.1000746, and if you are a graduate student or postdoctoral fellow, please consider contributing postcards at any future meetings of interest to the PLoS Computational Biology readership. We want to hear your view of the science being presented.",2010,PLoS Comput. Biol.
A Review of 2008 for PLoS Computational Biology,"A very Happy New Year to all our authors, readers, editors, and reviewers from everyone at the Public Library of Science! 2008 was a remarkable year for PLoS Computational Biology; which saw 50% more submissions than in 2007 (900 full articles and 175 presubmission inquiries), more than 260 high-quality research articles published, and regular contributions of Editorials, Reviews and Perspectives, and Education and Society pages. This growth and maturity of content leaves no doubt that our Journal has become a leading reference for the field of computational biology and a trusted place to publish. 
 
Such success has come through the hard work of our Editors, not only from our Editorial Board but also from the anonymous reviewers and Guest Editors who expend so much time and energy in the assessment of submitted manuscripts (each averaging 2.8 reviews and 1 to 2 rounds of revisions), and from the attention to detail and care taken over the content. 
 
Peer review by external experts is essential to ensuring that the work published in PLoS Computational Biology is of the very highest quality, and we are grateful to all of our reviewers for their thoughtful and informed comments. Guest Editors are those who step in to edit one particular paper that describes work in an area of research that falls outside the expertise of the more than 50 volunteer Editors on our Board. The flexibility and availability of these Guest Editors is invaluable in our being able to provide a high level of review, as well as playing an important role in maintaining the broad appeal and vibrancy of the Journal. Their names can be found together in Table S1 as an acknowledgment of the good work they do and the time they donate to improve the body of scientific literature and knowledge. 
 
In 2008, our pool of reviewers included approximately 1,300 scientists in 36 countries, including Vietnam, Mexico, Brazil, and Afghanistan, as well as in countries such as Israel, Germany, and Japan, where the Journal is better-known. This impressive geographical spread indicates that we are reaching the best of the best across the scientific world, something only a well-respected journal of quality is able to accomplish. 
 
Organic growth requires that we constantly assess both the kinds of papers we accept and the standards of research they represent. We have revised our scope statement to reflect slight changes in our focus (see http://www.ploscompbiol.org/static/information.action), and we constantly refine our Editorial Board (http://www.ploscompbiol.org/static/edboard.action) to handle the number and types of papers we are encouraging. Experiencing solid growth can come at a price to the speed of our Editorial processes, however, and while we aim to provide a decision to our authors within 35 days, some papers defy this time limit. We are confident, however, that with your continued help and support, we will reach our targets more consistently this year. As authors, you appreciate a swift response time, and as reviewers you can help us achieve this by making a commitment in 2009 to return reviews within two weeks. 
 
Looking ahead in 2009, you can expect to see not only more great research, but also greater connectivity between content found in different PLoS journals and among members of your community. As an example of the former, PLoS Computational Biology will be working with PLoS ONE to feature developments in software important to our discipline. For the latter, the community can read and participate in discussions that start when readers post a comment or rating on a published article. 
 
As we have done since our launch, we welcome your feedback on how we're doing and what we should be doing going forward. This is your Journal, and our open philosophy encourages your engagement in it. By working together, we can further establish the importance of our science to our understanding of living systems and make a positive contribution to moving it forward even in these uncertain times. 
 
Once again, many thanks to all of you for your support and commitment to making 2008 a successful year for PLoS Computational Biology and to ensuring that we are able to achieve even more in the upcoming year.",2009,PLoS Comput. Biol.
PLoS Computational Biology Conference Postcards from PSB 2010,"1 Berkeley Phylogenomics Group, QB3 Institute, University of California Berkeley, Berkeley, California, United States of America, 2 Virginia Bioinformatics Institute, Virginia Tech, Blacksburg, Virginia, United States of America, 3 Department of Pharmacology, University of California San Diego, La Jolla, California, United States of America, 4 Skaggs School of Pharmacy and Pharmaceutical Sciences, University of California San Diego, La Jolla, California, United States of America",2010,PLoS Comput. Biol.
One Year of PLoS Computational Biology,"The June 2006 issue of PLoS Computational Biology marked one year of publication of the journal. While it is too early to formally assess the impact of the journal, it is worth reflecting on what has been achieved in the first year of publication. Twelve monthly issues actually reflect eighteen months of submissions, given that we have been considering papers since January of 2005. At the end of June 2006, 631 research articles have been submitted to the journal, 110 have been published, and 64 (48 new submissions and 16 revisions) are currently under review. We estimate that our acceptance rate is currently about 30%, increasing in recent months from less than 20% as authors become more familiar with the expectations of the journal and do not submit papers that have little chance of being published. We are now publishing about 15 research articles per month. Accompanying these research articles over the year have been three Editorials, six Reviews, and five Perspectives, and the Education section just published its first tutorial. Time for review averages 13.2 days and for acceptance to publication is five to six weeks, although authors have the option of having their manuscripts posted as soon as they are accepted. The journal is published in association with the International Society for Computational Biology (ISCB), an important relationship whereby one author of each accepted paper gets a free one-year membership to the Society. Articles about ISCB activities are also a regular feature in the journal. As the relationship between PLoS and ISCB matures in the coming years, we hope that this will provide a stimulus for other scholarly societies to explore and adopt open access publishing. 
 
Submissions have been received from 41 countries (based on the location of the corresponding author). The top six countries submitting articles are the US (49.1%), the UK (5.1%), Japan and India (3.3% each), Netherlands (3.0%), and Israel (2.5%). 
 
Interest in the journal can be gauged by the number of people who have signed up to receive an electronic alert of journal contents (eTOC) and by the number of downloads of journal articles. Currently 7,125 people are signed up to receive eTOCs, a number that grows by several hundred each month. Since the launch, there have been more than 250,000 article downloads, comprising 200,019 research articles, 23,408 Editorials, 14,331 Perspectives, 13,869 Reviews, and a number of hits for the Education Column and for Message from the ISCB. The top ten papers downloaded thus far are shown in Table 1. 
 
 
 
Table 1 
 
Top Ten Papers Downloaded from PLoS Computational Biology in Its First Year 
 
 
 
Two of the top ten papers are in the area of neuroscience, and two others form part of the ongoing “Ten Rules” series of Editorials to aid our less experienced readers (see Table 1). Of the remainder, one is a Perspective discussing team versus individual science and the rest are in the realm of computational molecular biology. Demographics of materials downloaded show North America and Europe account for 60%–70% of the usage of the site, but there is significant usage from the developing world, a testament to open access. 
 
Overall, we are well on the way to meeting the original editorial goal of the journal—to establish a high-quality knowledge resource serving a community interested in advancing our understanding of living systems through the use of computational techniques. While reported advances have been predominantly at the molecular level, there is a growing body of work being submitted that covers different levels of biological organization. This reflects our goal to publish great work involving computational analyses on all biological scales. We want to make connections between researchers who are using conceptually related approaches to tackle diverse issues in biology. 
 
To put this goal in perspective, consider that in one year we have explored the RNA silencing pathway in issue 1(2), designed a nanotube using naturally occurring protein building blocks in issue 2(4), modeled the transition to quorum sensing in a population of Agrobacterium in issue 1(4), and understood more about the role of mechanical factors in the morphology of the primate cerebral cortex in issue 2(3), to name but a few articles. Not bad for the first year. 
 
We aspire to have PLoS Computational Biology develop into an exemplar open access community journal which will provide a model for scholarly publishing of the future. The publication fee for the journal has recently increased from US$1,500 to US$2,000, and, along with the growth of the journal (in terms of submissions and published articles), the journal is moving steadily along a path towards financial sustainability. This is good news, and at the same time PLoS retains a fee waiver policy for those authors with insufficient funds, so money is never a factor in disseminating good science. 
 
Over the course of the past year, our monthly submissions have increased to a record 55 in June. We are all tremendously gratified to see this strong community response, and it should be remembered that this support has been offered in the absence of an impact factor (perhaps not a good indicator for an open access journal, but that is another Editorial). That we have been able to maintain an efficient editorial and publishing service in the face of this growth is testament to the tremendous efforts of our Editorial Board. My thanks go to them, and also to the PLoS staff, notably Catherine Nancarrow, Emily Stevenson, and Mark Patterson who really are the ones who keep it all on track. 
 
Join us in our second year by getting your work published in this fast-growing and diverse journal, which we hope will make those of us involved in Computational Biology proud of our collective efforts in this rapidly evolving field.",2006,PLoS Comput. Biol.
PLoS Computational Biology Conference Postcard from PSB 2011,"We were pleased to see PLoS Conference Postcards return to the Pacific Symposium on Biocomputing (PSB), held January 4–11, 2011. This year we received a Postcard from A. Murat Eren, a PhD student at the University of New Orleans, in which he discusses a software package designed for use in microbial ecology research. We hope to see Conference Postcards at ISMB 2011, and if you would like to contribute a Postcard you can find out more at: http://www.ploscompbiol.org/doi/pcbi.1000746.",2010,PLoS Comput. Biol.
Advancing code sharing in the computational biology community,"Lauren CadwalladerID *, Feilim Mac GabhannID, Jason PapinID, Virginia E. PitzerID 1 AU : Pleasenotethatasperstyle; streetaddresses; postalcodes; orpostofficeboxshouldberemovedfromtheaffiliations: PLOS, San Francisco, California, United States of America, 2 Department of Bio edical Engineering and Institute for Computational Medicine, Johns Hopkins University, Baltimore, Maryland, United States of America, 3 Department of Biomedical Engineering, University of Virginia, Charlottesville, Virginia, United States of America, 4 Department of Epidemiology of Microbial Diseases and Public Health Modeling Unit, Yale School of Public Health, New Haven, Connecticut, United States of America",2022,PLoS Comput. Biol.
Wisdom of crowds in computational biology,"Scientific advances are frequently catalyzed by exploring the intersection of disciplines. From its inception, PLOS Computational Biology has published key insights that advance our understanding of biology and medicine—advances enabled by developments in computation and quantitative analyses. As an example, a recent initiative across the journals PLOS Medicine, PLOS ONE, and PLOS Computational Biology [1] resulted in a fantastic collection of research in Machine Learning in Health and Biomedicine. The breadth of research published in PLOS Computational Biology at this intersection of machine learning and health and biology was incredible, from the use of machine learning analysis to delineate biomarkers for soft tissue sarcomas [2] to the prediction of antibiotic resistance in Escherichia coli from pan-genome data [3]. We’re only beginning to see the power of machine learning applied to health and biology, with the hope of identifying patterns in the biological and clinical data that will lead to biomarkers of disease and the development of new clinical intervention strategies. As these data-driven strategies evolve and mature, they may also lead to a richer understanding of biological mechanisms, enabling models to predict the outcomes of scenarios and perturbations beyond the bounds of previous studies and data. Many more cross-journal initiatives are in the works, exploring how disparate disciplines can be brought together to tackle seemingly intransigent problems with unique perspectives. Just launched is a Targeted Anticancer Therapies and Precision Medicine call for papers jointly with PLOS ONE and PLOS Computational Biology [4]. With nearly 10 million people dying from cancer in 2018 [5] and an increasing appreciation of the heterogeneity of the disease [6], there is an urgent need to develop targeted therapies that can be dosed, scheduled, delivered, and combined in ways that are specific to the patient. Computational approaches to this precision medicine challenge can serve as the common framework that integrates the disparate fields of expertise needed to understand the biological, pharmacological, and physiological complexity of the system. Computation can link, leverage, and amplify expertise in all the areas that are needed to understand biology and medicine: molecular dynamics, biochemistry, cell biology, human physiology, pharmacometrics, clinical practice, and more. These interdisciplinary links enable us to predict protein structure changes from gene variant data, to predict pharmacodynamics of associated drug compounds, to identify correlations in data, and simply to handle and process the vast amounts of data. As we do in these scientific ventures, so we do in managing the activity of PLOS Computational Biology. This journal, like the other PLOS community journals, is led by more than 160 practicing scientists with a variety of disciplinary backgrounds. All papers submitted to the journal are evaluated by multiple scientists through the peer review process and by teams of scientists at the editorial stage. It is with this integration of these different opinions of scientists from different subdisciplines of computational biology that we try to identify and support the",2019,PLoS Comput. Biol.
Advancements and Challenges in Computational Biology,"Computational biology has soared from being an auxiliary discipline to being a crucial element for progress in practically all aspects of the biological sciences. In this annual Editorial, I would like to step back, consider significant computational biology advances of the last decade, and reflect on some key challenges ahead. The timing is particularly appropriate. PLOS Computational Biology, the premier journal in computational biology, is approaching its tenth anniversary. The task is daunting; not only has the field come a long way in ten years but it is broad with many advances to consider. In addition, since computational biology has become closely tied to experimental research, progress is not purely computational; it is tied to experiment. And that's as it should be. Ten years ago, computational biology was not entirely trusted by experimental biologists. By contrast, today computational biology is integrated in the community. It's easier for computational biologists to collaborate across disciplines. Laboratory scientists have a better understanding of the merit of computational models for hypothesis generation as well as the need to iterate between modeling and laboratory testing [1]. 
 
We have witnessed huge leaps in biological computing [2]. We now have at our disposal large information-rich resources, and we are increasingly able to integrate and understand the vast quantities of data that they encompass. We have also made big strides toward multiscale biological modeling, and we have a vastly more networked world of researchers and their data. Analysis of massive gene expression and proteomic data permitted the construction of comprehensive and predictive models for cellular pathways, as well as software for inferring interaction networks, and steps toward modeling of cells. Genes susceptible to disease have been identified and, on a different level, the electrical behavior of neurons has been modeled. Molecules have been imaged in action and networks that regulate cell functions untangled. Matching targets for selective cancer therapy is difficult. Nonetheless, recent strategies have been proposed to restrict the combinatorial space, minimize toxicity, and increase the precision and power of such restrictive combinations, altogether leading to drugs that could be tested in clinical trials. Leveraging the enhanced identification of drug targets, including repertoires of redundant pathway combinations, has been helped by such innovative concepts [3]. 
 
Formidable challenges include: the establishment of computer networks for surveillance of disease; mapping the pathways and biological networks associated with the initiation, growth and spread of cancer; predicting function and mutational dysfunction in disease from the structure of complex molecules; resolving the mechanisms of oncogenic mutations and the cellular network which is rewired in cancer; achieving accurate, efficient, and comprehensive dynamic models; and moving from artificial intelligence to the “connectome”—the connections among all of the neurons of the brain. Multiscale biological modeling—an area where vast progress has been made during the last decade—still faces major challenges. To tackle this aim, hybrid methods across disciplines, scales, and sources are essential. Hybrid methods integrate data from, for example, serial crystallography and time-resolved wide-angle X-ray scattering, micro- and nano-crystals for (future) free-electron lasers, electron microscopy, fluorescence resonance energy transfer (FRET), cross-linking data, small-angle X-ray scattering, crystallography, nuclear magnetic resonance (NMR), and more. Equally important is the development of protocols for model validation. We may expect an influx of models based on experimental data integration. If these are to be deposited in a public archival system, which is now a community aim, such clear protocols are essential for maintaining quality control. Finally, studying the dynamics of large integrated models is increasingly used to improve our understanding of how large complexes function in the cell and how they are regulated. The dynamics of such large associations provides an additional hugely complex layer; to date, we are still struggling to comprehend the dynamics of single molecules and their associations. This is compounded by the fact that large regions of the molecules can be disordered, and multiple temporal post-translational modifications take place, with different combinations spelling distinct functions. On a different level, improved tumor mutational analysis platforms and knowledge of the redundant pathways, which can take over in cancer, may not only supplement known actionable findings but forecast possible cancer progression and resistance. Such forward-looking can be powerful, endowing the oncologist with mechanistic insight and cancer prognosis, and consequently more informed treatment options. 
 
Lastly, the community faces the global challenge of linking genetics to phenotype, including the genetics of cancer. Genetics is mediated by dynamic conformational ensembles. Powerful ideas such as that of the free energy landscape [4], imported from physics and chemistry, can help solve the mysteries of life. Biomolecules are not static sculptures; they are dynamic objects that are always interconverting between structures with varying energies. Such ideas help to understand how and why one-dimensionally connected biomolecules can organize themselves into functionally relevant ensembles of three-dimensional conformation [5]. Designing high affinity drugs that work is yet another highly significant aim. 
 
The significance of any research advance and challenge—achieved or aspired to—is a matter of opinion. The list above is partial, incomplete, and possibly biased toward structural biology and cancer. Nonetheless, this list does indicate the magnitude of the tasks confronting computational biology as a discipline. In the absence of a meaningful way to quantify a journal's contribution to a field, it is unclear whether, and to what extent, PLOS Computational Biology has contributed to each advance and challenge. Manuscripts can be declined, for example, because of the absence of substantiating experimental data at the time, lack of sufficient rigor, or if the manuscripts included new experimental data, the authors may have opted for alternative journals. At the same time, it may also suggest that PLOS Computational Biology needs to be more open and receptive to new concepts. Differentiating between novel ideas that may lead to key advances and speculative propositions can, however, be challenging. 
 
PLOS Computational Biology aims to serve the biological community and welcomes manuscripts addressing all areas of computational biology. We encourage submission of research papers describing novel results that provide significant new insights into biological processes and of methods papers presenting new protocols for tackling key problems that have been shown, or have the promise to provide, new biological insights. We aspire to be the journal that will publish key computational advances in the next decade with the rigor that PLOS Computational Biology is known for. The PLOS Computational Biology editorial team seeks to identify and publish only the most outstanding papers, aiming to consider only those that are of exceptional quality. Our goal of furthering our understanding of living systems through the application of computational methods is shared with the International Society for Computational Biology (ISCB); together, we hope to meet the challenge. 
 
Finally, for 2015, our tenth anniversary year, PLOS Computational Biology plans to publish a series of “Focus Features” addressing key areas of computational biology. We welcome suggestions from our community.",2015,PLoS Comput. Biol.
From “What Is?” to “What Isn't?” Computational Biology,"This year, PLOS Computational Biology is celebrating its 10th birthday. Such a milestone provides an excellent occasion to reflect on the transformation that the field has undergone during the journal’s lifetime, and on the challenging question of where we may expect it to go next. As the leading journal in computational biology, PLOS Computational Biology encompasses the entire discipline and is therefore well placed to narrate this remarkable story. The evolution of the journal tells a rewarding story of success and accomplishment.",2015,PLoS Comput. Biol.
Education in Computational Biology Today and Tomorrow,"The etymology of the word “education” in Wikipedia is enlightening: “a rearing” and “I lead forth” (http://en.wikipedia.org/wiki/Education#Etymology). Computational biology educators are leading and raising the next generation of scientists and, in doing so, are in need of new tools, methods, and approaches. The need for education in science, and in computational biology in particular, is greater than ever. Large datasets, -omics technologies, and overlapping domains permeate many of the big research questions of our day. PLOS Computational Biology originally created the Education section to highlight the importance of education in the field [1]. Thus, it was a great honor when Fran Lewitter, Education Editor for the past eight years, along with Philip E. Bourne and Ruth Nussinov, contacted us to work as editors of the PLOS Computational Biology Education section. In our minds, educational initiatives in computational biology and bioinformatics serve two important goals: to communicate digital biology with each other, and to educate others on how best to do this. These are themes we practice as educators in our university teaching, in our involvement with the bioinfomatics.ca workshops series, and in our outreach efforts. We are very excited to continue Fran's great vision as we continue her work with the PLOS Computational Biology staff. 
 
Examples of tutorials, specialized workshops, and outreach programs that bridge the knowledge gap created by this fast pace of research have been previously highlighted in this collection. There have been several types of articles, but two stand out. Firstly, there are tutorials about a specific biological problem requiring a specific approach, tools, and databases. For example, ”Practical Strategies for Discovering Regulatory DNA Sequence Motifs„ by MacIsaak and Fraenkel [2]. Tutorial articles provide theoretical context, as well as the type of questions and how to answer them. The other type of article we frequently find in the Education collection are “primers” or “quick guides.” For example, Eglen's “A Quick Guide to Teaching R Programming to Computational Biology Students” [3] or Bassi's “A Primer on Python for Life Science Researchers” [4]. Both of these examples from the Education collection address an important niche within the community. The “Quick Guide” series provides a more generic introduction to an approach in computational biology that can be applied across multiple domains. All of these types of articles will continue to be well-supported and encouraged in the Education collection. Many other articles have also been well-received, and seem to address gaps in the education material. We want to revisit older collection papers and identify where methods and technologies have evolved to a point where new methods are now in use, and invite previous or new authors to contribute. 
 
These initiatives help to extend computational biology beyond the domain of specialized laboratories. Researchers, at all levels, need to keep themselves up-to-date with the quickly changing world of computational biology, and trainees need programs where bioinformatics skills are embedded so they can have comprehensive training. New bioinformatics workflows can be adopted more widely if education efforts keep pace. As previously pointed out [5], starting early is also very important. There is still room for programs that capture the excitement and enthusiasm of secondary school students and convey the potential of computational biology to the public. We welcome additions to the PLOS Computational Biology “Bioinformatics: Starting Early” collection (www.ploscollections.org/cbstartingearly). 
 
We would like to involve the community in this endeavor. With this editorial, we are calling out to educators and researchers who have experience in teaching, specifically, those keen to raise the expectations and the inquisitiveness of the next generation of biologists. The Education collection will continue to publish leading edge education materials in the form of tutorials that can be used in a “classroom” setting (whatever that may mean nowadays: stated more generically, “the places where people learn”). We will continue to encourage articles set in the context of addressing a particular biological question and, as mentioned above, we welcome new “primers” and “quick guides.” We will also be inviting tutorials from the various computational meetings. A new category of papers that is in the pipeline for the Education collection is the “Quick Tips” format, the first of which was just published [6]. The “Quick Tips” articles address specific tools or databases that are in wide use in the community. 
 
We also hope, and plan, to incorporate new thinking and perspectives in the greater field of education of computational biology and bioinformatics. For example, articles that highlight the use of new tools such as those used in cloud computing or methods for using third and fourth generation sequencing technologies are encouraged. We would also like to see articles that incorporate best practices in teaching, including the use of new media, flexible online teaching tools, and the use and re-use of large well-defined data sets that are computed on in classes, courses, and programs. We encourage articles that highlight new types of training initiatives, the use of workflows to help students in the path to reproducibility in science, and open course materials (open lecture notes and open course notes and datasets for exercises) that reach more learners. 
 
In the end, the Education section belongs to the community and thus comes with responsibilities. We need to identify the gaps and the material with which we want to educate ourselves; we need to recognize and encourage great teachers and writers to communicate openly about what works best with the specific methods. We invite you to contact us via gro.solp@loibpmocsolp with your ideas for the kind of articles you would like to see in the PLOS Computational Biology Education section. We hope to see you in the classroom soon, where we learn together. 
 
 
About The Authors 
Joanne A. Fox (xofnosilaennaoj@ on Twitter) has a PhD in Genetics from the University of British Columbia (UBC). As a faculty member at the Michael Smith Laboratories and in the Department of Microbiology and Immunology at UBC, she is involved in a range of education and outreach initiatives at the undergraduate and secondary school levels, and teaches a variety of courses. She is a former instructor and current review committee member of the Canadian Bioinformatics.ca Workshops. 
 
B.F. Francis Ouellette (offb@ on Twitter) did his graduate studies in Developmental Biology and is now an Associate Professor in Cell and Systems Biology at the University of Toronto, as well as a senior scientist and Associate Director of Informatics and Biocomputing at the Ontario Institute for Cancer Research. He was one of the founders and is still the scientific director and an instructor for the Canadian Bioinformatics.ca Workshops. 
 
The authors have worked together in the past, and have known each other for more than 15 years.",2013,PLoS Comput. Biol.
Bioinformatics and Computational Biology in Poland,"The series of articles in PLOS Computational Biology on the development of bioinformatics activities in various countries, e.g., China [1], Australia [2], and Singapore [3], and the formation and successful development of the Polish Bioinformatics Society over the last five years, have inspired us to present a personal perspective on the advances of bioinformatics in Poland.",2013,PLoS Comput. Biol.
Webb Miller and Trey Ideker To Receive Top International Bioinformatics Awards for 2009 from the International Society for Computational Biology,"Each year, the International Society for Computational Biology (ISCB; http://www.iscb.org) makes two major awards to recognize excellence in the field of bioinformatics. The ISCB Awards Committee, composed of current and past directors of the Society and previous award winners, has announced that the 2009 ISCB Accomplishment by a Senior Scientist Award will be given to Webb Miller (Image 1) of The Pennsylvania State University, University Park, Pennsylvania, United States of America (USA), and the 2009 ISCB Overton Prize for outstanding achievement in early- to mid-career will be awarded to Trey Ideker (Image 2) of the University of California San Diego, La Jolla, California, USA, who serves on the Editorial Advisory Board for PLoS Computational Biology. These awards represent the highest tribute and recognition of scientific excellence within the bioinformatics community, and they are seen as honors well beyond the boundaries of the discipline. Both 2009 award winners started their careers in basic computer science, but since moving into bioinformatics they have made important contributions to biological understanding as well as to algorithm design. 
 
 
 
Image 1 
 
Webb Miller. 
 
 
 
 
 
Trey Ideker. 
 
 
 
Both awards will be presented at the Society's flagship international conference, Intelligent Systems for Molecular Biology (ISMB), where the winners will give keynote presentations. In 2009, the (seventeenth) annual ISMB will be held in conjunction with the European Conference on Computational Biology (ECCB) for the third time; ISMB/ECCB 2009 will take place in Stockholm, Sweden, from June 27 through July 2.",2009,PLoS Comput. Biol.
Computational Biology in Brazil,"At the request of the PLoS Computational Biology Editor-in-Chief, I agreed to write about computational biology in Brazil (see author information in Box 1). That meant describing: a) the history of the field in our country (short as the history of the field itself is short); b) the current state of the field in Brazil; c) the influence of computational biology–related technologies on the development of the national economy; d) the entrepreneurship that is rising from already-established academic activities; and e) educational activities ongoing or planned which are deemed necessary to establish the required critical mass of well-trained specialists. Why is an article like this important now? It is estimated that Brazil combined with China, Russia, and India will have a larger gross national product (GNP) than the US, Japan, Germany, and the UK combined by 2020. In short, we can expect Brazil to have significant impact on the field of computational biology in the years to come, and now is the time to explore that promise. 
 
 
Box 1. Author Biography 
 
Goran Neshich, Ph.D., is the Structural Computational Biology (SCB) group leader at the Brazilian Agricultural Research Corporation (EMBRAPA), National Agricultural Information Technology Research Center (CNPTIA), Campinas, Sao Paulo, Brazil, and associate professor at UNICAMP's Department of Biology. After obtaining his Ph.D. in molecular biophysics from the University of Illinois Urbana-Champaign (Don DeVault's laboratory) in 1989, Neshich conducted his postdoctoral research with Barry Honig at Columbia University. Neshich is the principal author of the STING suite of programs (with the current version being BlueStarSTING), and STING_DB. STING is a popular database and visualization tool providing the largest collection of physicochemical parameters that describe protein structure, stability, function, and interaction with other macromolecules. Neshich chaired a session at the meeting held in November 2004, where the Brazilian Association for Computational Biology and Bioinformatics (AB3C) was inaugurated. He was a member of the Board of Directors of the International Society for Computational Biology (ISCB) from 2003 to 2005 and chair of the Intelligent Systems in Molecular Biology (ISMB) 2006 conference in Fortaleza, Brazil.",2007,PLoS Comput. Biol.
An Open Forum for Computational Biology,"I t is my pleasure to welcome you to PLoS Computational Biology. The International Society for Computational Biology (ISCB) is proud to be, with the Public Library of Science, the co-sponsor of this journal. We believe that PLoS Computational Biology will rapidly become a leading journal in the area of computational biology and, as an official journal of ISCB, will be an important venue in which ISCB members will publish their findings and learn of the work of others, both ISCB members and nonmembers. New journals appear at frequent intervals these days, and one may well ask: why a new computational biology journal? PLoS and ISCB feel that there is both need and interest in a journal with a new focus and new approach to computational biology. This feeling grows out of our recognition of the enormous changes that computation has wrought in both science and communication. In contrast to the situation of only a decade or so ago, almost every area of biology is now affected and enhanced by computational studies. But until the appearance of PLoS Computational Biology, there has been no single publication with a focus on the important contributions of computational studies to the understanding of living systems. PLoS Computational Biology meets this need and provides a forum in which experimentalists and computational scientists can meet, exchange ideas, and develop new solutions to biological questions. The revolution in communication that has grown out of the Internet and World Wide Web infrastructure provides the other motivating force behind the creation of a new journal. Free availability of protein and nucleic acid sequences, protein structures, and other biological data is critical to practitioners of computational biology— support of open-access journals is",2005,PLoS Comput. Biol.
The International Society for Computational Biology 10th Anniversary,"PLoS Computational Biology is the official journal of the International Society for Computational Biology (ISCB), a partnership that was formed during the Journal's conception in 2005. With ISCB being the only international body representing computational biologists, it made perfect sense for PLoS Computational Biology to be closely affiliated. The Society had to take more of a chance than similar societies, choosing to step away from an existing financially beneficial subscription journal to align with an open access publication as a matter of principle. To our knowledge, ISCB was the first major international scientific society to do so. 
 
Now, as PLoS Computational Biology reaches its two-year mark, ISCB simultaneously celebrates its tenth anniversary, having formed officially on June 18, 1997. We early presidents of ISCB reflect on the state of computational biology ten years ago, how far we have come since, and what thought-provoking future challenges might lie ahead with regard to innovations in publishing technologies.",2007,PLoS Comput. Biol.
"Comment on “Broadband Criticality of Human Brain Network Synchronization” by Kitzbichler MG, Smith ML, Christensen SR, Bullmore E (2009) PLoS Comput Biol 5: e1000314","I wish to comment on a paper recently published in the Journal of Neuroscience [1] and relate this paper to one previously published in PLOS Computational Biology [2]. In [1] and [2] among other results, a power law relationship was discovered in a measure of magnetoencephalography (MEG) intra-areal synchronization: the distribution of phase-locking intervals (PLI). However, in [1] the authors also show that the same PLI power law measure cannot distinguish between human MEG and empty MEG scanner data, suggesting that the measure is vulnerable to artefact. 
 
This is important because the first description of the PLI power law methodology, as well as its application to MEG and functional magnetic resonance imaging (fMRI) data, was published in PLOS Computational Biology [2]. The senior author of [2] is also an author of [1]. The results obtained with the PLI methodology published in [2] were presented as evidence for broadband criticality of human brain network synchronization because the PLI method (which applies a threshold to MEG/fMRI data and returns a power law) will also return a power law distribution for PLIs when applied to a model system of Kuramoto oscillators tuned to a critical phase transition. However, it should be noted in this regard that a recent modelling study indicates that power laws also emerge when PLI is applied to noncritical Kuramoto oscillators, and caution is needed when interpreting power laws derived from time series data passed through a threshold [3]. 
 
The paper published in PLOS Computational Biology [2] has received multiple citations, and the PLI methodology has been further applied to human neurophysiological data in other studies. From its use, claims have been made about changes in broadband criticality during disease states, e.g., epilepsy [4]. 
 
I am concerned that the results presented in [1] indicate that the methodology described in [2] may be unsound and therefore should not be used for inferring criticality of human brain network synchronisation. The presentation of the empty MEG scanner results in [1] is not sufficiently explicit in this sense, and it should have been made much clearer that the influential PLI power law methodology presented in [2] might be problematic. 
 
Science of course moves forward through a process of exploration and correction, and the initial idea presented in PLOS Computational Biology [2] was exciting and innovative; however, the authors should reconsider the interpretation of their findings in light of the empty MEG scanner data. I feel it is therefore important to bring these papers and their conflicting results to the attention of the readers of PLOS Computational Biology and I would ask that the authors of [1,2] clarify the discrepancy in between their data sets and publish an erratum in PLOS Computational Biology if a conflict exists.",2015,PLoS Comput. Biol.
PEtab—Interoperable specification of parameter estimation problems in systems biology,"Reproducibility and reusability of the results of data-based modeling studies are essential. Yet, there has been—so far—no broadly supported format for the specification of parameter estimation problems in systems biology. Here, we introduce PEtab, a format which facilitates the specification of parameter estimation problems using Systems Biology Markup Language (SBML) models and a set of tab-separated value files describing the observation model and experimental data as well as parameters to be estimated. We already implemented PEtab support into eight well-established model simulation and parameter estimation toolboxes with hundreds of users in total. We provide a Python library for validation and modification of a PEtab problem and currently 20 example parameter estimation problems based on recent studies.",2020,PLoS Comput. Biol.
Ten Simple Rules for a Community Computational Challenge,"In science, the relationship between methods and discovery is symbiotic. As we discover more, we are able to construct more precise and sensitive tools and methods that enable further discovery. With better lens crafting came microscopes, and with them the discovery of living cells. In the last 40 years, advances in molecular biology, statistics, and computer science have ushered in the field of bioinformatics and the genomic era. 
 
Computational scientists enjoy developing new methods, and the community encourages them to do so. Indeed, the editorial guidelines for PLOS Computational Biology require manuscripts to apply novel methods. However, it is often confusing to know which method to choose: which method is best? And, in this context, what does “best” mean? 
 
To help choose an appropriate method for a particular task, scientists often form community-based challenges for the unbiased evaluation of methods in a given field. These challenges help evaluate existing and novel methods, while helping to coalesce a community and leading to new ideas and collaborations. 
 
In computational biology, the first of these challenges was arguably the Critical Assessment of protein Structure Prediction, or CASP [1], whose goal is to evaluate methods for predicting three-dimensional protein structure from amino acid sequence. The first CASP meeting was held in December of 1994, following a “prediction period” where members of the community were presented with protein amino acid sequences and asked to predict their three dimensional structures. The sequences that were chosen had recently been solved by X-ray crystallography but had not been not published or released until after the predictions from the community were made. Since the first CASP, we have seen many successful challenges, including Critical Assessment of Function Annotation (CAFA) for protein function prediction [2], Critical Assessment of Genome Interpretation (CAGI) (for genome interpretation) [3], Critical Assessment of Massive (originally “Microarray”) Data Analysis (CAMDA) (for large-scale biological data) [4], BioCreative (for biomedical text mining) [5], the Assemblathon (for sequence assembly), and the NCI-DREAM Challenges (for various biomedical challenges), amongst others [6]. 
 
Computational challenges also help solve new problems. While the original CASP experiment was developed to evaluate existing methods applied to current problems, other communities often look at other areas for which there are no existing tools. These challenges have spread successfully to industry, and companies such as Innocentive [7] and X-Prize [8] offer large prizes for solving novel questions. 
 
Because these challenges are, on one hand, an exercise in community collaboration, and on the other, a competition, organizing a challenge is littered with difficulties and pitfalls. Having served as organizers, predictors, and assessors within several existing communities, we present ten rules we believe should be observed when organizing a computational methods challenge:",2015,PLoS Comput. Biol.
For long-term sustainable software in bioinformatics,This is a PLOS Computational Biology Benchmarking paper.,2024,PLoS Comput. Biol.
Ten simple rules for scientists engaging in science communication,"This is a PLOS Computational Biology Methods paper. There are clear moral and professional imperatives for scientists to participate in communicating their research to the public [1–3]. Many scientists want to share their findings and excitement of science or encourage interest and appreciation of their work. They may want to improve the public’s understanding of their field or dispel misinformation. They may also be curious about how their work is viewed by society and benefit from interacting with new ideas and perspectives. Increasingly, science communication is a formal responsibility for researchers as funding bodies and institutions encourage or even require scientists to engage in science communication or outreach work (Broader Impacts for National Science Foundation grants, NSERC’s PromoScience program, etc.). Despite there being many excellent reasons for scientists to engage in science communication, they often lack the tools to do so. Explicit training in nonexpert communication remains uncommon in graduate programs (though this is improving; [4]), as programs typically prioritize the development of scientist-to-scientist communication skills such as conference presentations and peer-reviewed articles. Scientists frequently cite lack of training and/or confidence in their science communication skills as a barrier to their participation in public-facing activities [4–8]. At the same time, there are ample opportunities for scientists to engage in science communication as a part of their official duties, as a secondary career (freelancing), or as a volunteer. As experts in their field, scientists can contribute through interviews in the media, writing op-eds or books for the popular press (see [9] for specific guidance on writing popular science books), public seminars or interactive events, social media posts (see [10] for specific suggestions on how to get started on Twitter as scientist), and other modalities. I am a postdoctoral fellow and freelance science writer and editor. Here, I describe 10 simple rules for planning, developing, and evaluating science communication activities. Though I focus on scientists communicating with nonscientists, much of the advice applies to other forms of science communication such as expert-to-expert communication (e.g., talks and posters at conferences). As my goal is to guide inexperienced scientist-science communicators through the practical basics of getting started with science communication, the rules are ordered to encourage a step-by-step process (but note the integrative nature of science communication activities, Rule 10).",2023,PLoS Comput. Biol.
Computational and Statistical Analysis of Protein Mass Spectrometry Data,"High-throughput proteomics experiments involving tandem mass spectrometry produce large volumes of complex data that require sophisticated computational analyses. As such, the field offers many challenges for computational biologists. In this article, we briefly introduce some of the core computational and statistical problems in the field and then describe a variety of outstanding problems that readers of PLoS Computational Biology might be able to help solve.",2012,PLoS Comput. Biol.
Ten quick tips for editing Wikidata,This is a PLOS Computational Biology Software paper,2023,PLoS Comput. Biol.
Ten simple rules and a template for creating workflows-as-applications,This is a PLOS Computational Biology Software paper,2022,PLoS Comput. Biol.
Ten simple rules for working with other people’s code,This is a PLOS Computational Biology Methods paper.,2023,PLoS Comput. Biol.
Ten simple rules for serving as an editor,This is a PLOS Computational Biology Methods paper,2023,PLoS Comput. Biol.
Ten simple rules for quick and dirty scientific programming,This is a PLOS Computational Biology Software paper,2021,PLoS Comput. Biol.
Ten simple rules to cultivate belonging in collaborative data science research teams,This is a PLOS Computational Biology Methods paper.,2022,PLoS Comput. Biol.
Correction: Mathematical modeling of the molecular switch of TNFR1-mediated signaling pathways applying Petri net formalism and in silico knockout analysis,[This corrects the article DOI: 10.1371/journal.pcbi.1010383.].,2023,PLoS Comput. Biol.
Directing Experimental Biology: A Case Study in Mitochondrial Biogenesis,"Computational approaches have promised to organize collections of functional genomics data into testable predictions of gene and protein involvement in biological processes and pathways. However, few such predictions have been experimentally validated on a large scale, leaving many bioinformatic methods unproven and underutilized in the biology community. Further, it remains unclear what biological concerns should be taken into account when using computational methods to drive real-world experimental efforts. To investigate these concerns and to establish the utility of computational predictions of gene function, we experimentally tested hundreds of predictions generated from an ensemble of three complementary methods for the process of mitochondrial organization and biogenesis in Saccharomyces cerevisiae. The biological data with respect to the mitochondria are presented in a companion manuscript published in PLoS Genetics (doi:10.1371/journal.pgen.1000407). Here we analyze and explore the results of this study that are broadly applicable for computationalists applying gene function prediction techniques, including a new experimental comparison with 48 genes representing the genomic background. Our study leads to several conclusions that are important to consider when driving laboratory investigations using computational prediction approaches. While most genes in yeast are already known to participate in at least one biological process, we confirm that genes with known functions can still be strong candidates for annotation of additional gene functions. We find that different analysis techniques and different underlying data can both greatly affect the types of functional predictions produced by computational methods. This diversity allows an ensemble of techniques to substantially broaden the biological scope and breadth of predictions. We also find that performing prediction and validation steps iteratively allows us to more completely characterize a biological area of interest. While this study focused on a specific functional area in yeast, many of these observations may be useful in the contexts of other processes and organisms.",2009,PLoS Comput. Biol.
Correction: Opponent learning with different representations in the cortico-basal ganglia pathways can develop obsession-compulsion cycle,[This corrects the article DOI: 10.1371/journal.pcbi.1011206.].,2023,PLoS Comput. Biol.
Correction: A quantitative modelling approach for DNA repair on a population scale,[This corrects the article DOI: 10.1371/journal.pcbi.1010488.].,2023,PLoS Comput. Biol.
"Correction: Hybrid predictive coding: Inferring, fast and slow",[This corrects the article DOI: 10.1371/journal.pcbi.1011280.].,2023,PLoS Comput. Biol.
Correction: cytoNet: Spatiotemporal network analysis of cell communities,[This corrects the article DOI: 10.1371/journal.pcbi.1009846.].,2022,PLoS Comput. Biol.
On the choice of metric in gradient-based theories of brain function,"This is a PLOS Computational Biology Education paper. The idea that the brain functions so as to minimize certain costs pervades theoretical neuroscience. Because a cost function by itself does not predict how the brain finds its minima, additional assumptions about the optimization method need to be made to predict the dynamics of physiological quantities. In this context, steepest descent (also called gradient descent) is often suggested as an algorithmic principle of optimization potentially implemented by the brain. In practice, researchers often consider the vector of partial derivatives as the gradient. However, the definition of the gradient and the notion of a steepest direction depend on the choice of a metric. Because the choice of the metric involves a large number of degrees of freedom, the predictive power of models that are based on gradient descent must be called into question, unless there are strong constraints on the choice of the metric. Here, we provide a didactic review of the mathematics of gradient descent, illustrate common pitfalls of using gradient descent as a principle of brain function with examples from the literature, and propose ways forward to constrain the metric.",2018,PLoS Comput. Biol.
"Comment on ""A comprehensive overview and evaluation of circular RNA detection tools""","A recent paper published in PLOS Computational Biology [1] provided a comprehensive evaluation of various circular RNA (circRNA)-detection tools. The authors compared 11 different circRNA-detection tools using four different datasets, including three simulated datasets (positive, background, and mixed datasets) and one real dataset. Since the advent of highthroughput next-generation sequencing technology, dozens of computational tools have been developed and used to successfully detect thousands of circRNAs in a diverse range of species. However, there are great discrepancies in the results obtained using different tools [2–7], and systematic evaluations of their performance have not been available. Indeed, the cited work has provided a useful guideline for researchers engaged in circRNA studies. Nevertheless, it seems inappropriate to use all CircBase-deposited circRNA candidates (14,689 events) identified in silico from RNA-seq data of HeLa cells [8] as the positive dataset. The qualification of the 14,689 candidates requires further evaluation. We suggest that three main confounding factors, which may affect the fairness of the evaluation of circRNA-detection tools, should be considered. First, it has been shown that non-co-linear (NCL) junctions (including circRNA and transspliced RNA junctions) that do not match annotated exon boundaries tend to be unreliable and are more likely to stem from mis-splicing [9–12], although we cannot eliminate the possibility that a few true backspliced junctions indeed originate from unannotated gene loci. Since circRNA candidates are regarded to be less or more reliable if their normalized read counts are depleted or enriched after RNase R treatment, respectively [13], we reexamined the circRNA candidates detected on the HeLa RNase R-treated and untreated samples (the circRNA candidates and the corresponding read counts were downloaded from the cited study). Of the circRNA candidates with unannotated exon boundaries, we can find that 50%~100% of them were “completely” depleted (not detected) after RNase R treatment, whereas only <8% of them were “significantly” enriched (i.e., 5-fold increase in normalized read count) after RNase R treatment (Fig 1). This result revealed that the candidates with unannotated exon boundaries are more likely to be false calls. Thus, we suggest that the CircBase circRNA candidates with unannotated exon boundaries (1,046 events; Table 1) should be excluded from the positive dataset. At least, since circRNA junctions were observed to be predominantly located at canonical splice sites [14–16], the candidates with junctions that have not canonical splice site sequences (GT-AG, GC-AG, or AT-AC) should be removed (778 events; Table 1). Second, ambiguous alignments originating from repetitive sequences or paralogous genes often result in false positive circRNA detection. In CircBase, most circRNA candidates were identified by find_circ [8]. It has been reported that some of find_circ-identified candidates were mis-predicted from paralogous genes [17]. Therefore, the factor of alignment ambiguity should be considered when using CircBase circRNAs as true positives. To this end, we concatenated the exonic sequence flanking the circRNA junction (within -100 nucleotides",2019,PLoS Comput. Biol.
Ten simple rules for researchers while in isolation from a pandemic,Part of the PLOS Computational Biology Ten Simple Rules Series. The paper is currently under review.,2020,PLoS Comput. Biol.
Strategies and opportunities for promoting bioinformatics in Zimbabwe,"CITATION: Shoko, R., et al. 2018. Strategies and opportunities for promoting bioinformatics in Zimbabwe. PLoS Computational Biology, 14(11):e1006480, doi:10.1371/journal.pcbi.1006480.",2018,PLoS Comput. Biol.
Ten Simple Rules for Writing Research Papers,"The importance of writing well can never be overstated for a successful professional career, and the ability to write solid papers is an essential trait of a productive researcher. Writing and publishing a paper has its own life cycle; properly following a course of action and avoiding missteps can be vital to the overall success not only of a paper but of the underlying research as well. Here, we offer ten simple rules for writing and publishing research papers. 
 
As a caveat, this essay is not about the mechanics of composing a paper, much of which has been covered elsewhere, e.g., [1], [2]. Rather, it is about the principles and attitude that can help guide the process of writing in particular and research in general. In this regard, some of the discussion will complement, extend, and refine some advice given in early articles of this Ten Simple Rules series of PLOS Computational Biology [3]–[8].",2014,PLoS Comput. Biol.
Human Dominant Disease Genes Are Enriched in Paralogs Originating from Whole Genome Duplication,"PLOS Computational Biology recently published an article by Chen, Zhao, van Noort, and Bork [1] reporting that, in contrast to duplicated nondisease genes, human monogenic disease (MD) genes are (1) enriched in duplicates (in agreement with earlier reports [2]–[5]) and (2) more functionally similar to their closest paralogs based on sequence conservation and expression profile similarity. Chen et al. then proposed that human MD genes frequently have functionally redundant paralogs that can mask the phenotypic effects of deleterious mutations.",2014,PLoS Comput. Biol.
Correction: New Maximum Likelihood Estimators for Eukaryotic Intron Evolution,"Correction: Structure Modeling of All Identified G Protein–Coupled Receptors in the Human Genome Yang Zhang, Mark E. DeVries, Jeffrey Skolnick DOI: 10.1371/journal.pcbi.0020013 In PLoS Computational Biology, volume 2, issue 2: The URL provided for the GPCR model database in the published article is no longer active. The database is now located at http://cssb.biology.gatech.edu/skolnick/files/gpcr/gpcr.html.",2005,PLoS Comput. Biol.
Getting Started in Text Mining: Part Two,"We are, in a sense, drowning in information. Today, it is unusual for scientists even to read a journal cover to cover—much less to personally parse all information pertinent to even a narrow research area. Increasingly complex content, large digital supplements, and a staggering volume of publications are now threatening old-fashioned scientific reading with extinction. But by using computers to sift through and scour published articles, the nascent technology of text mining promises to automate the rote information-gathering stage—hopefully leaving to human minds the more challenging (and rewarding) activity of higher thinking. 
 
This article is intended to continue where Cohen and Hunter [1] left off in “Getting Started in Text Mining,” an introduction in the January 2008 issue of PLoS Computational Biology which covered the actual mining of text and its digestion into small quanta of computer-manageable information (http://www.ploscompbiol.org/doi/pcbi.0040020). In this overview of the field, we begin by summarizing the major stages of current text-processing pipelines. We now focus on the downstream questions scientists can ask using text-mining and literature-mining engines. At times, we (deliberately) blur the boundary between today's approaches and tomorrow's possibilities. 
 
Figure 1 shows a high-level overview of the stages in text mining, with a focus on its applications. We begin at the top left of the figure, which shows the process of information retrieval—how we select relevant documents [2]. Unfortunately, free full-text access remains impossible for a large portion of scientific journals. In some fields, such as chemistry, even article abstracts are inaccessible for a large-scale analysis. The obvious outcome is that articles published in open-access journals have a better chance of being identified as relevant hits than others appearing in traditional “closed-access” journals. Electronic access to text obviously impacts all stages of text mining. 
 
 
 
Figure 1 
 
Major techniques and applications of text mining. 
 
 
 
Once the documents have been chosen by an information retrieval engine, a computer scans the text and picks out the various entities (objects, concepts, and symbols) in each sentence. This process, called named-entity recognition [3], draws upon dictionaries of synonyms and homonyms, in addition to machine-learning tools [4], so that an individual entity (say, a protein) is recognized consistently—even though it may be referred to by several different names and acronyms [5]. Named-entity recognition is closely related to the design of controlled terminologies [6] and ontologies for the annotation of texts and experimental data [7]—a process often requiring a monumental community effort [8]. 
 
The next step is information extraction (IE) (see pp. 545–559 in [9]). Here, entities are assembled into simple phrases and clauses that capture the meaning of the mined text. To accomplish this, two or more entities are juxtaposed, and meaningful action words—called predicates—are chosen to link the entities. For instance, we might say gene X genetically interacts with gene Y, or protein A binds to protein B. Each completed clause describes a basic relationship between entities. The question then becomes, what can we do with all these simple or complex clauses? 
 
The answer is, quite a lot—which helps explain why text mining is poised to become a powerful central pillar in scientific research and recordkeeping. The lower two-thirds of Figure 1 illustrates how the results of information extraction (IE) can be synthesized and used. 
 
Because IE yields a collection of phrases linking entities through predicates, one of its simplest but valuable uses is to answer simple questions posed to an automated system [10]. In this approach, human questions are digested by a linguistic engine (likely using the same process as employed on original mined text) and mapped to simple phrases. These question phrases are then queried against the database of phrases already stored in the computer, which were generated through the application of IE to analyzed text. (Another mode of question answering, bypassing generation and querying of a database entirely, involves direct search and analysis of relevant texts. These texts can be stored at a local computer disk or distributed on numerous computers around the world.) Figure 1 outlines the basic process by which the machine interprets the question, queries its database of stored relationships, and returns an answer. 
 
IE-generated knowledge often tracks closely the needs of experimental biologists. Typical IE systems are developed in direct response to acute practical problems, such as large-scale annotation of regulatory regions in genomes [11], collecting published claims about experimental evidence supporting a collection of assertions [12], and condensing sparse information about phenotypic effects of mutations in proteins [13]. 
 
Of course, IE-generated databases can be supplemented with additional data gleaned from experiment, or contributed through other non–text-mining means. A simple user interface could facilitate contributing raw experimental data or other information into the database of relationships expressed as simple phrases—again, entities linked by actions (see, for example, the REFLECT system, http://reflect.ws/). Adding more such data should correspondingly increase the effectiveness of the computer's answers to user questions. 
 
Another major use for the database of IE-generated phrases is to employ the collection itself for the discovery of new information [14],[15]. One approach to this is to seek out “idea isomorphisms”, by which we mean identifying similar types of logical constructs across different contexts. Finding that similar small ideas (or phrases) occur in different fields might allow researchers to bridge different areas of inquiry. Such bridging of fields, in turn, might uncover new connections, thereby suggesting new and unexpected hypotheses that can then be tested experimentally. 
 
The collection of phrases can also be used to vet and prune itself by examining the consistency among many entries. For instance, conflicting or erroneous data can be flagged. By examining each record situated within a large number of records, the preponderance of evidence could assist in identifying and resolving errors. Say, for example, that 20 distinct phrases all indicate that protein A interacts with protein B, and one phrase suggests otherwise; we might probabilistically argue, then, that the lone conflicting statement is false and should be disregarded—unless it is supported some other way. 
 
An additional approach to using these phrases—in a mega-scale fashion—is to construct a “map of science”, a global description of the interrelationships between different fields of inquiry. This is similar conceptually to PubNet [16], which highlights connections between authors. However, the map of science would be generated not through coauthor relationships but through clustering the underlying scientific fact claims themselves, as represented in the IE phrase collection. To do this, researchers would cluster papers according to their IE-derived phrase content; any two papers can be compared in this way to derive a measure of their similarity and overlap in terms of information content. By repeating this process, researchers could create a distance map of all papers in science, and, along the way, of all the factoids that the information content of the papers themselves comprise. 
 
In addition, researchers might track the changing nature of the IE phrases over time to examine the dynamics of scientific belief. This could involve observing as simple phrases themselves change in occurrence or content over time, or we might watch these simple ideas and truth claims crop up in the scientific literature and track their development that way. 
 
Finally, the middle right-hand section of Figure 1 depicts a very simple type of analysis involving the IE-generated simple phrase collection. This approach involves simply looking at the phrases' occurrence in the databases, and recording which statements tend to occur more than others. This type of analysis normally generates a kind of power law–type structure, where it becomes apparent that a few phrases occur many times, but most others only occur a few times. 
 
Text/literature mining is a powerful approach, one we expect to substantially bolster the scientific reporting and discovery process in coming years. Applying the organizational, storage, and pattern-matching capabilities of modern computers to the vast corpus of scientific information contained in the literature (present, past, and future) will not only transform the vast archives of science into rapid-access searchable computerized data, but no doubt also catalyze the discovery of much new knowledge. We hope that this brief “getting started” report highlights some of the major and promising avenues opening as a result of advances in text mining. 
 
Note to the reader: The field of text mining is young and growing rapidly, and our own interests and experiences have in large part shaped our perspective on it. We are constrained by length limits here to (reluctantly) omit several topics, such as text mining in conjunction with image analysis, important community text-annotation efforts, and ontology engineering—each important in its own right. Furthermore, every issue touched upon in this essay comes with a rich diversity of views and approaches in the text-mining community. While we cannot possibly do justice to this complexity, the reader should reject the impression that there is but a single correct way to perform text analysis.",2009,PLoS Comput. Biol.
Ten Simple Rules for Teaching Bioinformatics at the High School Level,"Fran Lewitter is Education Editor of PLoS Computational Biology. 
 
Given the availability of free, online genomic databases and tools for the analysis of biological data, it is now feasible to teach bioinformatics in the high school classroom [1]. There are a number of reasons why it is appropriate and desirable to introduce bioinformatics at the high school level. Students can engage in inquiry-based activities that involve approaching real-world problems using 21st century skills, while being tailored to high school biology frameworks. Many tools, such as 3-D protein visualization software, allow for differentiated and highly interactive instruction. The foremost reason may be that students can develop a research toolkit that they will be able to use subsequently during college and beyond. 
 
As a high school science teacher for the past 23 years, I (DF) have had the opportunity to incorporate bioinformatics into my courses to enrich the teaching of concepts of molecular biology, human biology, genetics, and evolution, providing increased opportunities for effective differentiated instruction and individual student research. This past experience has inspired the creation of this set of Ten Simple Rules. 
 
It is important to distinguish between curricula designed to teach the fundamentals of bioinformatics and those that utilize bioinformatics as a teaching tool. Examples of both types of successful teaching can be found in Text S1, Text S2, and Text S3.",2011,PLoS Comput. Biol.
The Roots of Bioinformatics,"Every new scientific discipline or methodology reaches a point in its maturation where it is fruitful for it to turn its gaze inward, as well as backward. Such introspection helps to clarify the essential structure of a field of study, facilitating communication, pedagogy, standardization, and the like, while retrospection aids this process by accounting for its beginnings and underpinnings. 
 
In this spirit, PLoS Computational Biology is launching a new series of themed articles tracing the roots of bioinformatics. Essays from prominent workers in the field will relate how selected scientific, technological, economic, and even cultural threads came to influence the development of the field we know today. These are not intended to be review articles, nor personal reminiscences, but rather narratives from individual perspectives about the origins and foundations of bioinformatics, and are expected to provide both historical and technical insights. Ideally, these articles will offer an archival record of the field's development, as well as a human face on an important segment of science, for the benefit of current and future workers. 
 
Upcoming articles, already commissioned, will cover the roots of bioinformatics in structural biology, in evolutionary biology, and in artificial intelligence, with more in the works. These topics are obviously very broad, and so are likely to be subdivided or otherwise revisited in future installments by authors with varying perspectives. Topics and authors will be chosen at the discretion of the editors along lines broadly corresponding to the usual content of this journal. 
 
The author, having been asked to serve as Series Editor by the Editor-in-Chief, will endeavor to maintain a uniform flow of articles solicited from luminaries in the field. As a starting point to the series, I offer below a few vignettes and reflections on some longer-term influences that have shaped the discipline. I first consider the unique status of bioinformatics vis-a-vis science and technology, and then explore historical trends in biology and related fields that anticipated and prepared the way for bioinformatics. Examining the context of key moments when computers were first taken up by early adopters reveals how deep the roots of bioinformatics go.",2010,PLoS Comput. Biol.
Correction: A new model for simultaneous dimensionality reduction and time-varying functional connectivity estimation,[This corrects the article DOI: 10.1371/journal.pcbi.1008580.].,2021,PLoS Comput. Biol.
Correction: Decentralized control of insect walking: A simple neural network explains a wide range of behavioral and neurophysiological results,[This corrects the article DOI: 10.1371/journal.pcbi.1007804.].,2021,PLoS Comput. Biol.
Correction: An antigenic diversification threshold for falciparum malaria transmission at high endemicity,[This corrects the article DOI: 10.1371/journal.pcbi.1008729.].,2021,PLoS Comput. Biol.
Correction: Dynamic metabolic adaptation can promote species coexistence in competitive microbial communities,[This corrects the article DOI: 10.1371/journal.pcbi.1007896.].,2021,PLoS Comput. Biol.
Of Toasters and Molecular Ticker Tapes,"Experiments in systems neuroscience can be seen as consisting of three steps: (1) selecting the signals we are interested in, (2) probing the system with carefully chosen stimuli, and (3) getting data out of the brain. Here I discuss how emerging techniques in molecular biology are starting to improve these three steps. To estimate its future impact on experimental neuroscience, I will stress the analogy of ongoing progress with that of microprocessor production techniques. These techniques have allowed computers to simplify countless problems; because they are easier to use than mechanical timers, they are even built into toasters. Molecular biology may advance even faster than computer speeds and has made immense progress in understanding and designing molecules. These advancements may in turn produce impressive improvements to each of the three steps, ultimately shifting the bottleneck from obtaining data to interpreting it. This is an ‘‘Editors’ Outlook’’ article for PLoS Computational Biology Moore’s law has characterized progress in microprocessor techniques (see Figure 1, dashed). In very good approximation, computers have doubled in speed every 2 years. At the same time, computing has progressively gotten cheaper, and we can now successfully solve many computing problems that once seemed hard, such as speech recognition. We are also solving entirely new computational problems, such as searching billions of web pages for information on neuroscience or molecular biology. Such exponential growth makes solutions to seemingly insurmountable problems seem trivial given a bit of time. Meanwhile, simple computers have gotten cheaper over time. This decrease in costs was so dramatic that many of today’s toasters contain microprocessors for time-keeping, switching on or off heat, light feedback of heating state, and the handling of key presses. This makes building toasters simpler and ultimately cheaper. When computers were invented, toasters were certainly not an expected application of computing. Importantly, the speed of computers has increased dramatically faster than the number of neurons that can be simultaneously recorded [1] (Figure 1, dotted). Still, the increase of the number of simultaneously recorded neurons has allowed the development of advanced algorithms that take advantage of this growing number. Indeed, the field that analyzes multivariate neural data is large now and can analyze complicated interactions between large numbers of neurons [2–4], electroencephalography (EEG) or magnetoencephalography (MEG) recordings, optical recordings, or functional magnetic resonance imaging (fMRI) voxels [5,6]. However, as the amount of data increases, so does the complexity of the questions. For example, many current studies of neural data analysis ask how neurons interact, but as the number of neurons grows, the number of potential connections grows quadratically as each neuron may interact with each other neuron. This, in turn, leads to models with many free parameters, which requires new statistical methods of fitting these parameters. Molecular biology has seen accelerating progress over the last decades. One readily quantifiable cost in molecular biology is that of sequencing DNA. The development of a host of different methods has allowed the cost of sequencing each base pair to dramatically decrease over time (Figure 1, solid). The rate of improvement is much faster than that of neuron recording techniques or even Moore’s law. This development allowed sequencing the entire human genome at a price of billions of dollars in the year 2003, and sequencing the same genome at higher quality now costs less than $2,000. The current push is to sequence an entire genome for less than $1,000 [7]. This development allows solving many problems of obvious importance, such as the search for gene-related markers of disease [8]. From a computational perspective, a central objective of neuroscience is to understand how neurons convert their inputs into outputs and collectively produce action based on stimuli and internal processes, such as memory and attention. This leads to what I would call the three central steps of experimental approaches in systems neuroscience. (1) Select the signals that are important for a given neuroscience question. As long as we cannot approach understanding the entire brain at the same time, it is highly useful to select what to stimulate and what to measure. (2) Get stimuli into the brain. To understand what neurons do, inputs need to be defined or known. (3) Get data out of the brain. Only large amounts of data allow meaningful statistical inferences. Virtually all experimental approaches to systems neuroscience can be phrased in these terms. It is interesting to ponder a few well-known examples. In a typical single-cell visual cortex experiment [9] that studies how the visual cortex encodes visual stimuli, we would put an electrode into primary visual cortex (1), show various visual stimuli on a monitor in front of the animal (2), and record neural activity from the electrode (3). In a typical fMRI experiment about visual cortex [10], we would choose a contrast that tells us about changes in blood flow in the brain (which is a proxy for average firing rate) (1), stimulate subjects by using a set of fixed visual stimuli (2), and read Citation: Kording KP (2011) Of Toasters and Molecular Ticker Tapes. PLoS Comput Biol 7(12): e1002291. doi:10.1371/journal.pcbi.1002291 Editor: Karl J. Friston, University College London, United Kingdom Published December 29, 2011 Copyright: 2011 Konrad P. Kording. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: Research was funded by the NIH grants 1R01NS063399 and 2P01NS044393. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. * E-mail: kk@northwestern.edu Competing Interests: The author has declared that no competing interests exist. PLoS Computational Biology | www.ploscompbiol.org 1 December 2011 | Volume 7 | Issue 12 | e1002291 out data using resonance signals (3). In a typical slice work experiment [11], we might identify a specific cell type under the microscope (1), activate other cells using glutamate uncaging (2), and then record signals from the intracellular electrode to find out how the other cells affect the recorded cell (3). In all these cases, selection, stimulation, and reading the data (‘‘data out’’) are crucial aspects of the work. Each of these steps has its own criteria for being maximally useful. For the selection step (1), we would like to select all the relevant signals and nothing else. For the stimulation step (2), we Figure 1. Comparison of the scaling laws between neuroscience (exponential fit from [1]), computer science (exponential fit of years 2000–2007 from [29]), and DNA sequencing, see [30]. Remarkably, the steepness of the curves is worth comparing while the offset on the y-axis is arbitrary. doi:10.1371/journal.pcbi.1002291.g001 Figure 2. The three central steps that define experiments in systems neuroscience. The question mark denotes areas where the author expects exciting developments. (A) Methods for selecting where neural signals come from. (B) Methods for stimulating neurons. (C) Methods for reading out the data. See text for detail. doi:10.1371/journal.pcbi.1002291.g002 PLoS Computational Biology | www.ploscompbiol.org 2 December 2011 | Volume 7 | Issue 12 | e1002291 want to be able to get in large amounts of well defined data with high information bandwidth and low noise [12]. Lastly, for the ‘‘data out’’ step (3), we want to get a lot of data out with low noise and high bandwidth. Lastly, given limited budgets, we want the techniques to be cheap. Certainly, as we scale up the analysis towards understanding larger systems, the price per unit of information must be limited. Each of the currently used approaches has limitations along the three steps. For example, fMRI has a huge number of channels, and can select essentially any brain area. Yet, it seems unlikely that it could be used to record only from certain cell types, such as interneurons [13], noise levels are high, and spatiotemporal resolution is low. Single-cell slice physiology is good at selecting neurons based on observable features like brain region and cell morphology, and it is low noise, but data out bandwidth is quite low and there may be biases in the selection of the recorded cells. Similarly, other current techniques are all rather limited on at least one of the three axes. Molecular biology is starting to offer powerful solutions to overcome limitations of the three steps [14], and I want to start by reviewing past progress. (1) The selection step. There are many different levels of selection. We might want to select individuals that have certain diseases for which there are genetic markers. In this case, we can select these individuals through genetic tests [15]. We might want to select neurons but not other cells (Figure 2A, upper), and this is possible through a set of well-established genetic methods that enable or disable gene expression using tissue-specific promoters [16,17]. We might want to select only a subset of all the cells (Figure 2A, middle) [18]. Lastly, we might only want to only select certain neurons that have defined physiological properties (Figure 2A, lower). Interestingly, it is even possible to select each cell individually and assign it a random color that is visible under the microscope [19]. Importantly, genetically selecting neurons enables certain ways of stimulating just those neurons and reading out only from those neurons (see below). Figure 3. Molecular ticker tapes. Neural activity affects an intracellular concentration. DNA polymerase copies a ",2011,PLoS Comput. Biol.
Correction: Spatiotemporal Expression Control Correlates with Intragenic Scaffold Matrix Attachment Regions (S/MARs) in Arabidopsis thaliana,"Matrix Attachment Regions (S/MARs) in Arabidopsis thaliana Igor V. Tetko, Georg Haberer, Stephen Rudd, Blake C. Meyers, Hans-Werner Mewes, Klaus F. X. Mayer DOI: 10.1371/journal.pcbi.0020021 In PLoS Computational Biology, vol 2, issue 3: Contributing author Blake Meyers’ name and institution should appear: Blake C. Meyers, Department of Plant and Soil Sciences, Delaware Biotechnology Institute, Newark, Delaware, United States of America This correction note may be found online at DOI: 10.1371/journal.pcbi.0020067. Published June 30, 2006. Citation: (2006) Correction: Spatiotemporal expression control correlates with intragenic scaffold matrix attachment regions (S/MARs) in Arabidopsis thaliana. PLoS Comput Biol 2(6): e67.",2006,PLoS Comput. Biol.
The Significance of the 2013 Nobel Prize in Chemistry and the Challenges Ahead,"Last week, the 2013 Nobel Prize in Chemistry was awarded to Martin Karplus, Michael Levitt, and Arieh Warshal for “the development of multiscale models for complex chemical systems”. As the Royal Swedish Academy of Sciences noted, “Chemists used to create models of molecules using plastic balls and sticks. Today, the modelling is carried out in computers. In the 1970s, Martin Karplus, Michael Levitt and Arieh Warshel laid the foundation for the powerful programs that are used to understand and predict chemical processes. Computer models mirroring real life have become crucial for most advances made in chemistry today.” Furthermore, “Today the computer is just as important a tool for chemists as the test tube. Simulations are so realistic that they predict the outcome of traditional experiments.” [1] 
 
This event is a milestone for the broad community that PLOS Computational Biology represents. Along with Philip E. Bourne, the Founding Editor-in-Chief, and our Editorial Board, which proudly lists Michael Levitt among its members, I extend the warmest congratulations to the winners. Beyond the specific, personal scientific achievements that have already been widely discussed, we must consider the more general and broader context of this unique prize. Here, I would like to present this Nobel Prize within this framework, emphasizing its magnitude and far-reaching implications not only for computational biology, but for the biological community at large. 
 
In recent decades, molecular biology has progressed by leaps and bounds. Huge technological advances have taken place in sequencing; in mapping structure and dynamics via electron microscopy (EM), X-ray, and nuclear magnetic resonance (NMR); in manipulating imaging of nuclei and cells; in sequencing single biomolecules; and more. These have led to fundamental new insights; biology and medicine have soared to new heights with the DNA double helix providing the molecular basis for genetics and Darwinism. Many steps were required to identify and untangle DNA-RNA-protein sequence-structure-function and reverse transcription processes; RNA enzymes; the importance of key multi-partnered scaffolding molecules under normal physiological conditions and in disease; their structures, mutations, and the principles and mechanisms of their dynamic regulation; and other landmark developments. These involved technological breakthroughs and greater understanding of the specific mechanisms involved. Most of the Nobel prizes in chemistry and medicine in recent years have been awarded at these junctures. 
 
Vast amounts of information on sequences and structures are yet to be explained and pose a challenge for computational biology. Recently, this has been compounded by interdisciplinary studies of the nervous system, posing questions such as how it is structured, how it develops, how it works, the mechanisms of signal processing, and more, all at multiple levels, ranging from the molecular and cellular levels to the systems and cognitive levels. Thus, even if we gain in-depth insight into static properties such as the genomic data and structural snapshots of proteins (DNA and RNA) at different levels of resolution, the truly monumental challenge of understanding their dynamics still looms ahead. And eventually, it is the dynamics of molecules that provides the basis for cells, tissues, and organisms' development and work. 
 
The systems in question operate at all scales: force fields and free energy landscapes relevant for protein folding and function, large complexes, biomolecular recognition involving proteins, DNA, RNA, lipids, post-translational (and DNA) modifications, and interactions with small molecules. On a larger scale we see cellular locomotion, cell division and trafficking, and cell-cell recognition. Furthermore, beyond these lurks the working of the complex cell as a cohesive unit: the cellular network controls metabolism and regulation, intra- and inter-cellular signaling, and the neural circuits of nerve cells, where the activity of one cell directly influences many others. All are dynamic, all change with the cellular environment, and all present a daunting challenge. The relevant timescales range from femtosecond for simple chemical reactions to the eons of evolution; however, all operate with the same underlying physical principles of conformational variability and selection. 
 
At each timescale and corresponding physical size we strive to identify the relevant moving parts and degrees of freedom and to formulate effective—though often approximate—rules for their mutual interactions and resulting motion. Solving, understanding, and computing the dynamic behavior at any given scale is of great interest in its own right and provides approximate dynamical input for the next scale, which is one rung above it. Only at the lowest, most basic scale of individual atoms and electrons are the dynamical rules (electrostatics and Schrodinger's equation) completely well defined. And the all-important work cited by the Nobel Prize Committee and which is carried out by our community is roughly at the first/second level, making it of fundamental importance. 
 
This Nobel Prize is the first given to work in computational biology, indicating that the field has matured and is on a par with experimental biology. It may also be the very first prize given in any area of the exact sciences for calculations. What is different in the present case? I believe that the answer is simple: the present calculations are of much greater interest to a much broader community. In endeavoring to imitate the basic processes of life in silico, great strides are being made toward understanding the secret of life. Computational biology, and simulations, for which Martin Karplus, Michael Levitt, and Arieh Warshal shared the Nobel Prize, can carry the torch leading the sciences to decipher the elemental processes and help alleviate human suffering. 
 
What are the challenges ahead? Are simulations with timescales of microseconds, milliseconds, or beyond, under the current force field framework, capable of producing results in agreement with experiments also for large and complex proteins like membrane receptors? Do the challenges also lie in the type of questions which are asked, for which such long timescale simulations can be useful in providing answers? Or is it the biology behind the questions that is also the key? Ultimately, as in experimental biology which also exploits methods and machines, it is likely to be all of the above. Computations are our treasured tool; they are not our aim. Merely running long molecular dynamics trajectories is unlikely to advance science. 
 
PLOS Computational Biology joins the International Society of Computational Biology (ISCB) and our computational biology community in congratulating the awardees and celebrating this momentous event. 
 
This Editorial was first published as a blog post on PLOS Biologue on October 18, 2013.",2014,PLoS Comput. Biol.
Power Law Scaling in Human and Empty Room MEG Recordings,"In 2009, we published a paper in PLOS Computational Biology [1] that described using a new, wavelet-based metric of phase synchronization in human MEG data. Specifically, we showed that this metric of phase synchronization, that we called the phase lock index (PLI), demonstrated power law scaling across all frequency intervals or wavelet scales from the low frequency delta band (1–2 Hz) to the high frequency gamma band (35–70 Hz). Based on these experimental results, and additional confirmatory data obtained from PLI measurements on time series generated by computational models of critical systems, we offered the interpretation that power law scaling of phase synchronization in human MEG recordings was compatible with the prior theory that human brain dynamics demonstrate self-organized criticality. 
 
More recently, in collaboration with a group at the National Institutes of Health (NIH), we published a paper in the Journal of Neuroscience [2] that described using a similar phase synchronization metric to explore scaling behaviour in MEG data recorded from normal human subjects and, crucially, in MEG data recorded with no human subject present, so-called “empty room” data. In Figure 9 of [2], we showed data indicating that phase synchronization appeared to demonstrate power law scaling even in empty room data; as reproduced here in Fig 1. 
 
 
 
Fig 1 
 
First two panels of Figure 9 from Shriki et al. [2]. 
 
 
 
 
We originally judged this issue to be of minor concern, because, as shown in Figure 1 of [2], NIH empty scanner amplitude variance is about 1–2 orders of magnitude less than equivalent brain scans. This is most likely an underestimation given that data are Z-normalized and absolute amplitudes of empty scanner data should be lower than brain scans. Thus, we reasoned the contribution from “empty scanner” effects to PLI scaling in brain recordings should be insignificant. Nonetheless, the issue was addressed in the Discussion of [2], where we stated: 
 
“because of the ambiguity of PLI for brain scans and empty scanner, additional steps such as amplitude comparisons need to be taken into account” (page 7089 of [2]).",2015,PLoS Comput. Biol.
Correction: Inferring transmission heterogeneity using virus genealogies: Estimation and targeted prevention,[This corrects the article DOI: 10.1371/journal.pcbi.1008122.].,2021,PLoS Comput. Biol.
Correction: Bayesian inference and comparison of stochastic transcription elongation models,[This corrects the article DOI: 10.1371/journal.pcbi.1006717.].,2021,PLoS Comput. Biol.
Correction: Up-down biphasic volume response of human red blood cells to PIEZO1 activation during capillary transits,[This corrects the article DOI: 10.1371/journal.pcbi.1008706.].,2021,PLoS Comput. Biol.
Ten Simple Rules for Reviewers,"Last summer, the Student Council of the International Society for Computational Biology prompted an Editorial, “Ten Simple Rules for Getting Published” [1]. The interest in that piece (it has been downloaded 14,880 times thus far) prompted “Ten Simple Rules for Writing a Grant” [2]. With this third contribution, the “Ten Rules” series would seem to be established, and more rules for different audiences are in the making. Ten Simple Rules for Reviewers is based upon our years of experience as reviewers and as managers of the review process. Suggestions also came from PLoS staff and Editors and our research groups, the latter being new and fresh to the process of reviewing. 
 
The rules for getting articles published included advice on becoming a reviewer early in your career. If you followed that advice, by working through your mentors who will ask you to review, you will then hopefully find these Ten Rules for Reviewers helpful. There is no magic formula for what constitutes a good or a bad paper—the majority of papers fall in between—so what do you look for as a reviewer? We would suggest, above all else, you are looking for what the journal you are reviewing for prides itself on. Scientific novelty—there is just too much “me-too” in scientific papers—is often the prerequisite, but not always. There is certainly a place for papers that, for example, support existing hypotheses, or provide a new or modified interpretation of an existing finding. After journal scope, it comes down to a well-presented argument and everything else described in “Ten Simple Rules for Getting Published” [1]. Once you know what to look for in a paper, the following simple reviewer guidelines we hope will be useful. Certainly (as with all PLoS Computational Biology material) we invite readers to use the PLoS eLetters feature to suggest their own rules and comments on this important subject.",2006,PLoS Comput. Biol.
Dog as an Outgroup to Human and Mouse,"In a recent contribution to PLoS Computational Biology, Cannarozzi, Schneider, and Gonnet published evidence that rodents form an outgroup to human and dog [1], in disagreement with several recent studies suggesting that the dog is an outgroup to the primate–rodent clade [2,3]. The authors' arguments rest on a variety of analyses of human, mouse, and dog genes, using opossum to root the phylogeny. Here I argue that despite the large number of characters used in this study, their results may well be erroneous. I then provide new and, I believe, conclusive evidence in favour of the current consensus phylogeny, and I briefly review other recent studies that support this conclusion. 
 
The problem of determining the evolutionary relationship between all extant mammals has a long history. Traditionally, morphological features were used to group “like” mammals together in a tree, purportedly reflecting their phylogeny. More recently, molecular data have generally confirmed these inferences, but have also led to surprising revisions. While sequence analysis is more objective than morphology, it nevertheless emerged that it has its own set of issues, and some phylogenies remain contentious. In [1], Cannarozzi et al. suggested that this contention extends to the phylogeny of human, mouse, and dog, and inferred a phylogeny of these species that disagrees with a recently emerging consensus. Here I challenge their findings, providing new evidence in support of the consensus phylogeny, and suggest that their results may have been biased by long branch attraction (LBA), a known issue in molecular phylogenetic inference. 
 
It is well-known that phylogenetic inferences can be biased, and may be inaccurate even with strong bootstrap or posterior support. Felsenstein showed that in parsimony analyses, long branches in the phylogeny tend to attract one another [4]. In contrast to what the authors claim, maximum likelihood methods, although less vulnerable, are similarly affected by LBA [5], particularly when small numbers of taxa are used [6]. This methodological bias has led to various erroneous inferences, such as the now-discredited claim that “the guinea pig is not a rodent” [7,8]. Perhaps counterintuitively, the effect of LBA does not diminish with increasing amounts of sequence data. To quote from a review, “spurious conclusions are often derived from an over-credibility of enormous numbers of nucleotide or amino acid characters (e.g., complete genomes) when combined with poor taxon sampling” [9]. 
 
The recently emerging consensus on mammalian phylogeny based on molecular data is surprisingly different from the traditional, morphological phylogeny [2,3]. It proposes four mammalian cohorts, including the Laurasiatheria (of which the dog lineage is part), which separated from the Euarchontoglires about 85–95 million years ago (Mya) [10]. The subsequent speciation separating the Euarchontoglires into Glires (including rodents) and Euarchonta (which includes primates) occurred roughly 80 Mya. The difference is small compared with the total branch length to opossum (180 + 90 My), so that a relatively small bias would suffice to bring about a topology change. As the mouse genome sequence has been evolving fast relative to those of human and dog [11], its branch is expected to be affected by LBA to the opossum branch, which would result in the reported grouping. 
 
These considerations throw some doubt on both the parsimony and maximum likelihood analyses. What about the genome rearrangement argument? After all, genome rearrangements are large-scale but relatively infrequent events, so that the parsimony approximation might be justified. However, the opossum genome had not yet been assembled, and the authors had to resort to chicken, which diverged ∼310 Mya from the mammalian lineage, considerably earlier than the opossum did. Moreover, there is strong evidence for hotspots of breakage [12] and breakpoint reuse [13], discounting the “random breakage” model. The use of (nuclear) gene orderings to analyze rearrangements further exacerbates these issues, as it affords little power to resolve breakpoints and artificially increases inhomogeneities in breakage rates, because of large and highly variable intergenic distances. For these reasons, the parsimony approximation may well be invalid, which makes LBA a concern for the genome rearrangement analysis, too. 
 
I thus considered whether the reported tree might be incorrect. To investigate the issue, I used a simple (and, to my knowledge, novel) summary statistic based on the distribution of transposable elements (TEs) in pairwise alignments, which does not require an outgroup genome to root the phylogeny. If a family of TEs is specific to lineage x when compared with y, each occurrence in x is expected to be located opposite a gap in a whole-genome alignment of species x to y. In contrast, if the family is ancestral to x and y, a proportion of TEs will have survived in both species and will align. To quantify the evidence for these alternatives, I defined a statistic A(y|x) (for “ancestralness”) as the proportion of nucleotides from a particular TE family in species x that is aligned to a secondary species y. This statistic is near-zero if a family of TEs is specific to x, and non-zero if it is ancestral to the species split. For an outgroup x and a particular family of TEs, the statistics A(y|x) are thus expected to be consistent across ingroup species y (either zero, or non-zero, for all). In contrast, for an ingroup species, some TE families may be ancestral with respect to another ingroup, but lineage-specific when compared with the outgroup. Provided such TE families exist, this would then determine the topology of the phylogeny. 
 
The results (Figure 1 and Table 1) show clear support for the rodent–primate grouping. For example, the MLT2B2 long terminal repeat element is clearly ancestral in the human-to-mouse and mouse-to-human comparisons (A > 0.20), but is highly lineage-specific in the other comparisons, each of which include the dog (A < 0.03 for all). This pattern can be explained if dog is assumed to be an outgroup to both human and mouse, and that the element has been active primarily between the two speciation events. The same pattern was observed for several other TE families (MLT1A0, MLT2B1, L1MA9, L1MB1, L1MC1, MER31A, MER21B, MER34), while no examples supporting alternative groupings were found. Unlike analyses based on nucleotide characters, TE-based studies are not expected to suffer from LBA, because the size of TEs allows for reliable homology assignments (if well-anchored alignments are used), and the marked differences between the TE insertion and small deletion processes means that back mutations are rare. It thus appears that the dog lineage is basal to the primate and rodent lineages. 
 
 
 
Figure 1 
 
Evidence for the ((Human, Mouse), Dog) Phylogeny 
 
 
 
 
 
Table 1 
 
Ancestralness of TE Families in the Six Pairwise Comparisons between Human, Dog, and Mouse 
 
 
 
Numerous recent studies support this conclusion. When many taxa are analyzed simultaneously, the dog consistently appears as an outgroup to human and mouse, when using either nuclear or mitochondrial DNA [2,3,9,14–16]. Studies of rare genomic changes (which are less vulnerable to LBA) consistently support this grouping. For example, by rooting the phylogeny using the consensus sequence of TEs, the evolutionary distance between the speciation events was estimated to be 0.024 substitutions per site [11]. In another study, two of the TE families found here, MLT1A0 and L1MA9, were identified as clear examples supporting the rodent–primate grouping [17], and a recent analysis of several single TE insertions provides additional support [18], as does a method that uses multiple alignments of TEs to infer phylogenies in very similar ways to ours [19]. Rare indels at homologous positions in otherwise well-conserved protein-coding genes also support this phylogeny [20]. Finally, a large cluster of PRAME genes that is absent in chicken and dog, but present in homologous locations in human and mouse, again support the same grouping [21]. 
 
Taken together with the possible influence of LBA on the analysis of Cannarozzi et al. [1], it appears unjustified to continue to consider the phylogeny of primates, rodents, and canines as contentious.",2007,PLoS Comput. Biol.
Correction: Emergence of Protein Fold Families through Rational Design,"In PLoS Computational Biology, volume 2, issue 7: DOI: 10.1371/journal.pcbi.0020085 
 
The references to the figure parts in the legend of figure 3 were incorrect. The correct caption is as follows: 
 
Figure 3. The Sequence Identity for the Constructed Homologous Structures 
 
Three different protein folds are studied: HPR domain (A,D), ROSSMAN fold (B,E), and SH3 domain (C,F). (A,B,C) The sequence identities of the redesigned proteins using the flexible-backbone design simulation are presented as the function of the backbone-RMSD from the reference protein. (D,E,F) The sequence identity of the core is also plotted against the overall sequence identity. The “twilight zone” of sequence identity (20%–30%) corresponds to regions between horizontal (A,B,C) or vertical (D,E,F) lines.",2006,PLoS Comput. Biol.
Teaching Bioinformatics at the Secondary School Level,"Bioinformatics is now an integral part of biology and biological research. The field began with a few people from other disciplines teaching themselves and each other the techniques that are now considered commonplace. These pioneers then began graduate programs [1]–[3] to educate the next generation. Those early graduate students typically came as bench biologists or as computer scientists, both groups requiring significant time to “hybridize”. Not surprisingly, this then led to undergraduate majors in bioinformatics to better prepare students for graduate school and research careers in bioinformatics. In addition, teaching bioinformatics in undergraduate biology classes is also a priority [4], [5]. Through the Education section of PLoS Computational Biology we have tried to support this evolution through a collection of educational articles pertinent to the undergraduate level and beyond. It is only natural that we would take the next step [6]. 
 
We now introduce a subsection of the Education section with articles devoted to teaching bioinformatics in secondary schools that is derived from the work of the Education committee of the International Society for Computational Biology (ISCB), who identified a need to address the issue of incorporating bioinformatics into secondary school biology classes. They also recognized the interest among researchers to build and participate in outreach programs at the secondary school level given that many funding agencies worldwide encourage such a component in grant applications. 
 
To move the ball forward on secondary school bioinformatics education, at ISCB's 2010 international conference, Intelligent Systems in Molecular Biology (ISMB), the ISCB Education committee organized a half-day tutorial aimed at secondary school biology and chemistry teachers in the Boston area interested in learning about bioinformatics and how to include it in their curricula. The tutorial also attracted researchers involved in organizing or formulating outreach programs in their community. The main focus of the ISMB tutorial was the presentation of lesson plans by a secondary school teacher (David Form, a biology teacher at Nashoba Regional High School, Bolton, Massachusetts) who has successfully incorporated bioinformatics into his courses for more than five years. His is one example of such an effort and is embraced in the Ten Simple Rules and its supplementary material found in this issue. Also in this issue we have an article by Suzanne Gallagher and colleagues on the experience of teaching secondary school level bioinformatics in Boulder, Colorado. 
 
There are many examples of outreach efforts to high school students that we would like to feature in coming months, which incorporate bioinformatics into their programs (see Table 1). 
 
 
 
Table 1 
 
Examples of Online Resources and Outreach Programs. 
 
 
 
There are many other examples of educators doing similar work in school districts worldwide. A recent issue of Briefings in Bioinformatics was dedicated to bioinformatics education [7] with a specific example of programs for secondary school students [8], [9].The ISCB Education committee is building a resource of information useful to secondary school teachers who would like to incorporate bioinformatics into their curriculum. In addition, the committee has begun to explore how to include bioinformatics in Advanced Placement courses and exams in the United States, which we also hope to feature in the Education section of the journal. 
 
We encourage feedback of any form, including comments on this editorial, and hearing about your experience teaching bioinformatics to secondary school students.",2011,PLoS Comput. Biol.
Ten simple rules when considering retirement,This is an article submitted to the Ten Simple Rules series of professional development articles published by PLOS Computational Biology.,2018,PLoS Comput. Biol.
Correction: (A)Symmetric Stem Cell Replication and Cancer,"In PLoS Computational Biology, volume 3, issue 3: doi: 10.1371/journal.pcbi.0030053 
 
An author's name was misspelled as Fransizka Michor and should read: 
 
Franziska Michor",2007,PLoS Comput. Biol.
Correction: In Search of the Biological Significance of Modular Structures in Protein Networks,"In PLoS Computational Biology, volume 3, issue 6: 10.1371/journal.pcbi.0030107 
 
In the subsection ""Evolutionary conservation of modules and proteins"" of the Materials and Methods section, a link was incomplete. The correct link reads: 
 
http://rd.plos.org/10.1371_journal.pcbi.0030107_01_0",2007,PLoS Comput. Biol.
How to Write a Presubmission Inquiry,"Like many other journals, the journal PLOS Computational Biology admits and in some cases requires presubmission inquiries to be submitted before the submission of a full paper. Presubmission inquiries serve the purpose of informing the journal’s Editorial Board of the essence of the intended submission. Based on the information in the inquiry, the editors can make a quick assessment of its contribution with respect to the criteria for publication in the journal. This assessment is then communicated to the authors. This enables fast turnaround to the authors about the basic suitability of a submission for processing by the journal and spares the editors and reviewers the effort of detailed inspection of submissions that clearly do not meet the criteria of the journal. 
 
In this Editorial, we give suggestions for preparing presubmission inquiries for journal submissions. We exemplify these suggestions with reference to presubmission inquiries for the Methods section of PLOS Computational Biology. However, our suggestions generalize to presubmission inquiries of other kinds and for other journals, and in places, we will make specific comments to that effect. 
 
 
Over two years ago, PLOS Computational Biology opened a special section dedicated to Methods papers. As the scope statement spells out, 
 
Methods papers should describe outstanding methods of exceptional importance that have been shown, or have the promise to provide new biological insights. The method must already be widely adopted, or have the promise of wide adoption by a broad community of users. Enhancements to existing published methods will only be considered if those enhancements bring exceptional new capabilities. 
 
 
 
 
Since Methods papers are different from other research papers in PLOS Computational Biology and also differ from typical papers on bioinformatics methods published in other journals, a mandatory presubmission stage has been introduced for the submission of Methods papers to PLOS Computational Biology. (Note that a presubmission inquiry is not mandatory for general research papers in PLOS Computational Biology, though it is also mandatory for submission to the Software papers category.) 
 
Since the Methods section was launched in October 2012, we have received 334 presubmission inquiries. For roughly half of them (159), we encouraged submission and received full papers, of which 41 papers were published, so far, as Methods papers. We find that, while many presubmission inquiries are informative enough to make an educated decision on the submission, we also receive a number of presubmission inquiries that are not sufficiently informative, such that submission may be discouraged not on the basis of the quality or scope of the paper but on that of the presubmission inquiry. We generally do not allow revisions of presubmission inquiries. In order to minimize the number of papers that fail to get a chance to be published in PLOS Computational Biology merely because of the inadequacy of the presubmission inquiry, here we give a number of suggestions for preparing such an inquiry. 
 
The goal of a presubmission inquiry is to make the statement to the Editorial Board that the paper to be submitted reasonably satisfies the criteria detailed in the scope statement. The presubmission inquiry must be detailed enough to convincingly make that point. Most of the insufficiently informative inquiries that we get are either too terse, i.e., they do not give enough detail, or they are not specific enough. Therefore, we suggest a way of structuring a presubmission inquiry. These suggestions are the result of our experience with presubmission inquiries over the past couple of years. 
 
 
What is the problem? Please summarize the problem domain and statement and the relevance of the problem to the general readership of the journal—in the case of PLOS Computational Biology, the biological research community or a substantial subcommunity. What is that subcommunity? How relevant is the problem to them? 
 
 
What is the innovation? Here it is important that you give enough detail on your contribution to allow the Editorial Board to form an image of the substance and relevance of the advance over the state of the art in the field and over your previous work. If your paper presents material that rests on or is related to your own previous publications, this entails addressing dual publication issues. In order to argue your point, you have to summarize the state of the art on which you base your contribution and give the essential ingredients of your innovation. Depending on the journal, the innovation can take different shapes: a contribution to technology or experimental design, a biological finding, a methodical or theoretical piece of work, etc. For papers in the Methods section of PLOS Computational Biology, the computational method is expected to be at the center of the innovation. This section is not for papers whose methodical core has been published elsewhere and for which you present a—possibly extended or modified—application scenario. Also, studies presenting a comparative assessment of existing methods on an application domain are not within the scope of a Methods paper. We expect a concrete and specific relationship to underlying biological issues. This is why general methods on statistical learning that find their application in biology as well as in other fields of science are typically not considered in scope, unless the paper focuses on sufficiently deep issues of the configuration of the method that are specific to biology. Finally, the method must be the major innovation of the paper. However interesting it may be, a biological finding that has been obtained with methods that are prepublished or only minor modification of prepublished methods is not within the scope of the Methods papers category. (On the other hand, it may be a suitable General research paper for the journal.) 
 
 
How is the method validated? Validation can take manifold forms but is a key element in most scientific papers. For a theoretical paper, the validation often takes the form of a proof. In contrast, methods in computational biology have manifold forms of validation and, usually, a single form is not sufficient to make the point. For instance, for the Methods section of PLOS Computational Biology, we expect more than an anecdotal validation based on a couple of biological use cases. A validation purely on synthetic data is not sufficient either. Rather, the validation must make a convincing argument for the general applicability of the method in a substantial biological problem domain. Please note, however, that papers that center on the validation of a method that has been published elsewhere are not considered in scope either. The paper has to contain both the method and its application. 
 
 
How is the method being made available? Availability of research results becomes an increasingly desired and often required aspect of a publication. For papers that are based on experimental data, making the data and the protocols of the experimental design available is a prerequisite for making the research reproducible. For methods papers, reusability of the method also becomes an issue. For the Methods section of PLOS Computational Biology, we only accept papers on methods that are useful to and can be readily applied by other scientists. The best way of satisfying this criterion is to make the software implementation of the methods openly available. For methods that are not based on software, a workable protocol for how to use the methods must be provided. 
 
 
 
If you have the full submission ready at the time of the presubmission inquiry, we encourage you to attach it to the inquiry as an optional supplement and mention that you have done this in your cover letter. However, your presubmission inquiry should be worded such that the editors do not need to inspect the complete paper for making their assessment. 
 
We wish you much success with your future submission to the Methods section of PLOS Computational Biology.",2015,PLoS Comput. Biol.
Correction: Humans adapt their anticipatory eye movements to the volatility of visual motion properties,[This corrects the article DOI: 10.1371/journal.pcbi.1007438.].,2020,PLoS Comput. Biol.
"Bioinformatics in Malaysia: Hope, Initiative, Effort, Reality, and Challenges","The published articles in PLoS Computational Biology on the development of computational biology research in Mexico, Brazil, Cuba, Costa Rica, and Thailand have inspired us to report on the development of bioinformatics activities in Malaysia. Rapid progress in molecular biology research and biotechnology in Malaysia has created sufficient demand for bioinformatics in Malaysia. Although bioinformatics in Malaysia started in the early 1990s, the initial focus on the development of the biotechnology industry has curtailed the early gains and overshadowed the systematic development of bioinformatics in Malaysia, which currently lacks in human capital development, research, and commercialization. However, government initiatives have been devised to develop the necessary national bioinformatics network and human resource development programs and to provide the necessary infrastructure, connectivity, and resources for bioinformatics. Stakeholders are experiencing reorientation and consolidating existing strengths to align with the global trends in bioinformatics. This exercise is expected to reinvigorate the bioinformatics industry in Malaysia. Tapping into niche expertise and resources such as biodiversity and coupling it with the existing biotechnology infrastructure will help to create sustainable development momentum for the future. An initiative arose from several senior scientists across local universities in Malaysia to promote this new scientific discipline in the country.",2009,PLoS Comput. Biol.
Correction: Cell Type Specific Alterations in Interchromosomal Networks across the Cell Cycle,"The PLOS Computational Biology Staff Figure 1 is incorrect. In Figure 1E, S phase calculations were deleted from the figure during formatting. The authors have provided a corrected version here. The figure legend remains correct.",2014,PLoS Comput. Biol.
Moving Education Forward,"The need to train biologists in computational methods is greater than ever before. Recently, I asked an MIT Biology Professor if his newer graduate students were more knowledgeable in bioinformatics and computational biology than in past years. He hesitated and said he needed to think more about it; there wasn't a clear answer. What is clear, however, is that most graduate students are computer savvy and are comfortable using computers for writing and searching the Web. They may also have used computational tools on the Web but with little understanding of the guts of the algorithm. It is this group of students, as well as those further along in their careers, whom we hope to reach with the publication of learned articles in the Education section of PLoS Computational Biology. Regardless of readers' levels of experience, we strive to provide a starting point for inquisitive minds. 
 
The Education column of PLoS Computational Biology was introduced one year ago [1] with the goal to provide both practical and background information on important computational methods used to investigate interesting biological problems. During this first year, we have made strides toward this goal by publishing three excellent Education articles on diverse topics. 
 
Our first tutorial was presented in April 2006 and was well-received, staying on the list of PLoS Computational Biology Top 10 articles downloaded well after its publication date. Practical Strategies for Discovering Regulatory DNA Sequence Motifs by MacIsaac and Fraenkel [2] explores microarray experiments, a common way of investigating gene expression. Identifying sequence motifs in sets of differentially regulated genes can provide insights into the mechanisms of regulation, but care must be taken to avoid the many spurious motifs that occur in genomic sequences. This first tutorial provides strategies to improve the chances of finding functional regulatory sites. The continued interest in this first of many tutorials to come demonstrates the need for high-quality tutorials for colleagues at all career levels. 
 
“Regardless of readers' levels of experience, we strive to provide a starting point for inquisitive minds.” 
 
The Education section has also featured two review articles addressing topics of wide ranging interest—Functional Classification Using Phylogenomic Inference by Brown and Sjolander [3] and Modularity and Dynamics of Cellular Networks by Qi and Ge [4]. The article by Brown and Sjolander reviews the field of phylogenomic inference, a field that attempts to address, in an evolutionary context, the question: “What function does this protein perform?” This is a question many scientists struggle with in their research. The possibility of using phylogenomic inference as a solution relies heavily on the quality of available tools and on available data. Brown and Sjolander provide an excellent overview of the field, including its pitfalls and successes. 
 
In the second review article [4], Qi and Ge summarize strategies and results for analyzing the architecture and dynamics of cellular networks. Given the quantity of high-throughput data available, it is important to learn about computational tools that can be used to understand cellular networks. To this end, Qi and Ge provide an overview of current methods as well as some insight into what is to come. The topological features and dynamic properties of complex biological networks may be the underlying controls of phenotypes and behaviors of cells. 
 
In this first issue of PLoS Computational Biology in 2007, we publish the first of several tutorials presented at the Intelligent Systems for Molecular Biology meeting in Fortaleza, Brazil, in August 2006 (Genomes, Browsers and Databases: Tools for Automated Data Integration across Multiple Genomes by Schattner [5]). This tutorial is particularly useful as it describes tools for facilitating automated, genome database–querying. Many important biological questions cannot be answered simply by querying one genomic location at a time. As the author argues, it is far more efficient and informative to use batch- and programmatic database–querying. 
 
Journal plans for this coming year include tutorials on machine learning applications in biology, text mining, and computational proteomics—just to name a few topics. But to ensure we are able to provide educational materials of value to our readership, we look to the community. If you have prepared and presented a tutorial for an oral presentation, consider submitting it to PLoS Computational Biology. Likewise, let us know of material already on the Web that you have found useful in educating yourselves and your students. And last, we encourage readers to indicate topics of special interest and, even better, to submit articles on these topics. In your so doing, we may better serve and inspire the widening computational biology community. Please send correspondence to gro.solp@noitacude;f5000x#&loibpmocsolp.",2007,PLoS Comput. Biol.
The Young PI Buzz: Learning from the Organizers of the Junior Principal Investigator Meeting at ISMB-ECCB 2013,"If you are a young principal investigator (PI) in the field of computational biology or bioinformatics, you may have noticed recently there is a buzz surrounding you: a plethora of meetings and seminars are being organized specifically for young PIs (P2P workshop at ISMB 2012, An Excellent Research Career Workshop 2012, EMBO Young Scientists' Forum, Young PI Forum at Weizmann Institute 2009–2013, Young Investigators' Meeting by NCI). The challenges faced by young PIs are being discussed widely [1], particularly across social media [2]; funding agencies are searching for new ways to encourage young investigators; new awards are being created; and novel journals, such as EuPA Open Proteomics, provide opportunities tailored for junior scientists. Picking up on this buzz and recognizing the need for a discussion platform, PLOS has established the About My Lab collection of publications. This article is a part of this collection, highlighting the latest event organized by, and for, young PIs: the Junior PI (JPI) meeting. 
 
The JPI meeting took place in Berlin, Germany, at this year's ISMB-ECCB 2013, the flagship conference of the International Society for Computational Biology (ISCB). With the support of the ISCB Board of Directors, the meeting was conceived and organized by a group of ISCB's young PIs, most of whom are former ISCB Student Council leaders. The meeting was a mixture of scientific talks, round-table discussions, and peer-to-peer interaction. To facilitate discussion and interaction, all participants introduced themselves during the joint breakfast. This was followed by three Frontiers in Science talks, in which researchers who recently started their own group gave a review-like overview of their research field and the challenges ahead. The keynote, by Jean Peccoud, dealt with how to run a research lab as a business [3], and how to use tracking tools to account for the productivity of lab members, which invoked plenty of discussion. In the afternoon, several round-table discussions ensued, with summaries presented to the entire audience at the end of the meeting. Since the prospective participants were asked in advance for topics of importance, these discussions were precisely tailored to reflect the interests of the audience. 
 
The meeting turned out to strike the right balance between scientific talks, experience exchange, getting to know each other, and networking opportunities. The success of the JPI meeting, while critically dependent on the input of the participants, may also be accredited to its organizers, each of whom brought his/her own experience, questions, and passions. Interestingly, some of the organizers are still in the postdoc-PI transition phase, which may explain why they are highly motivated to improve the life of a young PI. Moreover, it is becoming increasingly common in modern science for many postdocs to be involved in supervision of research staff, blurring the conventional distinction between a postdoc and PI. This rise of the postdoc as principal investigator was reflected in a recent report by the European University Institute [4]. 
 
This article is different from other About My Lab articles, each following the approach “one author—one interview.” Inspired by the experimental approach of the JPI meeting itself, we present you with six short interviews with the JPI meeting organizers, carried out by the Guest Editor of About My Lab (TA). By providing different opinions, these interviews shed light on some of the key issues of a young PI's career.",2013,PLoS Comput. Biol.
Correction: Exploring the Conformational Transitions of Biomolecular Systems Using a Simple Two-State Anisotropic Network Model,"Published Copyright: ß 2014 The PLOS Computational Biology Staff. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",2014,PLoS Comput. Biol.
"Moving Education Forward, Again!","Educating biologists and computational biologists in methods and analyses is an ever-growing challenge. The amount of data and tools available to the scientific community continue to grow, and with that, there is a growing need to teach how to get the most out of this information. The PLOS Computational Biology Education section was launched in January 2006 [1] to address this challenge, with the first contribution published in April of that year [2]. In these past eight years, we have published more than 50 Education articles on different topics and have started two specialized Education collections: “Bioinformatics: Starting Early” [3], which addresses the needs of teaching bioinformatics at the secondary school level, and the first PLOS online textbook, “Translational Bioinformatics” [4]. 
 
After eight years in the role of editor of this section, I have decided that it is time to step aside to allow fresh ideas to be introduced into building the Education section and new directions to be followed as our science continues to mature. I am delighted that Francis Ouellette (Ontario Institute for Cancer Research, Toronto, Canada) and Joanne Fox (University of British Columbia, Vancouver, Canada) will be the new Education Editors at PLOS Computational Biology. They are both leaders in their fields with widespread interests in education and training of various audiences. 
 
It has been extremely fulfilling to be the Education Editor during this time. I have worked with so many talented people on the PLOS Computational Biology staff. Special thanks go to Evie Browne and Catherine Nancarrow for helping to get the section off the ground so many years ago. It has also been a pleasure and incredible learning experience to work with Philip E. Bourne, Founding Editor-in-Chief and, more recently, with Ruth Nussinov, Editor-in-Chief. 
 
In addition to the staff (both past and current), it has been wonderful to work with so many creative scientists in the field who have proposed and written articles on diverse topics. And, of course, thanks to all the reviewers who helped improve these articles. In the beginning, I invited people I knew to contribute articles to the Education section. Little by little, as the section matured, proposals were submitted directly to us. I have learned so much reading and editing the published articles and have used this wealth of information to continue to educate and train biologists here at Whitehead Institute. It is also rewarding to hear from colleagues at other institutions about their creative uses of these materials. 
 
I'll end by remarking on the power of networking in the scientific community. While attending the ISMB 2005 conference in Detroit, Michigan, I had a brief conversation with Mark Patterson who was then Director of Publishing at PLOS. I had worked with him on some of his earlier ventures and chatted with him about some of my then current interests. I told him that I had been focusing on training and educating biologists in computational methods. My group (Bioinformatics and Research Computing) at Whitehead Institute had developed numerous three-day courses to teach biologists at the Institute and in the greater Boston community. He said something to the effect, “You should talk to Phil Bourne who recently became Editor-in-Chief of a new journal, PLOS Computational Biology. He is interested in something related to education for the journal.” It was that five- or ten-minute conversation that led to a very rewarding activity and rich experience during these past eight years. My advice: don't be shy; share your thoughts and ideas with others and see where it leads. For me it was an extremely short but valuable conversation. 
 
I look forward to following PLOS Computational Biology and the Education section as they continue to grow in ways unimaginable eight years ago.",2013,PLoS Comput. Biol.
Mean-Fields and Neural Masses,"This issue's front section features a special Review: The Dynamic Brain: From Spiking Neurons to Neural Masses and Cortical Fields. I was asked to write this Editorial on behalf of the editors and coauthors of the article, as an introduction— an introduction that we hope will serve as an aperitif for the article itself. The Review was solicited by Rolf Kotter (PLoS Computational Biology Reviews Editor) at one of the annual Brain Connectivity Workshops (http://www.hirnforschung.net/bcw), which we cofounded many years ago. These are highly interactive, discussion-orientated meetings that focus on the latest advances in modelling functional integration and coupling in the brain. They naturally attract a broad range of computational neuroscientists, neurophysiologists, neuroanatomists, and cognitive scientists. That year, we seemed drawn to computational neurobiology and the models of neuronal dynamics that were being used to understand interactions in the brain. 
 
After a couple of days, it became clear that we were all using exactly the same rhetoric in reference to completely different things. Terms such as mean-field approximations, mass-action, neural-mass models, neural-field models, density-dynamics, etc., were used with exuberance but did not really help us to communicate. As a result, Rolf challenged us to define and synthesise these perspectives in a coherent and pragmatic way; the response to that challenge is the article in this issue of PLoS Computational Biology by Deco, Jirsa, Robinson, Breakspear, and Friston, which took more than two years to prepare. The difficulties became evident quickly when we started to integrate our respective contributions. These were far from coherent and disclosed some fundamental differences in the perspectives adopted on modelling distributed brain activity at a statistical level. These differences ranged from the semantic (e.g., what does “mass” mean in neural-mass models) to the sublime (e.g., some of us clearly found it difficult, if not heartbreaking, to give up our favourite variables to ensure notational consistency). This, in part, reflects the fact that the coauthors are all autonomous and senior scientists in their own fields, working on different continents, and emerging from very distinct intellectual backgrounds. Even the way in which these models were being used differed markedly among the contributors. After numerous iterations and cross-revision, the end product started to attain a degree of coherence (although some might argue there is still room for improvement). 
 
The basic issue our synthesis tries to address is how different models, used to simulate and predict observed brain dynamics, can be traced back to their common fundaments. We then try to illustrate the diversity of applications that can be entertained with these models. The basis of these models rests on modelling, not on the behaviour of individual nerve cells or neurons, but on the probability density over ensembles or populations of similar neurons. The Fokker-Planck formalism becomes central here and can be harnessed using neuronal models that are cast in terms of differential equations, with or without discrete behaviours (e.g., neuronal spiking or firing). From the density dynamics afforded by the Fokker-Planck equation, we then pursue various simplifications and special cases. An important example is when the density becomes a point-mass over the expected states of a population. These are referred to as neural-mass models and predominate in the computational neuroscience literature. A key generalisation of these neural-mass models is to neural-field models, where the location of the mass or expected state of a population becomes a function of both time and position on the brain's cortical surface or subcortical structures. These models generate all sorts of interesting and neuronally plausible patterns and self-organising phenomena, which can be inferred through invasive or non-invasive electrophysiological recordings of real brains. 
 
The applications of these models are essentially twofold; some authors use them to understand the basic principles of neuronal dynamics and implicit computations; for example, understanding dynamics in terms of nonlinear mechanisms such as bifurcations, understanding perceptual categorisation in terms of multistability, or identifying the domains of parameter–space that support commonly observed spatiotemporal patterns of activity. Other authors use these models as forward or generative models, whose parameters can be optimised to reproduce an observed dataset. This is known as model inversion and allows one to estimate important biophysical constants and parameters from empirical data. In this context, one can also explore model spaces and use data to adjudicate among various neural-mass or field models. We have chosen a few key examples that highlight the necessary role of density-dynamics and mean-field approximations in computational neuroscience. 
 
Participating in this Review has been an enormously enlightening experience; at the same time it was exhausting and something I will think twice about before engaging in again. This is not meant to be a brief orientation to the field but more an attempt to provide a reference framework for people to understand their own contributions, in relation to others. It is a Review that I look forward to giving to my students; although I suspect they will take more than a weekend to digest it.",2008,PLoS Comput. Biol.
Mean-Fields and Neural Masses,"This issue's front section features a special Review: The Dynamic Brain: From Spiking Neurons to Neural Masses and Cortical Fields. I was asked to write this Editorial on behalf of the editors and coauthors of the article, as an introduction— an introduction that we hope will serve as an aperitif for the article itself. The Review was solicited by Rolf Kotter (PLoS Computational Biology Reviews Editor) at one of the annual Brain Connectivity Workshops (http://www.hirnforschung.net/bcw), which we cofounded many years ago. These are highly interactive, discussion-orientated meetings that focus on the latest advances in modelling functional integration and coupling in the brain. They naturally attract a broad range of computational neuroscientists, neurophysiologists, neuroanatomists, and cognitive scientists. That year, we seemed drawn to computational neurobiology and the models of neuronal dynamics that were being used to understand interactions in the brain. 
 
After a couple of days, it became clear that we were all using exactly the same rhetoric in reference to completely different things. Terms such as mean-field approximations, mass-action, neural-mass models, neural-field models, density-dynamics, etc., were used with exuberance but did not really help us to communicate. As a result, Rolf challenged us to define and synthesise these perspectives in a coherent and pragmatic way; the response to that challenge is the article in this issue of PLoS Computational Biology by Deco, Jirsa, Robinson, Breakspear, and Friston, which took more than two years to prepare. The difficulties became evident quickly when we started to integrate our respective contributions. These were far from coherent and disclosed some fundamental differences in the perspectives adopted on modelling distributed brain activity at a statistical level. These differences ranged from the semantic (e.g., what does “mass” mean in neural-mass models) to the sublime (e.g., some of us clearly found it difficult, if not heartbreaking, to give up our favourite variables to ensure notational consistency). This, in part, reflects the fact that the coauthors are all autonomous and senior scientists in their own fields, working on different continents, and emerging from very distinct intellectual backgrounds. Even the way in which these models were being used differed markedly among the contributors. After numerous iterations and cross-revision, the end product started to attain a degree of coherence (although some might argue there is still room for improvement). 
 
The basic issue our synthesis tries to address is how different models, used to simulate and predict observed brain dynamics, can be traced back to their common fundaments. We then try to illustrate the diversity of applications that can be entertained with these models. The basis of these models rests on modelling, not on the behaviour of individual nerve cells or neurons, but on the probability density over ensembles or populations of similar neurons. The Fokker-Planck formalism becomes central here and can be harnessed using neuronal models that are cast in terms of differential equations, with or without discrete behaviours (e.g., neuronal spiking or firing). From the density dynamics afforded by the Fokker-Planck equation, we then pursue various simplifications and special cases. An important example is when the density becomes a point-mass over the expected states of a population. These are referred to as neural-mass models and predominate in the computational neuroscience literature. A key generalisation of these neural-mass models is to neural-field models, where the location of the mass or expected state of a population becomes a function of both time and position on the brain's cortical surface or subcortical structures. These models generate all sorts of interesting and neuronally plausible patterns and self-organising phenomena, which can be inferred through invasive or non-invasive electrophysiological recordings of real brains. 
 
The applications of these models are essentially twofold; some authors use them to understand the basic principles of neuronal dynamics and implicit computations; for example, understanding dynamics in terms of nonlinear mechanisms such as bifurcations, understanding perceptual categorisation in terms of multistability, or identifying the domains of parameter–space that support commonly observed spatiotemporal patterns of activity. Other authors use these models as forward or generative models, whose parameters can be optimised to reproduce an observed dataset. This is known as model inversion and allows one to estimate important biophysical constants and parameters from empirical data. In this context, one can also explore model spaces and use data to adjudicate among various neural-mass or field models. We have chosen a few key examples that highlight the necessary role of density-dynamics and mean-field approximations in computational neuroscience. 
 
Participating in this Review has been an enormously enlightening experience; at the same time it was exhausting and something I will think twice about before engaging in again. This is not meant to be a brief orientation to the field but more an attempt to provide a reference framework for people to understand their own contributions, in relation to others. It is a Review that I look forward to giving to my students; although I suspect they will take more than a weekend to digest it.",2008,PLoS Comput. Biol.
Seven Years; It's Time for a Change,"After seven years as the Editor-in-Chief of PLOS Computational Biology, I have decided to step to the side. It's time to bring in new leadership and a new vision. As scientists we generally do not learn a lot of management skills (a mistake in my opinion), but if I have learnt two management skills it is the value of enablement and to start planning for your successor on day one. Well, I did not start on day one, but Ruth Nussinov has been the Deputy Editor-in-Chief since October 2008, and she is an outstanding scientist and editor ideally suited to take over as editor-in-chief. So please welcome Ruth to this leadership role. The journal is in very safe hands. The journal staff, editors, and Ruth have not seen the last of me, however—this is just too much fun. As Founding Editor-in-Chief, I will continue to be involved with the journal, with a focus on special projects, helping Ruth and the team where I can, and, of course, continue as an author of both research articles and front matter, such as the Ten Simple Rules series. 
 
In changing roles, I would like to make a few personal comments about the evolution of the journal and where it might go next. When Steven Brenner, Michael Eisen, and I founded the journal, we had a vision for how it would fill a gap between journals supporting purely computational methods and the array of experimental journals with the odd, token computational paper [1]. Thanks to you, the readers and authors, that vision has been realized beyond what we imagined, and I am very proud of how the journal is a voice for our broad and important community and at the same time helps build that community. The appearance of the journal in June 2005 was timely since it both propelled our field of science at a time when it was being recognized as a critical part of the life sciences, and made a strong statement about the importance of open access. 
 
I must confess that when we started planning for the journal in 2004, support for open access seemed like the right thing to do, but it was not a major driver for me. That changed when, early on in my tenure, I realized open access is critical to maximizing the rate of scientific discovery. Supporting open access, and the new forms of scholarly communication it fosters, enriched my career and brought me into contact with many amazing people I would not otherwise have met. Emphasizing that enrichment, of the 22 invited lectures I gave in 2011, 18 were evangelizing about the importance of open access and open science, and four were directly related to my science. In my opinion, we have yet to see open access reach its full potential, but we will [2], and our journal will be poised to play an ever-increasing role as an exemplar for what is possible. In short, PLOS and this journal will continue to foster change, which maximizes the accessibility and comprehension of science. I am proud to continue to be a part of that. 
 
It only remains for me to thank and acknowledge a variety of people who have all been so critically important in the past seven years. First and foremost are the approximately 150 editors who have worked tirelessly to shape and ensure a high-quality product over the years. There is no journal without community-driven efforts, which offer limited reward beyond a job well done. I can't mention everyone, but I must call out Karl Friston, who between 2005 and 2010 worked tirelessly to make computational neuroscience such a rich part of the journal, and Steven Brenner, Simon Levin, and Sebastian Bonhoeffer, who, since the journal's inception, have always responded with good advice. 
 
Thanks are also due to Mark Patterson, who, until late 2011, was Director of Publishing at PLOS. Mark was critical to the success of the journal and to PLOS as a whole. We first began talking about the journal in May 2004, and he listened to, refined, and contributed to a lot of crazy ideas that define what the journal is today. To Catherine Nancarrow, who managed the journal from 2005 to 2010. A nicer and more dedicated person will not be found. In the current era of 140 character snapshots, her emails conferred a quality, beauty, and caring that you just don't see anymore. To Evie Browne, who contributed so much as Publications Assistant and Publications Manager between 2006 and 2009. Her spreadsheet analyses of how the journal was doing were amazing. To Andy Collings, who in various roles from Publications Assistant to Editorial Manager from 2005 to 2012 just made all ideas work, however outside the box they were. To Fran Lewitter, who, as Education Editor since the beginning, has created an important community jewel. To Scott Markel, who has been a voice of reason and an interface to the International Society of Computational Biology (ISCB) over the years. Finally, to all the other staff who have contributed over the past seven years: Emily Stevenson, Johanna Dehlinger, Helen Budd, Sheran Basra, and Cecy Marden, and the amazing current staff of Laura Taylor, Clare Weaver, and Chris Hall, all so well led by Rosemary Dickin and Theo Bloom. 
 
PLOS is a family of journals and a family of people; in short, a family organization whose goal is to disseminate science in the most open and useful way possible. It is a successful organization able to recruit the best and most dedicated staff, and to adjust to its growing success and the success of open access itself. The world of scientific publishing will never be the same again because of PLOS and the people who drive it. I am proud to continue to be a member of this amazing family. There is no other publisher like PLOS and no other journal like PLOS Computational Biology.",2012,PLoS Comput. Biol.
Correction: Persistent Activity in Neural Networks with Dynamic Synapses,"In PLoS Computational Biology, volume 3, issue 2: doi: 10.1371/journal.pcbi.0030035 
 
 
Equation 28 had incorrect symbols in lines 3, 6, and 7. Here is the correct Equation 28:",2007,PLoS Comput. Biol.
Correction: Hard real-time closed-loop electrophysiology with the Real-Time eXperiment Interface (RTXI),[This corrects the article DOI: 10.1371/journal.pcbi.1005430.].,2017,PLoS Computational Biology
Correction: Inferring Loss-of-Heterozygosity from Unpaired Tumors Using High-Density Oligonucleotide SNP Arrays,"In PLoS Computational Biology, vol 2, issue 12: doi:10.1371/journal.pcbi.0020156 
 
In the Acknowledgments section of this Perspective, the spelling of the name of Pavel Pevzner was incorrect.",2007,PLoS Comput. Biol.
Correction: The Role of Compensatory Mutations in the Emergence of Drug Resistance,"In PLoS Computational Biology, volume 2, issue 10: doi:10.1371/journal.pcbi.0020137 
 
 
Equation 1 contained typographical errors in the first and last lines. The correct equation is as follows:",2007,PLoS Comput. Biol.
BioLINK Special Interest Group Session on the Future of Scientific Publishing,"Scott Markel is Vice President, International Society for Computational Biology (ISCB); Co-Chair, ISCB Publications Committee; and Associate Editor, PLoS Computational Biology. 
 
The BioLINK SIG meeting has been regularly held in association with the ISMB conference (Intelligent Systems for Molecular Biology—the annual conference of the International Society for Computational Biology) since 2001, focusing on the development and application of resources and tools for biomedical text mining. The SIG (Special Interest Group) is interdisciplinary in nature, and brings together researchers applying natural language processing, text mining, and information extraction and retrieval in the biomedical domain with scientists from bioinformatics and biology. This year's meeting at the combined ISMB/ECCB (European Conference on Computational Biology) conference in Stockholm includes two new sessions, one dedicated to extraction of information from images, and one devoted to the future of scientific publishing. The publishing session, co-organized by BioLINK with the collaboration of the ISCB Publications Committee (http://www.iscb.org/iscb-leadership-a-staff-/117) and PLoS Computational Biology (http://www.ploscompbiol.org), has been added in response to the very favorable reviews of last year's Special Session on the same topic. The session format has been expanded to two two-hour segments, both of which will be open to ISMB conference registrants. The first segment will feature scientific presentations from David Shotton, Anita de Waard, Dietrich Rebholz-Schuhmann, and Philip E. Bourne. The second segment will include presentations from journal publishers and will finish with an open discussion.",2009,PLoS Comput. Biol.
Correction: How Does Cross-Reactive Stimulation Affect the Longevity of CD8+ T Cell Memory?,"In PLoS Computational Biology, volume 2, issue 6: DOI: 10.1371/journal.pcbi.0020055 
 
The Figure 5 title and legend should read: 
 
Figure 5 Variance of the Natural Logarithm of Size of Memory Lineages as the Function of the Number of Exposures in the Absence (squares, 0) and Presence (diamonds, ) of Variation in Cross-Reactivity between Different Memory Lineages 
 
Other parameters are the same as in Figure 2B and 2D, and the mean cross-reactivity is kept the same at . Lines show the predictions according to Equation 9, and .",2006,PLoS Comput. Biol.
The ISCB: Growing and Evolving in Step with Science,"I n 1993, the Intelligent Systems for Molecular Biology conference was successfully launched as an annual meeting for scientific exchange within the nascent interdisciplinary science of computational biology. Demand grew stronger each year for an umbrella organization that extended its reach beyond a once-a-year conference. The International Society for Computational Biology (ISCB) was formed in 1997 to provide computational biologists and bioinformaticians worldwide with a community of peers with whom they could interact year-round in their mutual quest to advance the understanding of living systems through computation. Over the past 13 years, the ISCB has grown and evolved along with the fields of computational biology and bioinformatics. As the society’s president, I am proud to report that our membership now comprises nearly 2,000 members in over 50 countries. Each year, PLoS Computational Biology, as the official journal of the ISCB, will publish the bylaws of the society. These are the legally registered rules by which the elected and appointed leaders of the society, along with the membership at large, must abide. It is my hope that ISCB members will take the time to read these rules, and to understand, thereby, the legal framework in which ISCB operates. Clearly, as ISCB grows, these rules will be adapted to changing circumstances—knowing the current rules is a first step to making any improvements to them. We also believe that our bylaws may be useful to other groups trying to form similar organizations. International Society for Computational Biology Bylaws",2005,PLoS Comput. Biol.
ISCB Honors David Haussler and Aviv Regev,"Each year, the International Society for Computational Biology (ISCB; http://www.iscb.org) gives two major awards to leaders and innovators in the field of bioinformatics. The awards committee, composed of current and past directors of the Society and previous award winners, has announced that the 2008 Accomplishment by a Senior Scientist Award will be given to David Haussler of the University of California Santa Cruz, Santa Cruz, California, United States, and the 2008 Overton Prize for outstanding achievement in early to midde career will be awarded to Aviv Regev of MIT and the Broad Institute of MIT and Harvard, Cambridge, Massachusetts, United States. Both award winners are Associate Editors for PLoS Computational Biology, an official journal of ISCB. 
 
Soren Brunak, director of the Center for Biological Sequence Analysis at Technical University of Denmark and ISCB Awards Committee chair, admitted that the choice of award winners had not been easy. “These awards are a sign of recognition of achievement not just from ISCB, but from the whole bioinformatics community,” he said. “It is a significant honor to receive one.” He highlighted the fact that the 2008 award winners had made major contributions to both the development of algorithms and the application of those algorithms to “real life” biological problems. 
 
Both awards will be presented at the Society's flagship international conference, ISMB (Intelligent Systems for Molecular Biology) 2008, where the winners will give Keynote Presentations. The Sixteenth Annual ISMB Conference will take place July 19–23 in Toronto, Canada.",2008,PLoS Comput. Biol.
The Long and Thorny Road to Publication in Quality Journals,"Within the “Ten Simple Rules” series in PLoS Computational Biology, Dr. Bourne suggests that for younger investigators it is better to publish one paper in a quality journal rather than having multiple papers in lesser journals [1]. While this is certainly advisable, it can be very difficult. Indeed, for young scientists or, more to the point, for researchers with a short record of publications, it may be almost impossible to make their work and themselves visible to a larger scientific community via higher impact journals. A not-too-small share of “seasoned” scientists will argue without malignity that “we experienced similar or the same” and “good researchers will eventually be recognized.” What they imply is that those who continue to provide good science shall be rewarded later, i.e., their papers will eventually find a home in quality journals, thus yielding better chances that the work will have impact. And yet, a much-cited case study ([2]; cited 264 times as of November 18, 2007, according to http://isiwebofknowledge.com/) may illustrate that the road to publication and recognition can be thorny and long for younger and less-recognized scientists. 
 
Indeed, this “experiment” by Peters and Ceci provided empirical evidence 25 years ago that to get a paper accepted for publication can be very difficult for lesser-known scientists from less-recognized institutions. In this study, 12 psychology articles that had already been published by prestigious scientists from prestigious institutions were resubmitted to the journals that had accepted and printed the papers in the first place. Data presentation remained almost unaltered, but fictitious names and not-well-known institutions replaced the original ones. Only three of the resubmissions were identified as such, and of the other nine manuscripts, eight were rejected, mainly for methodological reasons. The Peters and Ceci study was widely discussed, and one interpretation for their observations was that work from lesser-known researchers may be subjected to a more critical peer review than material submitted by well-known investigators in institutions with a long track record. To exemplify this notion, 1977 Nobel Laureate Rosalyn Yalow commented on the article by Peters and Ceci “. . . . I am in full sympathy with rejecting papers from unknown authors working in unknown institutions. How does one know that the data are not fabricated? . . . on the average, the work of established investigators in good institutions is more likely to have had prior review from competent peers and associates even before reaching the journal” [3]. 
 
Despite this background, Dr. Bourne is right when he suggests that young investigators should aim at publication in quality journals. After all, you can only score high if you try. But be prepared that it takes very good material and perseverance to publish in well-known journals. Be aware, also, that even the highest-quality work may not see publication in high-impact journals, for numerous reasons, with the novice status of the submitting author(s) likely being a primary one. In this vein, both less and more experienced researchers may want to read the following paper for empirical comfort: “Consolation for the scientist: Sometimes it is hard to publish papers that are later highly cited” [4].",2007,PLoS Comput. Biol.
Correction: Specificity and Evolvability in Eukaryotic Protein Interaction Networks,"In PLoS Computational Biology, volume 3, issue 2: doi:10.1371/journal.pcbi.0030025 
 
 
The set of four equations in the Materials and Methods section had three extra minus signs, which were incorrect. The following are the correct equations.",2007,PLoS Comput. Biol.
Correction: Bang-Bang Control of Feeding: Role of Hypothalamic and Satiety Signals,"In PLoS Computational Biology, volume 3, issue 5, in the Introduction section: 10.1371/journal.pcbi.0030097 
 
The word glucagon was misspelled glucagons. This is the correct sentence: 
 
In addition, amylin [18] and glucagon [19], which are secreted from the pancreatic islets during meals, also reduce meal size. 
 
The following sentence was missing a reference (34): 
 
Conversely, bilateral PVN lesions cause a hyperphagic obesity syndrome, whereas bilateral lesioning of the LHA causes anorexia and weight loss [31,33,34]. 
 
The following sentence had an incorrect reference (52 is correct instead of 50): 
 
The bang-bang CINT model also accounts quantitatively for the complexities of meal-intermeal correlations [52] (see also [9,54]).",2007,PLoS Comput. Biol.
In Remembrance: Reinhart Heinrich 1946–2006,"Reinhart Heinrich, former professor at Humboldt University, Berlin, and best-known as a founder of metabolic control theory, died October 23, 2006, at the age of 60. Among his many services to the scientific community, Reinhart was associate editor of PLoS Computational Biology. His far-reaching theoretical work on metabolism, signal transduction, and other cellular processes has made him one of the most influential forerunners of present-day systems biology. 
 
Having been educated as a theoretical physicist at Technical University, Dresden, Reinhart conducted his postdoctoral research in the early 1970s at the Institute of Biochemistry in East Berlin. He could not fail to notice the absence of mathematical theory from cell biology as compared with other natural sciences. Enzyme kinetics was a notable exception. However, how enzymes affect the flux through a metabolic pathway was still discussed using the rather vague term rate-limiting step. Working with Tom Rapoport on mathematical models of glycolysis in red blood cells, Reinhart discovered a precise and general definition of rate limitation in metabolic pathways. The parallel development of metabolic control theory by Henrik Kacser and Jim Burns in Edinburgh shows that the time was ripe for a quantitative understanding of metabolic regulation. Instead of postulating a single rate-limiting step, Reinhart's and Henrik Kacser's theories evaluated the degree of flux control exerted by an individual enzyme in a linear pathway or in a more complex network. The corresponding measure—termed flux control coefficient—turned out to be a truly systemic quantity, depending not only on the kinetic parameters of the enzyme itself but also on those of other enzymes, as well as on the position of the reaction in the network. Several years after its original publication, metabolic control theory became widely absorbed by biochemists. Control coefficients have been measured for many pathways, confirming the theoretical prediction that flux control is frequently shared by several reactions. This finding has recently become of very practical importance for the genetic engineering of large metabolic networks in biotechnology. 
 
The dual approach—modeling concrete cellular processes and, at the same time, searching for general laws—has been a characteristic of Reinhart's work. The areas he worked in were amazingly diverse, including metabolic control, osmoregulation, cell shapes, signal transduction, vesicular transport, protein translation and transport, as well as the population dynamics of malaria parasites. 
 
Perhaps his most favored questions were those of evolution. To understand the kinetic design of enzymes and enzymatic reaction networks, Reinhart strove to rationalize, in mathematical terms, the selective pressures and physico–chemical constraints that these systems were subjected to. Reinhart's work on this topic is full of original insight and makes specific predictions, some of which have begun to be tested successfully in recent years. 
 
Reinhart was author of more than 150 research articles and, together with Stefan Schuster, wrote the book The Regulation of Cellular Systems—already a classic of cell systems biology. In addition to this large body of original work, he was a gifted mentor of young scientists and for more than ten years ran the highly successful interdisciplinary graduate program “Dynamics and Evolution of Cellular Processes” at Humboldt University, Berlin. Reinhart's many talents made him appear as a modern Renaissance man. He played the violin, wrote poetry, and published an autobiographic novel (Jenseits von Babel). Reinhart had a large international circle of friends and collaborators by whom he will be remembered for his kindness, generosity, and contagious humor. 
 
From the Editor: Reinhart was a recent, highly commended, member of the PLoS Computational Biology Editorial Team. Thomas has made it abundantly clear why we asked Reinhart to join us. He was an active and timely contributor and he will be sorely missed by all of us at PLoS as well as by the rest of the scientific community. Our condolences go out to his family. — Philip E. Bourne, Editor-in-Chief",2007,PLoS Comput. Biol.
Correction: Time to Organize the Bioinformatics Resourceome,"Nicola Cannata, Emanuela Merelli, Russ B. Altman DOI: 10.1371/journal.pcbi.0010076 In PLoS Computational Biology, volume 1, issue 7. Two quotes and reference citations were omitted in error: ‘‘More information needs to be in a form that the machine can ‘understand’ rather than simply display’’ —T. Berners-Lee and J. Hendler [7] ‘‘We invite the life science communities to participate together and begin realizing the semantic Web vision.’’ —E. Neumann [12] This correction note may be found online at DOI: 10.1371/journal.pcbi.0020020. Published February 24, 2006 Citation: Cannata N, Merelli E, Altman RB (2006) Correction: Time to organize the bioinformatics resourceome. PLoS Comput Biol 2(2): e20.",2006,PLoS Comput. Biol.
Correction: Folding Very Short Peptides Using Molecular Dynamics,"In PLoS Computational Biology, volume 2, issue 4: DOI: 10.1371/journal.pcbi.0020027 
 
The abbreviation GB/SA had an incorrect definition in the Abstract, Introduction, and Abbreviations list. The correct definition of GB/SA is generalized-Born/surface area. 
 
Table 4 has several rows that did not appear in bold font in the published article, and Table 5 had four rows with incorrect spacing in the published article. Both tables appear correctly below. 
 
 
 
Table 4 
 
Ground Mesostrings of β-Sheet Proteins 
 
 
 
 
 
Table 5 
 
Comparison of the Structural Bias with the Native Structure",2006,PLoS Comput. Biol.
Correction: Ten Simple Rules for Selecting a Postdoctoral Position,"Philip E. Bourne, Iddo Friedberg doi: 10.1371/journal.pcbi.0020121 In PLoS Computational Biology, volume 2, issue 11: In the author affiliation section, Dr. Iddo Friedberg should have been listed as a research scientist (instead of as a research assistant). In the section called Rule 1: Select a Position That Excites You, the sentence, ‘‘Just because the mentor is excited about the project does not mean that you will be six months into it.’’, had the two words ‘‘that’’ and ‘‘you’’ switched. This correction note may be found online at doi:10.1371/journal.pcbi.0020181. Published December 29, 2006. Citation: (2006) Correction: Ten simple rules for selecting a postdoctoral position. PLoS Comput Biol 2(12): e181. doi:10.1371/ journal.pcbi.0020181",2006,PLoS Comput. Biol.
Inferring Regulatory Networks from Experimental Morphological Phenotypes: A Computational Method Reverse-Engineers Planarian Regeneration,"Transformative applications in biomedicine require the discovery of complex regulatory networks that explain the development and regeneration of anatomical structures, and reveal what external signals will trigger desired changes of large-scale pattern. Despite recent advances in bioinformatics, extracting mechanistic pathway models from experimental morphological data is a key open challenge that has resisted automation. The fundamental difficulty of manually predicting emergent behavior of even simple networks has limited the models invented by human scientists to pathway diagrams that show necessary subunit interactions but do not reveal the dynamics that are sufficient for complex, self-regulating pattern to emerge. To finally bridge the gap between high-resolution genetic data and the ability to understand and control patterning, it is critical to develop computational tools to efficiently extract regulatory pathways from the resultant experimental shape phenotypes. For example, planarian regeneration has been studied for over a century, but despite increasing insight into the pathways that control its stem cells, no constructive, mechanistic model has yet been found by human scientists that explains more than one or two key features of its remarkable ability to regenerate its correct anatomical pattern after drastic perturbations. We present a method to infer the molecular products, topology, and spatial and temporal non-linear dynamics of regulatory networks recapitulating in silico the rich dataset of morphological phenotypes resulting from genetic, surgical, and pharmacological experiments. We demonstrated our approach by inferring complete regulatory networks explaining the outcomes of the main functional regeneration experiments in the planarian literature; By analyzing all the datasets together, our system inferred the first systems-biology comprehensive dynamical model explaining patterning in planarian regeneration. This method provides an automated, highly generalizable framework for identifying the underlying control mechanisms responsible for the dynamic regulation of growth and form.",2015,PLoS Comput. Biol.
Ten Simple Rules for Reducing Overoptimistic Reporting in Methodological Computational Research,"In most scientific fields, and in biomedical research in particular, there have long been many discussions on how to improve research practices and methods. The trend has increased in recent years, as illustrated by the series on “reducing waste,” published in The Lancet in January 2014 [1], or by the recent essay by John Ioannidis on how to make published results more true [2], which echoes his earlier provocative paper entitled “Why most published research findings are false” [3]. One of the important aspects underlying these discussions is that biomedical literature is most often overoptimistic with respect to, for example, the superiority of a new therapy or the strength of association between a risk factor and an outcome. Published results appear more significant, more spectacular, or sometimes more intuitive—in a word, more “satisfactory”—to authors and readers than they actually would if they reflected the truth. Causes of this problem are diverse, numerous, and interrelated. The effects of “fishing for significance” strategies or selective/incomplete reporting are exacerbated by design issues (e.g., small sample sizes, many investigated features) [3] or publication bias [4], to cite only a few of the factors at work. 
 
Research and guidelines on how to reduce overoptimistic reporting in the context of computational research, including computational biology as an important special case, however, are surprisingly scarce. Many methodological articles published in computational literature report the (vastly) superior performance of new methods [5], too often in general terms and—directly or indirectly—implying that the presented positive results are generalizable to other settings. Such overoptimistic reporting confuses readers, makes literature less credible and more difficult to interpret, and might even ultimately lead to a waste of resources in some cases. Here I take advantage of the popular “ten-simple-rules” format [6] to address the problem of overoptimistic reporting in methodological computational biology research, that is papers—termed “methodological papers” here—devoted primarily to the development and testing of new computational methods (intended to be used by other researchers on other data in the future) rather than to the biological question itself or the specific dataset at hand.",2015,PLoS Comput. Biol.
Systems Biology-Based Investigation of Cellular Antiviral Drug Targets Identified by Gene-Trap Insertional Mutagenesis,"Viruses require host cellular factors for successful replication. A comprehensive systems-level investigation of the virus-host interactome is critical for understanding the roles of host factors with the end goal of discovering new druggable antiviral targets. Gene-trap insertional mutagenesis is a high-throughput forward genetics approach to randomly disrupt (trap) host genes and discover host genes that are essential for viral replication, but not for host cell survival. In this study, we used libraries of randomly mutagenized cells to discover cellular genes that are essential for the replication of 10 distinct cytotoxic mammalian viruses, 1 gram-negative bacterium, and 5 toxins. We herein reported 712 candidate cellular genes, characterizing distinct topological network and evolutionary signatures, and occupying central hubs in the human interactome. Cell cycle phase-specific network analysis showed that host cell cycle programs played critical roles during viral replication (e.g. MYC and TAF4 regulating G0/1 phase). Moreover, the viral perturbation of host cellular networks reflected disease etiology in that host genes (e.g. CTCF, RHOA, and CDKN1B) identified were frequently essential and significantly associated with Mendelian and orphan diseases, or somatic mutations in cancer. Computational drug repositioning framework via incorporating drug-gene signatures from the Connectivity Map into the virus-host interactome identified 110 putative druggable antiviral targets and prioritized several existing drugs (e.g. ajmaline) that may be potential for antiviral indication (e.g. anti-Ebola). In summary, this work provides a powerful methodology with a tight integration of gene-trap insertional mutagenesis testing and systems biology to identify new antiviral targets and drugs for the development of broadly acting and targeted clinical antiviral therapeutics.",2016,PLoS Comput. Biol.
A Systematic Computational Analysis of Biosynthetic Gene Cluster Evolution: Lessons for Engineering Biosynthesis,"Bacterial secondary metabolites are widely used as antibiotics, anticancer drugs, insecticides and food additives. Attempts to engineer their biosynthetic gene clusters (BGCs) to produce unnatural metabolites with improved properties are often frustrated by the unpredictability and complexity of the enzymes that synthesize these molecules, suggesting that genetic changes within BGCs are limited by specific constraints. Here, by performing a systematic computational analysis of BGC evolution, we derive evidence for three findings that shed light on the ways in which, despite these constraints, nature successfully invents new molecules: 1) BGCs for complex molecules often evolve through the successive merger of smaller sub-clusters, which function as independent evolutionary entities. 2) An important subset of polyketide synthases and nonribosomal peptide synthetases evolve by concerted evolution, which generates sets of sequence-homogenized domains that may hold promise for engineering efforts since they exhibit a high degree of functional interoperability, 3) Individual BGC families evolve in distinct ways, suggesting that design strategies should take into account family-specific functional constraints. These findings suggest novel strategies for using synthetic biology to rationally engineer biosynthetic pathways.",2014,PLoS Comput. Biol.
OpenSim: Simulating musculoskeletal dynamics and neuromuscular control to study human and animal movement,"Movement is fundamental to human and animal life, emerging through interaction of complex neural, muscular, and skeletal systems. Study of movement draws from and contributes to diverse fields, including biology, neuroscience, mechanics, and robotics. OpenSim unites methods from these fields to create fast and accurate simulations of movement, enabling two fundamental tasks. First, the software can calculate variables that are difficult to measure experimentally, such as the forces generated by muscles and the stretch and recoil of tendons during movement. Second, OpenSim can predict novel movements from models of motor control, such as kinematic adaptations of human gait during loaded or inclined walking. Changes in musculoskeletal dynamics following surgery or due to human–device interaction can also be simulated; these simulations have played a vital role in several applications, including the design of implantable mechanical devices to improve human grasping in individuals with paralysis. OpenSim is an extensible and user-friendly software package built on decades of knowledge about computational modeling and simulation of biomechanical systems. OpenSim’s design enables computational scientists to create new state-of-the-art software tools and empowers others to use these tools in research and clinical applications. OpenSim supports a large and growing community of biomechanics and rehabilitation researchers, facilitating exchange of models and simulations for reproducing and extending discoveries. Examples, tutorials, documentation, and an active user forum support this community. The OpenSim software is covered by the Apache License 2.0, which permits its use for any purpose including both nonprofit and commercial applications. The source code is freely and anonymously accessible on GitHub, where the community is welcomed to make contributions. Platform-specific installers of OpenSim include a GUI and are available on simtk.org.",2018,PLoS Comput. Biol.
"What Is Stochastic Resonance? Definitions, Misconceptions, Debates, and Its Relevance to Biology","Stochastic resonance is said to be observed when increases in levels of unpredictable fluctuations—e.g., random noise—cause an increase in a metric of the quality of signal transmission or detection performance, rather than a decrease. This counterintuitive effect relies on system nonlinearities and on some parameter ranges being “suboptimal”. Stochastic resonance has been observed, quantified, and described in a plethora of physical and biological systems, including neurons. Being a topic of widespread multidisciplinary interest, the definition of stochastic resonance has evolved significantly over the last decade or so, leading to a number of debates, misunderstandings, and controversies. Perhaps the most important debate is whether the brain has evolved to utilize random noise in vivo, as part of the “neural code”. Surprisingly, this debate has been for the most part ignored by neuroscientists, despite much indirect evidence of a positive role for noise in the brain. We explore some of the reasons for this and argue why it would be more surprising if the brain did not exploit randomness provided by noise—via stochastic resonance or otherwise—than if it did. We also challenge neuroscientists and biologists, both computational and experimental, to embrace a very broad definition of stochastic resonance in terms of signal-processing “noise benefits”, and to devise experiments aimed at verifying that random variability can play a functional role in the brain, nervous system, or other areas of biology.",2009,PLoS Comput. Biol.
Detailed Simulations of Cell Biology with Smoldyn 2.1,"Most cellular processes depend on intracellular locations and random collisions of individual protein molecules. To model these processes, we developed algorithms to simulate the diffusion, membrane interactions, and reactions of individual molecules, and implemented these in the Smoldyn program. Compared to the popular MCell and ChemCell simulators, we found that Smoldyn was in many cases more accurate, more computationally efficient, and easier to use. Using Smoldyn, we modeled pheromone response system signaling among yeast cells of opposite mating type. This model showed that secreted Bar1 protease might help a cell identify the fittest mating partner by sharpening the pheromone concentration gradient. This model involved about 200,000 protein molecules, about 7000 cubic microns of volume, and about 75 minutes of simulated time; it took about 10 hours to run. Over the next several years, as faster computers become available, Smoldyn will allow researchers to model and explore systems the size of entire bacterial and smaller eukaryotic cells.",2010,PLoS Comput. Biol.
"rDock: A Fast, Versatile and Open Source Program for Docking Ligands to Proteins and Nucleic Acids","Identification of chemical compounds with specific biological activities is an important step in both chemical biology and drug discovery. When the structure of the intended target is available, one approach is to use molecular docking programs to assess the chemical complementarity of small molecules with the target; such calculations provide a qualitative measure of affinity that can be used in virtual screening (VS) to rank order a list of compounds according to their potential to be active. rDock is a molecular docking program developed at Vernalis for high-throughput VS (HTVS) applications. Evolved from RiboDock, the program can be used against proteins and nucleic acids, is designed to be computationally very efficient and allows the user to incorporate additional constraints and information as a bias to guide docking. This article provides an overview of the program structure and features and compares rDock to two reference programs, AutoDock Vina (open source) and Schrödinger's Glide (commercial). In terms of computational speed for VS, rDock is faster than Vina and comparable to Glide. For binding mode prediction, rDock and Vina are superior to Glide. The VS performance of rDock is significantly better than Vina, but inferior to Glide for most systems unless pharmacophore constraints are used; in that case rDock and Glide are of equal performance. The program is released under the Lesser General Public License and is freely available for download, together with the manuals, example files and the complete test sets, at http://rdock.sourceforge.net/",2014,PLoS Comput. Biol.
Approximate Bayesian Computation,"Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology).",2013,PLoS Comput. Biol.
Translational Systems Biology of Inflammation,"Inflammation is a complex, multi-scale biologic response to stress that is also required for repair and regeneration after injury. Despite the repository of detailed data about the cellular and molecular processes involved in inflammation, including some understanding of its pathophysiology, little progress has been made in treating the severe inflammatory syndrome of sepsis. To address the gap between basic science knowledge and therapy for sepsis, a community of biologists and physicians is using systems biology approaches in hopes of yielding basic insights into the biology of inflammation. “Systems biology” is a discipline that combines experimental discovery with mathematical modeling to aid in the understanding of the dynamic global organization and function of a biologic system (cell to organ to organism). We propose the term translational systems biology for the application of similar tools and engineering principles to biologic systems with the primary goal of optimizing clinical practice. We describe the efforts to use translational systems biology to develop an integrated framework to gain insight into the problem of acute inflammation. Progress in understanding inflammation using translational systems biology tools highlights the promise of this multidisciplinary field. Future advances in understanding complex medical problems are highly dependent on methodological advances and integration of the computational systems biology community with biologists and clinicians.",2008,PLoS Comput. Biol.
Direct Solution of the Chemical Master Equation Using Quantized Tensor Trains,"The Chemical Master Equation (CME) is a cornerstone of stochastic analysis and simulation of models of biochemical reaction networks. Yet direct solutions of the CME have remained elusive. Although several approaches overcome the infinite dimensional nature of the CME through projections or other means, a common feature of proposed approaches is their susceptibility to the curse of dimensionality, i.e. the exponential growth in memory and computational requirements in the number of problem dimensions. We present a novel approach that has the potential to “lift” this curse of dimensionality. The approach is based on the use of the recently proposed Quantized Tensor Train (QTT) formatted numerical linear algebra for the low parametric, numerical representation of tensors. The QTT decomposition admits both, algorithms for basic tensor arithmetics with complexity scaling linearly in the dimension (number of species) and sub-linearly in the mode size (maximum copy number), and a numerical tensor rounding procedure which is stable and quasi-optimal. We show how the CME can be represented in QTT format, then use the exponentially-converging -discontinuous Galerkin discretization in time to reduce the CME evolution problem to a set of QTT-structured linear equations to be solved at each time step using an algorithm based on Density Matrix Renormalization Group (DMRG) methods from quantum chemistry. Our method automatically adapts the “basis” of the solution at every time step guaranteeing that it is large enough to capture the dynamics of interest but no larger than necessary, as this would increase the computational complexity. Our approach is demonstrated by applying it to three different examples from systems biology: independent birth-death process, an example of enzymatic futile cycle, and a stochastic switch model. The numerical results on these examples demonstrate that the proposed QTT method achieves dramatic speedups and several orders of magnitude storage savings over direct approaches.",2014,PLoS Comput. Biol.
Stochastic Simulation of Biomolecular Networks in Dynamic Environments,"Simulation of biomolecular networks is now indispensable for studying biological systems, from small reaction networks to large ensembles of cells. Here we present a novel approach for stochastic simulation of networks embedded in the dynamic environment of the cell and its surroundings. We thus sample trajectories of the stochastic process described by the chemical master equation with time-varying propensities. A comparative analysis shows that existing approaches can either fail dramatically, or else can impose impractical computational burdens due to numerical integration of reaction propensities, especially when cell ensembles are studied. Here we introduce the Extrande method which, given a simulated time course of dynamic network inputs, provides a conditionally exact and several orders-of-magnitude faster simulation solution. The new approach makes it feasible to demonstrate—using decision-making by a large population of quorum sensing bacteria—that robustness to fluctuations from upstream signaling places strong constraints on the design of networks determining cell fate. Our approach has the potential to significantly advance both understanding of molecular systems biology and design of synthetic circuits.",2015,PLoS Comput. Biol.
A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation,"While many models of biological object recognition share a common set of “broad-stroke” properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model—e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct “parts” have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3's IBM Cell Processor). In analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.",2009,PLoS Comput. Biol.
Canalization of Gene Expression and Domain Shifts in the Drosophila Blastoderm by Dynamical Attractors,"The variation in the expression patterns of the gap genes in the blastoderm of the fruit fly Drosophila melanogaster reduces over time as a result of cross regulation between these genes, a fact that we have demonstrated in an accompanying article in PLoS Biology (see Manu et al., doi:10.1371/journal.pbio.1000049). This biologically essential process is an example of the phenomenon known as canalization. It has been suggested that the developmental trajectory of a wild-type organism is inherently stable, and that canalization is a manifestation of this property. Although the role of gap genes in the canalization process was established by correctly predicting the response of the system to particular perturbations, the stability of the developmental trajectory remains to be investigated. For many years, it has been speculated that stability against perturbations during development can be described by dynamical systems having attracting sets that drive reductions of volume in phase space. In this paper, we show that both the reduction in variability of gap gene expression as well as shifts in the position of posterior gap gene domains are the result of the actions of attractors in the gap gene dynamical system. Two biologically distinct dynamical regions exist in the early embryo, separated by a bifurcation at 53% egg length. In the anterior region, reduction in variation occurs because of stability induced by point attractors, while in the posterior, the stability of the developmental trajectory arises from a one-dimensional attracting manifold. This manifold also controls a previously characterized anterior shift of posterior region gap domains. Our analysis shows that the complex phenomena of canalization and pattern formation in the Drosophila blastoderm can be understood in terms of the qualitative features of the dynamical system. The result confirms the idea that attractors are important for developmental stability and shows a richer variety of dynamical attractors in developmental systems than has been previously recognized.",2009,PLoS Comput. Biol.
Getting Started in Computational Immunology,"The immune system acts across multiple scales involving complex interactions and feedback, from somatic modifications of DNA to the systemic inflammatory reaction. Computational modeling provides a framework to integrate observational data collected from multiple modes of experimentation and insight into the immune response in health and disease. This Message attempts to illustrate how different computational methods have been integrated with experimental observations to study an immunological question from multiple perspectives by focusing on a very particular, though fundamental, component of adaptive immunity: B cells and affinity maturation (Figure 1). B cells bind foreign antigens through their Immunoglobulin (Ig) receptor. Affinity maturation is the process by which B cell receptors that initially bind antigen with low affinity are modified through cycles of somatic mutation and affinity-dependent selection to produce high-affinity memory and plasma cells. How this process can reliably generate orders of magnitude increases in affinity over a period of weeks is one of the many questions where computational modeling has made important contributions (for example, the cyclic re-entry model [1]). Yet, even the seemingly straightforward matter of detecting antigen-driven selection remains controversial, and such fundamental questions as whether increased proliferation or decreased death drives the preferential expansion of higher-affinity B cell mutants remain unanswered. A good biological introduction to the immune system is available on the NIH website [2], while more detailed information can be found in any number of textbooks [3]. An animation by Julian Kirk-Elleker provides a visual introduction to the affinity maturation process (http://web.mac.com/patrickwlee/Antibody-affinity_maturation/Movie.html). The kinds of computational techniques described here have been widely applied in other areas of immunology, including the innate response [4],[5], viral dynamics [6], and immune memory [7]. A classic introduction to computational immunology geared to the more mathematically inclined was written by Perelson and Weisbuch [8]. The rapidly expanding area of immunoinformatics was covered in a recent issue of PLoS Computational Biology [9], and several other applications were explored in a 2007 volume of Immunological Reviews (216) devoted to quantitative modeling of immune responses. 
 
 
 
Figure 1 
 
A wide range of experimental techniques are used in combination with computational modeling to probe the process of affinity maturation at multiple scales (from DNA to tissue).",2008,PLoS Comput. Biol.
Ten simple rules for running and managing virtual internships,"The importance of managing projects virtually and effectively has increased over time. Today, many research groups in the computational and natural sciences have become more international. In addition, many students are participating in virtual internships and research projects. This trend also extends to how students are supervised on various projects—often remotely and by faculty members or organizational supervisors outside their own institutions. In the following paper, the authors outline ten rules for any faculty interested in successfully running virtual projects with students from other institutions than their own. These recommendations build on existing guidance related to preparing for and hosting traditional on-site internships [1–3]. Virtual internships are computer-mediated internships where the intern works for an organization, usually employers, remotely. This means that these virtual interns and the organization are often located in different cities, countries, and even time zones. Many interns complete such internships to gain work experience or complete projects for academic credit [4]. Due to the ease of locating suitable talent for research projects, such internships are also of interest to many academics who seek suitable and motivated talent for research projects. Faculty members can play a similar supervisory role as the managers in organizations. In the following section, we outline ten rules that can guide research supervisors in higher education and research institutions who are interested in running such virtual internships. These rules are relevant to both virtual and traditional internships that involve a dispersed team collaborating on a shared project. Nonetheless, the nature of virtual internships implies that project success is contingent on some aspects that are usually easier to track in traditional internships. Open communication, progress monitoring, proactive tool adoption, learning, and documentation are particularly critical in virtual settings to create transparency and facilitate project success. While some of these rules were created based on previous work we reference, none of the previous virtual internships focused on internships in research facilities. We believe that these rules can be readily applied to research projects focusing on computational biological analysis in multiple research disciplines as today’s technical infrastructure now allows labs and researchers to collaborate virtually on joint projects, using shared online resources and analytical tools. In order to highlight the relevance of our rules, we included feedback and observations from three female and two male students who completed virtual projects in bioinformatics. Virtual internships are known in various fields; however, to our knowledge, this is the first time that virtual internships have been applied to the field of bioinformatics. In contrast to PLOS COMPUTATIONAL BIOLOGY",2021,PLoS Comput. Biol.
Reconciliation of Genome-Scale Metabolic Reconstructions for Comparative Systems Analysis,"In the past decade, over 50 genome-scale metabolic reconstructions have been built for a variety of single- and multi- cellular organisms. These reconstructions have enabled a host of computational methods to be leveraged for systems-analysis of metabolism, leading to greater understanding of observed phenotypes. These methods have been sparsely applied to comparisons between multiple organisms, however, due mainly to the existence of differences between reconstructions that are inherited from the respective reconstruction processes of the organisms to be compared. To circumvent this obstacle, we developed a novel process, termed metabolic network reconciliation, whereby non-biological differences are removed from genome-scale reconstructions while keeping the reconstructions as true as possible to the underlying biological data on which they are based. This process was applied to two organisms of great importance to disease and biotechnological applications, Pseudomonas aeruginosa and Pseudomonas putida, respectively. The result is a pair of revised genome-scale reconstructions for these organisms that can be analyzed at a systems level with confidence that differences are indicative of true biological differences (to the degree that is currently known), rather than artifacts of the reconstruction process. The reconstructions were re-validated with various experimental data after reconciliation. With the reconciled and validated reconstructions, we performed a genome-wide comparison of metabolic flexibility between P. aeruginosa and P. putida that generated significant new insight into the underlying biology of these important organisms. Through this work, we provide a novel methodology for reconciling models, present new genome-scale reconstructions of P. aeruginosa and P. putida that can be directly compared at a network level, and perform a network-wide comparison of the two species. These reconstructions provide fresh insights into the metabolic similarities and differences between these important Pseudomonads, and pave the way towards full comparative analysis of genome-scale metabolic reconstructions of multiple species.",2011,PLoS Comput. Biol.
Drug Off-Target Effects Predicted Using Structural Analysis in the Context of a Metabolic Network Model,Recent advances in structural bioinformatics have enabled the prediction of protein-drug off-targets based on their ligand binding sites. Concurrent developments in systems biology allow for prediction of the functional effects of system perturbations using large-scale network models. Integration of these two capabilities provides a framework for evaluating metabolic drug response phenotypes in silico. This combined approach was applied to investigate the hypertensive side effect of the cholesteryl ester transfer protein inhibitor torcetrapib in the context of human renal function. A metabolic kidney model was generated in which to simulate drug treatment. Causal drug off-targets were predicted that have previously been observed to impact renal function in gene-deficient patients and may play a role in the adverse side effects observed in clinical trials. Genetic risk factors for drug treatment were also predicted that correspond to both characterized and unknown renal metabolic disorders as well as cryptic genetic deficiencies that are not expected to exhibit a renal disorder phenotype except under drug treatment. This study represents a novel integration of structural and systems biology and a first step towards computational systems medicine. The methodology introduced herein has important implications for drug development and personalized medicine.,2010,PLoS Comput. Biol.
Critical Role of Transient Activity of MT1-MMP for ECM Degradation in Invadopodia,"Focal degradation of extracellular matrix (ECM) is the first step in the invasion of cancer cells. MT1-MMP is a potent membrane proteinase employed by aggressive cancer cells. In our previous study, we reported that MT1-MMP was preferentially located at membrane protrusions called invadopodia, where MT1-MMP underwent quick turnover. Our computer simulation and experiments showed that this quick turnover was essential for the degradation of ECM at invadopodia (Hoshino, D., et al., (2012) PLoS Comp. Biol., 8: e1002479). Here we report on characterization and analysis of the ECM-degrading activity of MT1-MMP, aiming at elucidating a possible reason for its repetitive insertion in the ECM degradation. First, in our computational model, we found a very narrow transient peak in the activity of MT1-MMP followed by steady state activity. This transient activity was due to the inhibition by TIMP-2, and the steady state activity of MT1-MMP decreased dramatically at higher TIMP-2 concentrations. Second, we evaluated the role of the narrow transient activity in the ECM degradation. When the transient activity was forcibly suppressed in computer simulations, the ECM degradation was heavily suppressed, indicating the essential role of this transient peak in the ECM degradation. Third, we compared continuous and pulsatile turnover of MT1-MMP in the ECM degradation at invadopodia. The pulsatile insertion showed basically consistent results with the continuous insertion in the ECM degradation, and the ECM degrading efficacy depended heavily on the transient activity of MT1-MMP in both models. Unexpectedly, however, low-frequency/high-concentration insertion of MT1-MMP was more effective in ECM degradation than high-frequency/low-concentration pulsatile insertion even if the time-averaged amount of inserted MT1-MMP was the same. The present analysis and characterization of ECM degradation by MT1-MMP together with our previous report indicate a dynamic nature of MT1-MMP at invadopodia and the importance of its transient peak in the degradation of the ECM.",2013,PLoS Comput. Biol.
MinePath: Mining for Phenotype Differential Sub-paths in Molecular Pathways,"Pathway analysis methodologies couple traditional gene expression analysis with knowledge encoded in established molecular pathway networks, offering a promising approach towards the biological interpretation of phenotype differentiating genes. Early pathway analysis methodologies, named as gene set analysis (GSA), view pathways just as plain lists of genes without taking into account either the underlying pathway network topology or the involved gene regulatory relations. These approaches, even if they achieve computational efficiency and simplicity, consider pathways that involve the same genes as equivalent in terms of their gene enrichment characteristics. Most recent pathway analysis approaches take into account the underlying gene regulatory relations by examining their consistency with gene expression profiles and computing a score for each profile. Even with this approach, assessing and scoring single-relations limits the ability to reveal key gene regulation mechanisms hidden in longer pathway sub-paths. We introduce MinePath, a pathway analysis methodology that addresses and overcomes the aforementioned problems. MinePath facilitates the decomposition of pathways into their constituent sub-paths. Decomposition leads to the transformation of single-relations to complex regulation sub-paths. Regulation sub-paths are then matched with gene expression sample profiles in order to evaluate their functional status and to assess phenotype differential power. Assessment of differential power supports the identification of the most discriminant profiles. In addition, MinePath assess the significance of the pathways as a whole, ranking them by their p-values. Comparison results with state-of-the-art pathway analysis systems are indicative for the soundness and reliability of the MinePath approach. In contrast with many pathway analysis tools, MinePath is a web-based system (www.minepath.org) offering dynamic and rich pathway visualization functionality, with the unique characteristic to color regulatory relations between genes and reveal their phenotype inclination. This unique characteristic makes MinePath a valuable tool for in silico molecular biology experimentation as it serves the biomedical researchers’ exploratory needs to reveal and interpret the regulatory mechanisms that underlie and putatively govern the expression of target phenotypes.",2016,PLoS Comput. Biol.
Ten simple rules for creating a sense of belonging in your research group,"Across academic fields and disciplines, we’re hearing more and more buzz around the importance of sense of belonging. However, two important elements are missing from these conversations. First, we need to have a grounded understanding of what sense of belonging is, and how it can be cultivated. Secondly, these conversations tend to apply to a typical classroom context but what about the sense of belonging in research and lab spaces? Sense of belonging is generally defined as the experience of positive personal relationships with others in a given environment [1]. It is an important predictor of well-being [2] and retention in science, technology, engineering, and mathematics (STEM) [3]. There are many benefits associated with developing a strong sense of belonging including improvements in academic performance, mental health, self-esteem, sense of purpose, and connectedness [4,5,6,7]. On the other hand, there are many significant consequences of having a low sense of belonging such as increased risk of stress, anxiety, depression, health problems, feelings of loneliness, rejection, and low self-esteem [2,5,8,9]. Creating environments that are conducive to well-being and belonging not only serve the principal investigator (PI) and the trainee, but also has the potential to increase research integrity, increase and retain the diversity among scientific leaders, and transform academia. Research labs, whether wet or dry, academic or field-based, are important learning environments where many graduate students, trainees, technicians, and postdocs spend a significant amount of time. Yet, despite the amount of time spent in the lab, trainees do not always feel that they belong there [10]. This paper addresses this lack of belonging by proposing ten simple rules for creating a sense of belonging in your research group. These rules (S1 Fig) are grounded in evidence-based practices that have increased the retention of systemically and historically excluded groups [11,12,13,14,15], and improved the mental health crisis in graduate and postdoc education, as belonging is strongly tied to mental health and well-being [16,17,18,19,20]. Because an individual’s sense of belonging can change and evolve over time, PIs and senior lab members can use these rules to establish and maintain a culture where individuals feel valued, heard, and appreciated. Sense of belonging impacts workplace performance [21]; therefore, having a strong sense of belonging within your research group positions lab members to not only have a higher quality of life, but also produce higher-quality research and successfully progress in their research careers. This model can then be emulated by emerging scientists who will become leaders and effective mentors themselves, allowing the cycle to repeat. In this article, we draw from research on sense of belonging and educational psychology to inform best practices for fostering a sense of belonging in the lab. This paper is informed by our experiences as former lab members (graduate students/postdocs), current PIs PLOS COMPUTATIONAL BIOLOGY",2022,PLoS Comput. Biol.
Ten simple rules for principled simulation modelling,"ed counterpart (Fig 2 in [13]). However, it is an updated version of this second map that is given to millions of tourists travelling across London every year. The inclusion of only important information, abstraction, and omission of extraneous geographical detail has made it one of the most famously useful graphics in history. Models of complex systems should be the same—as detailed as needed but no more detailed than that. So, if you shouldn’t model the system you know in complete detail, what should you model instead? This depends to a great extent on the type of model that you have decided to make (Rule 1). Assuming that some mechanistic insight is the aim, based on knowledge of the system and precise formulation of the research question, a good strategy is to identify putative interactions or mechanisms that could underlie the behaviour or phenomenon under investigation and model those mechanisms. In other words, it is sensible to propose an engine that could be driving the phenomenon of interest and then make characterising and investigating that engine the central focus of the model. Anything in the system irrelevant to that engine is irrelevant to the model. Part of the art of modelling is identifying putative mechanisms and ensuring that the logical thread between mechanism and behaviour is unbroken. Rule 3: Be rigorous, and allow the time to be rigorous Perhaps this is an obvious rule—cience and rigour usually go hand in hand. Successful modelling requires considerable attention to mathematical and computational detail. It is vital that all concepts and all code used in a simulation model are clear to the author and rigorously tested—from their construction to their downstream implications (see Rule 6). Using convenient mathematical or statistical concepts with little or no knowledge about their inner workings will likely lead to their misuse and maybe to serious fundamental errors. Similarly, it can be dangerous to use, for example, software packages or useful-looking chunks of code as untested black boxes (but see Rule 6). The assumptions of the methods implemented in the code, and the implementation itself, must be clear and consistent with the assumptions and aims of your model. Alongside the mathematical and technical details of each part of the model, careful consideration should be given to the order in which these parts or processes are executed and what effect that ordering (or scheduling) has on the output. All of this is to say that modelling decisions need careful thought and, as with empirical data analysis, designing a model that describes your system in adequate (but not excessive) detail, and deriving robust inferences from it, can take significant time. This is time which needs to be budgeted for in any simulation modelling project from the outset. Rule 4: Have a plan for analysis The aim of this rule is to help you to avoid a surprisingly common and deeply frustrating situation: creating a monster simulation with so many moving parts that it just can’t be analysed meaningfully—a situation where you could say to yourself something like “I’ve finished coding this simulation, but how on earth can I analyse so many interactions and so many outputs?” Reaching this point probably means that something has gone wrong in the implementation of Rules 1 or 2—or that the model has been coded without a good plan for its analysis. So, perhaps you know the question you want to answer, you know or have guessed the relevant aspects of the system, but you didn’t plan how the model can answer the question. In general, it is important to have an idea of the analysis steps necessary to answer aspects of the central question. These plans are flexible and develop throughout a project, of course, but an initial plan is crucial. Your exact plan for analysis will depend on your aim and the kind of model you have constructed. For example, if you are interested in unravelling mechanistic relationships, you might use simulated data to explore the workings of the model and to analyse exactly how PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1009917 March 31, 2022 3 / 8 different parameters or processes affect the outcome. To do this successfully, the number of parameters and processes should be manageable (see Rule 2), and parameter explorations should be principled (see Rule 8). Or, if you are interested in fitting your model to observed data, you may need to consider exactly how the model output maps to existing and future data and identify appropriate statistical techniques to robustly test between models and estimate parameters. Here, it will be important to be aware of the limitations and assumptions of the statistical techniques in addition to those of the model itself (see Rule 3). Rule 5: Be kind to your future self (and your readers), and minimise opportunities for errors You will need to revisit your code, sometimes many months after the analysis has been done and the paper has been written. This can either be a difficult exercise for your future self or a simple matter of glancing at your structured, documented, and organised code. Of course, it is best to aim for the latter. This means that code needs to be well structured and well commented so that you are able to easily reconstruct what has been done. To this end, this rule advises a number of good coding practices that are particularly important for simulation models, which can become large and unwieldy projects very quickly. All of these practices will benefit the readers of your model as much as they benefit your future self. First, the names of variables, functions, and parameters should be intuitive, consistent, and immediately intelligible. This makes the code more readable and minimises unnecessary comments. Similarly, adopting consistent formatting conventions (such as indentation and other whitespace), much like punctuation in a sentence, improves code readability. Commenting should be done with both your future self and your future readers in mind. For larger-scale multiresearcher and multiyear projects, more detailed documentation should be created and developed in tandem with the code. As a project grows, it becomes difficult to keep track of the interactions and dependencies between the different parts of the code. This complexity can be made more manageable by giving your code a well-defined and modular structure, meaning that sections of code should be divided into logically related units and the interfaces between these units of code should be well defined. Different programming languages offer different tools to achieve this, and the following advice applies equally to all of them. We’ll use the concept of functions as an example because this is the most fundamental and most commonly used. The aim here is to identify units of self-contained logic and create functions that encapsulate this logic. Functions should have a single clear purpose so, if you find you’re writing a function which performs multiple conceptual actions, consider breaking it up into 2 or more functions. Encapsulating logic in this way creates higher-order building blocks that can be reused and avoid code duplication. This reduces the opportunity for errors, not only because there will be less code to begin with, but also because each logical unit can be easily tested individually. In fact, it is good practice to test functions as you write them—this helps to avoid bugs in “low-level” functions that often manifest as difficult to diagnose problems downstream. The ubiquitously useful coding mantra “fail early and fail loudly” posits that it is better to raise an error as soon as it could feasibly be detected. So, for example, it is best to have functions check that their inputs are valid rather than allowing them to blindly compute and pass nonsensical output downstream. Despite taking these steps to manage the complexity of your project, you will write bugs. Everyone does and it is unavoidable. But, if you have written modular, well-structured code, it will be much easier to test the code, find the bugs, and squash them. PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1009917 March 31, 2022 4 / 8 With a similar aim in mind, it is now common practice and explicitly required by some journals that model code is published alongside the paper it generates. This has a number of important benefits. It facilitates open and reproducible science. It also benefits you as a modeller. Bugs and coding issues can be identified by reviewers and readers who go through the code. This can be an important source of error checking. Finally, but equally importantly, you will need to test that the (now hopefully bug free) code actually does what it is supposed to in terms of the model’s dynamics. The full code, and each constituent part, should be put through its paces using simple test cases for which you have either good intuitions or solid analytical expectations to which the model outcomes should conform. Rule 6: Write practical code Practical code doesn’t have to be perfect, fully optimised, or even beautifully written, but it is fit for purpose, well tested, and clearly documented so that others (and you) can have confidence in it (see Rule 5). Practical coding also means using your coding time efficiently. Investing some time in learning to use the debugging tools available to your programming language will quickly pay for itself in time saved identifying and fixing bugs more quickly. There are a great many well-tested and already optimised scientific libraries/packages, and there is usually nothing to be gained by reimplementing functionality that already exists. Making use of good quality libraries will speed up development and reduce your opportunities to write bugs (see Rule 5), although it is critical to ensure that the methods used are appropriate for your context (see the note abou",2022,PLoS Comput. Biol.
Key Role of Local Regulation in Chemosensing Revealed by a New Molecular Interaction-Based Modeling Method,"The signaling network underlying eukaryotic chemosensing is a complex combination of receptor-mediated transmembrane signals, lipid modifications, protein translocations, and differential activation/deactivation of membrane-bound and cytosolic components. As such, it provides particularly interesting challenges for a combined computational and experimental analysis. We developed a novel detailed molecular signaling model that, when used to simulate the response to the attractant cyclic adenosine monophosphate (cAMP), made nontrivial predictions about Dictyostelium chemosensing. These predictions, including the unexpected existence of spatially asymmetrical, multiphasic, cyclic adenosine monophosphate–induced PTEN translocation and phosphatidylinositol-(3,4,5)P3 generation, were experimentally verified by quantitative single-cell microscopy leading us to propose significant modifications to the current standard model for chemoattractant-induced biochemical polarization in this organism. Key to this successful modeling effort was the use of “Simmune,” a new software package that supports the facile development and testing of detailed computational representations of cellular behavior. An intuitive interface allows user definition of complex signaling networks based on the definition of specific molecular binding site interactions and the subcellular localization of molecules. It automatically translates such inputs into spatially resolved simulations and dynamic graphical representations of the resulting signaling network that can be explored in a manner that closely parallels wet lab experimental procedures. These features of Simmune were critical to the model development and analysis presented here and are likely to be useful in the computational investigation of many aspects of cell biology.",2006,PLoS Comput. Biol.
Ten simple rules in biomedical engineering to improve healthcare equity,"In an address at the Convention of the Medical Committee for Human Rights in 1966, Dr. Martin Luther King Jr. stated, “Of all the forms of inequality, injustice in healthcare is the most shocking and inhumane [1].” Despite this call to action, there remains a great divide in health outcomes today with statistics that are staggering and unjust. For example, babies born to black women in the United States die at more than double the rate of babies born to white women [2]; black patients have higher rates of mortality than white patients from many diseases, including inflammatory bowel diseases and cancer [3,4]; American Indians and Alaska Native populations experience increased rates of cardiovascular disease and related risk factors [5]. Women, especially black women, experience higher rates of myocardial infarction or fatal coronary heart disease [6,7]. Each of these instances illustrates the prevalence of health disparities in diseases, with racial and ethnic minority patients being 1.5 to 2 times more likely than white patients to have major chronic diseases [8]. Health disparities have been categorized across race and ethnicity, gender, sexual identity and orientation, disability status or special healthcare needs, and geographic location (rural and urban). The unprecedented nature of the Coronavirus Disease 2019 (COVIDAU : PleasenotethatCOVID 19hasbeenfullyspelledoutasCoronavirusDisease2019atitsfirstmentioninthesentenceTheunprecedentednatureoftheCoronavirusDisease2019ðCOVID 19Þpandemic:::Pleasecorrectifnecessary: -19) pandemic has brought these disparities into th spo light and reignited the conversation about how to improve health equity in our country. Health disparities are defined as preventable population-specific differences in the burden of disease, health outcomes, or access to healthcare [9]. Here we focus on healthcare disparities, which refers to differential access, use, and quality of medical care. Social determinants and implicit bias are well established as drivers of health disparities [10]; however, the impact of biomedical engineers who develop healthcare technologies that further propagate these inequities has only been implicitly stated. Given that nondiverse research teams have predominantly led medical device and therapeutic research, it is not surprising that the individual needs of different communities are often not considered in the design and optimization processes. This has resulted in numerous cases of technologies and therapies, be it unknowingly or not, that render the technology either ineffective or hazardous, in particular for women and racially minoritized populations. For example, pulse oximeters, which are used to monitor a patient’s supplemental oxygen needs and guide diagnostic decisions, were found to be 3 times less likely to detect hypoxemia in black patients as compared to white patients [11]. Furthermore, therapeutic dosing has been historically only determined in men whose metabolism is generally faster than that of women, leaving women at a higher risk [12,13], and in regard to biomaterial design, researchers have previously not considered differences in skeletal structure between men and women [14]. Other illustrative examples of these harmful oversights are discussed PLOS COMPUTATIONAL BIOLOGY",2022,PLoS Comput. Biol.
Ten simple rules for a successful EU Marie Skłodowska-Curie Actions Postdoctoral (MSCA) fellowship application,"The success of an academic career is dependent upon research accomplishments. The single most important factor for the success of a researcher on the academic job market is the scientific output, i.e., published papers [1], accompanied by other factors such as networking and participating in science communication. Academic mobility helps to create international collaborations and is seen very positively in job applications. Only 4% of researchers move to a new country, but mobile scholars have about 40% higher citation rates compared to nonmobile colleagues [2]. Further, researchers need to raise funding to successfully execute their own projects and to gain scientific independence. For this, postdocs and principal investigators need to know how to win grants independently [3,4]. The Marie Skłodowska-Curie Actions (MSCA) specifically support the aforementioned factors and help researchers to develop their skillsets. The MSCA is part of the ninth European research and innovation framework programme Horizon Europe and is the European Union’s (EU) flagship career development support programme with a budget totalling €6.6 billion from 2021 to 2027. The programme comprises a set of major research funding schemes including the renowned MSCA Postdoctoral Fellowships, Doctoral Networks, Staff Exchanges, Cofunding of regional, national, and international programmes (COFUND), and MSCA Citizens schemes. The aim of MSCA Postdoctoral Fellowships are to enhance the creative and innovative potential of postdoctoral researchers with new skillsets acquired through advanced training as well as international, interdisciplinary, and intersectoral mobility. MSCA Postdoctoral Fellowships support researchers with a doctoral degree and a maximum of 8 years of post-graduate experience. The mobility of researchers is a key requirement in all MSCA schemes. There is a general rule that researchers cannot have resided or carried out their main activity in the country of the host institution for more than 12 months in the 36 months immediately prior to the call deadline. This requirement is intended to direct researchers holding a PhD to move abroad to work on cutting-edge research projects. In the new Horizon Europe framework programme, there are 2 types of MSCA Postdoctoral Fellowships. The European Postdoctoral Fellowships support a stay of up to 24 months in any European host institution. The Global Postdoctoral Fellowships include an “outgoing phase” of 12 to 24 months to a host institution in a non-European country, followed by a mandatory “return phase” of 12 months at a host institution based in an EU Member State or a Horizon Europe Associated Country. In the MSCA programme, a host institution can be a public (university or research institute) or a PLOS COMPUTATIONAL BIOLOGY",2022,PLoS Comput. Biol.
Computationally Derived Points of Fragility of a Human Cascade Are Consistent with Current Therapeutic Strategies,"The role that mechanistic mathematical modeling and systems biology will play in molecular medicine and clinical development remains uncertain. In this study, mathematical modeling and sensitivity analysis were used to explore the working hypothesis that mechanistic models of human cascades, despite model uncertainty, can be computationally screened for points of fragility, and that these sensitive mechanisms could serve as therapeutic targets. We tested our working hypothesis by screening a model of the well-studied coagulation cascade, developed and validated from literature. The predicted sensitive mechanisms were then compared with the treatment literature. The model, composed of 92 proteins and 148 protein–protein interactions, was validated using 21 published datasets generated from two different quiescent in vitro coagulation models. Simulated platelet activation and thrombin generation profiles in the presence and absence of natural anticoagulants were consistent with measured values, with a mean correlation of 0.87 across all trials. Overall state sensitivity coefficients, which measure the robustness or fragility of a given mechanism, were calculated using a Monte Carlo strategy. In the absence of anticoagulants, fluid and surface phase factor X/activated factor X (fX/FXa) activity and thrombin-mediated platelet activation were found to be fragile, while fIX/FIXa and fVIII/FVIIIa activation and activity were robust. Both anti-fX/FXa and direct thrombin inhibitors are important classes of anticoagulants; for example, anti-fX/FXa inhibitors have FDA approval for the prevention of venous thromboembolism following surgical intervention and as an initial treatment for deep venous thrombosis and pulmonary embolism. Both in vitro and in vivo experimental evidence is reviewed supporting the prediction that fIX/FIXa activity is robust. When taken together, these results support our working hypothesis that computationally derived points of fragility of human relevant cascades could be used as a rational basis for target selection despite model uncertainty.",2007,PLoS Comput. Biol.
Ten simple rules for leveraging virtual interaction to build higher-level learning into bioinformatics short courses,"The emergence of the Coronavirus Disease 2019 (COVID-19) crisis forced training providers worldwide to move their face-to-face (F2F) courses to virtual environments, challenging course organisers to transfer successful F2F concepts into a virtual format. While recording traditional stand-and-deliver lectures was simple enough, facilitating interaction among course participants proved the key challenge, as virtual environments were new to trainees, trainers, and organisers. Interaction in learning always carries an element of risk, whether F2F or virtual. Sometimes, groups do not get along, a strong personality takes over a course discussion, or trainees get lost in activities as they do not feel comfortable asking for help. Interaction in a virtual environment amplifies this problem as disengaging is easier. Trainees can leave their camera and mic off, leave meetings, or simply not attend more easily than in a F2F setting. Trainers too can struggle, as they are unable to walk around the room peering at screens, easily monitoring trainee progress, and initiating casual chats. Trainees can then become lost and feel excluded. Creating a comfortable and efficient interactive learning atmosphere that satisfies a wide variety of learning preferences and home/office-working settings is challenging but crucial for an efficient learning experience in a classroom setting [1]. Indeed, well-planned and well-facilitated interactive learning ensures that trainees feel comfortable and included, allowing them to fully engage with each other, the trainers and the training material. In addition to learning, training courses provide networking opportunities—particularly crucial for early career researchers hoping to find collaborators. This networking can happen organically in a F2F setting during poster sessions, over coffee or during dinner. However, this is not without its challenges—more introverted trainees may feel uncomfortable in these networking situations. Entire books are dedicated to networking for those that dislike networking [2]. Delivering networking virtually requires extensive restructuring—simply leaving nibbles in a seminar room is no longer feasible—but also provides an opportunity to better capture those who feel alienated in F2F networking events. PLOS COMPUTATIONAL BIOLOGY",2022,PLoS Comput. Biol.
Molecular source attribution,"In the field of epidemiology, source attribution refers to a category of methods with the objective of reconstructing the transmission of an infectious disease from a specific source, such as a population, individual, or location. For example, source attribution methods may be used to trace the origin of a new pathogen that recently crossed from another host species into humans, or from one geographic region to another. It may be used to determine the common source of an outbreak of a foodborne infectious disease, such as a contaminated water supply. Finally, source attribution may be used to estimate the probability that an infection was transmitted from one specific individual to another, i.e., ""who infected whom"". Source attribution can play an important role in public health surveillance and management of infectious disease outbreaks. In practice, it tends to be a problem of statistical inference, because transmission events are seldom observed directly and may have occurred in the distant past. Thus, there is an unavoidable level of uncertainty when reconstructing transmission events from residual evidence, such as the spatial distribution of the disease. As a result, source attribution models often employ Bayesian methods that can accommodate substantial uncertainty in model parameters. Molecular source attribution is a subfield of source attribution that uses the molecular characteristics of the pathogen — most often its nucleic acid genome — to reconstruct transmission events. Many infectious diseases are routinely detected or characterized through genetic sequencing, which can be faster than culturing isolates in a reference laboratory and can identify specific strains of the pathogen at substantially higher precision than laboratory assays, such as antibody-based assays or drug susceptibility tests. On the other hand, analyzing the genetic (or whole genome) sequence data requires specialized computational methods to fit models of transmission. Consequently, molecular source attribution is a highly interdisciplinary area of molecular epidemiology that incorporates concepts and skills from mathematical statistics and modeling, microbiology, public health and computational biology. There are generally two ways that molecular data are used for source attribution. First, infections can be categorized into different ""subtypes"" that each corresponds to a unique molecular variety, or a cluster of similar varieties. Source attribution can then be inferred from the similarity of subtypes. Individual infections that belong to the same subtype are more likely to be related epidemiologically, including direct source-recipient transmission, because they have not substantially evolved away from their common ancestor. Similarly, we assume the true source population will have frequencies of subtypes that are more similar to the recipient population, relative to other potential sources. Second, molecular (genetic) sequences from PLOS COMPUTATIONAL BIOLOGY",2022,PLoS Comput. Biol.
Ten simple rules for organizing a webinar series,"The webinars series form part of the regular activities of any scientific consortium that aims to strengthen research activities and foster collaborations amongst the different partners. The webinars are intended to foster the exchange of ideas, build potential collaborations across multiple disciplines and enabling the participation and sharing of knowledge in current research. 
 
The Ten Simple Rules for organizing a webinar series can be summarized as follow: The webinar coordination team assists with all the planning and logistics for hosting a webinar (Rule 1); Choosing webinar themes requires mapping of the target audience needs and interest (Rule 2); Drafting a webinar planning checklist through regular planning meetings as well as post webinar meetings (Rule 3); Decentralized webinar organization of tasks and resources through accessible shared space (Rule 4); Planing early and settling on the provisional dates and times of the webinar events along with their themes (Rule 5); Choosing and settling on convenient and user friendly webinar platform (Rule 6); Approaching and confirming potential speakers (Rule 7); Obtaining the webinar title, abstract and presenter’s biography for creating the webinar announcement through emails and social media channels (Rule 8); Allocating time for the platform orientation (Rule 9); and Keeping close-up track of webinar metrics for regular assessment and evaluation (Rule 10). 
 
These Ten Simple Rules shared with the computational biology community will help those who have not yet ventured into training through webinars to learn from our experience. In our experience, the feedback from the post-webinar surveys clearly demonstrate that webinars are an effective way to create a two-way conversation between presenters and participants via a web-based platform.",2019,PLoS Comput. Biol.
Managing and Analyzing Next-Generation Sequence Data,"Centralized Bioinformatics Core Facilities provide shared resources for the computational and IT requirements of the investigators in their department or institution. As such, they must be able to effectively react to new types of experimental technology. Recently faced with an unprecedented flood of data generated by the next generation of DNA sequencers, these groups found it necessary to respond quickly and efficiently to the informatics and infrastructure demands. Centralized Facilities newly facing this challenge need to anticipate time and design considerations of necessary components, including infrastructure upgrades, staffing, and tools for data analyses and management. 
 
The evolution of the sequencing instrumentation is far from static. Sequence throughput from this new generation of instruments continues to increase exponentially at the same time that the cost of sequencing a genome continues to fall. These realities make the technology accessible to greater numbers of investigators while leading them to a greater usage of sequencing for a variety of experimental techniques, including variation discovery, whole transcriptome analysis, and DNA–protein interaction analysis. This places unique challenges upon the Bioinformatics Core Facility, whose mission could vary from the support of a single department or sequencing core to a Facility that supports many disparate and independent groups that run their own sequencers but rely on the Central Facility to host the informatics, research cyberinfrastructures, or both. It is worth noting that the initial investment in the instrument is accompanied by an almost equal investment in upgrading the informatics infrastructure of the institution, hiring staff to analyze the data produced by the instrument, and storing the data for future use. Many investigators do not realize that these extensive investments are necessary prior to purchasing the new technology. This is why it is advantageous to have a centralized Bioinformatics Core to put in place platforms that acquire, store, and analyze the very large datasets created by these instruments. A Bioinformatics Core, already familiar with data of this type and complexity, dedicated to investigators, and jointly working with IT personnel, can span multiple domains rather effortlessly. 
 
The large sequencing centers (e.g., Sanger, Broad Institute, and Washington University) have automated processes and architectures not generally replicable in medium and small sequencing groups. However, as these smaller groups obtain next-generation technology they can nevertheless learn lessons from the larger centers. Through collaboration and sharing best practices, small and medium-sized groups can prepare for the arrival of the technology and develop methods to manage and analyze the data. The Bioinfo-Core Special Interest Group [1], affiliated with the International Society for Computational Biology, has been actively collaborating to formulate best practices to assist small and medium-sized Cores in setting up platforms for next-generation sequencing. Here, we provide a Perspective for such a Core Facility in accomplishing this task, using collective experiences from Facilities that have solved many of these issues.",2009,PLoS Comput. Biol.
Ten simple rules for managing communications with a large number of coauthors,"The grand challenges of the 21st century, such as global change, biodiversity loss, and sustainable resource use, can only adequately be addressed through global collaboration and global solutions. Such solutions are ideally evidence based and require a mechanistic understanding of the underlying drivers and processes operating at different spatial–temporal scales. This requires the synthesis and analysis of large datasets to draw strong generalizations, while at the same time appreciating the context dependence and local deviations from these generalizations. Such cutting-edge advances in theoretical and applied research are therefore increasingly based on large collaborations [1]. Big research teams can integrate large and heterogeneous data sources, leverage a broad range of technical expertise, provide detailed insights in the local reality of different study systems, and wield advanced skills in statistics, computer programming, and interpretation. During the past decades, these factors have led to a substantial increase in the number of coauthors on scientific papers [2,3]. However, effectively communicating with a large number of coauthors presents multiple challenges. The objective of this guide is to provide recommendations for effective communication with large coauthor teams. The recommendations can be helpful for manuscripts involving relative few authors (approximately 10), but the benefits will increase as the size of your coauthor team grows to dozens or hundreds of coauthors. These “rules” are based on lessons learned from personal experience [4–6]. Of course, alternatives exist, but we hope that our suggestions will help to promote an efficient and equitable writing process. Numerous software applications have been developed to aid collaborative work (try googling “collaborative project software”). If you and your coauthors are already comfortable using one or more of these platforms (congratulations!), then some of our suggestions may seem obvious or irrelevant. However, many people may be unwilling or unable to engage with these specialized platforms because of a diversity of cultures, regions, backgrounds, or research fields among coauthors. This guide will be most valuable for lead author(s) of large coauthor teams for whom using specialized collaborative software is unrealistic or undesirable. Several important issues for large coauthor groups are nAU : Pleasenotethatasperstyle; italicsshouldnotbeusedforemphasis: ot addressed by this guide, such as how to determine eligibility for coauthorship or author order. Journals typically publish their standards for coauthorship (e.g., https://journals.plos.org/ploscompbiol/s/authorship), and many online resources can be found to guide the assessment what contributions should warrant coauthorship [7]. This guide neither focuses on the actual writing process (e.g., size and structure of the core lead writing team and division of labor), considerations which are PLOS COMPUTATIONAL BIOLOGY",2022,PLoS Comput. Biol.
UV-Induced Mutagenesis in Escherichia coli SOS Response: A Quantitative Model,"Escherichia coli bacteria respond to DNA damage by a highly orchestrated series of events known as the SOS response, regulated by transcription factors, protein–protein binding, and active protein degradation. We present a dynamical model of the UV-induced SOS response, incorporating mutagenesis by the error-prone polymerase, Pol V. In our model, mutagenesis depends on a combination of two key processes: damage counting by the replication forks and a long-term memory associated with the accumulation of UmuD′. Together, these provide a tight regulation of mutagenesis, resulting, we show, in a “digital” turn-on and turn-off of Pol V. Our model provides a compact view of the topology and design of the SOS network, pinpointing the specific functional role of each of the regulatory processes. In particular, we suggest that the recently observed second peak in the activity of promoters in the SOS regulon (Friedman et al., 2005, PLoS Biology 3(7): e238) is the result of positive feedback from Pol V to RecA filaments.",2007,PLoS Comput. Biol.
Ten quick tips for teaching with participatory live coding,"Participatory live coding is a technique in which a teacher or instructor writes and narrates code out loud as they teach and invites learners to join them by writing and executing the same code. Learners watch as an instructor writes code live in real time, typically via 1 or more projector screens that show the same screen as the instructor sees. Instructors also read out loud what they type, explaining the different elements and principles that are relevant for learners to understand the code. At the same time, each learner is invited to copy and execute the exact code or commands that are being written on their own work station. Learners thus “code-along” with the instructor. There are frequent, often short, exercises, in which learners are asked to solve a small relevant problem on their own. This approach aims to be an improvement on teaching programming through lecturing showing static code or relying on learners reading a textbook or compendium. What is taught is immediately applied rather than just shown on a slide or on paper: It embodies the “I do, we do, you do” approach to knowledge transfer that is used both formally and informally to teach everything from laboratory bench skills to grant writing [1]. It also slows the instructor down, giving learners more time to actively engage with the material before moving on to the next concept. Importantly, the thought process behind coding can also be made explicit. Learner’s questions can immediately be answered and misconceptions corrected by coding them. Exercises enable immediate practice using the material. Crucially, the technique also allows for teaching handling of mistakes. Beyond deliberately introducing mistakes during the live coding, instructors will often make unplanned mistakes. Novice learners are likely to make many such mistakes themselves, and diagnosing and solving mistakes is an integral aspect of learning programming. The participatory aspect engages learners, which helps them become active practitioners rather than passive observers of the programming process. Participatory live coding is most beneficial for novices who are unfamiliar with the tools. More experienced learners may gain enough by listening passively or engaging with the material and classroom differently. Participatory live coding for teaching programming should not be confused with live coding used to demonstrate software (for example, at a conference, with an audience passively observing) or live streaming programming [2] or live coding used as a form of performing art (e.g., while creating computer music [3]). A video recording demonstrating the participatory live coding technique can be found here: https://vimeo.com/139316669. PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
Ten Simple Rules for Organizing an Unconference,"An academic conference is a traditional platform for researchers and professionals to network and learn about recent developments and trends in a particular academic field [1–4]. Typically, the organizing committees and sponsors decide the main theme and sub-topics of the conference and select the presenters based on peer-reviewed papers [5]. The selected speakers usually share their research with a large audience by means of presentations and posters. However, the most stimulating discussions generally take place over coffee breaks when attendees can interact with each other and discuss various topics, including their own research interests, in a more informal manner [1, 6, 7], while expanding their own professional networks. An emphasis on facilitating such informal/networking interactions is a central focus of “unconventional conferences”—or “unconferences.” 
 
While many people may not yet have taken part in an unconference, the concept has been around for more than two decades. Events with unconference formats, beginning as early as 1985, include Open Space Technology, Foo Camp, BarCamp, Birds of a Feather, EdCamp, ScienceOnline, and many others. The success of these events has made the unconference format increasingly popular and widely known [8–11]. 
 
Unlike traditional conferences, an unconference is a participant-oriented meeting where the attendees decide on the agenda, discussion topics, workshops, and, often, even the time and venues. The informal and flexible program allows participants to suggest topics of their own interest and choose sessions accordingly. The format provides an excellent opportunity for researchers from diverse disciplines to work collaboratively on topics of common interest. The overarching goal for most unconferences is to prioritize conversation over presentation. In other words, the content for a session does not come from a select number of individuals at the front of the room, but is generated by all the attendees within the room, and, as such, every participant has an important role. 
 
Advantages of the unconference format include: a focus on topics that are relevant to the attendees (because they suggested them), an opportunity for teamwork development, flexibility of schedule, and an emphasis on contributions from every participant. The relationships built during an unconference often continue well past the event. The interactions can lead to productive collaborations, professional development opportunities, and a network of resources and are very effective at building a community amongst participants. The unconference format, therefore, gives participants experience in working together, and this can change how they think about their day-to-day work. 
 
A range of articles offer tips and advice for organizing and delivering aspects of scientific conferences and meetings or observations on features of successful meetings [5, 12, 13], including several from the PLOS Computational Biology “Ten Simple Rules” collection [14–16]. While the rules presented in this article are of particular relevance to the organization of unconferences, several of these points are also useful and complementary guidelines for organizing other kinds of events.",2015,PLoS Comput. Biol.
Ten simple rules to make your research more sustainable,"Sustainable development can be defined as a principle that regulates human activity without causing irreparable damage to the Earth’s natural system. It also aims to preserve resources so that future generations can benefit from them as much as present generations. To address the global challenges today’s world faces to manage the impact of human activity on the environment, the United Nations have defined a set of sustainable development goals to be achieved in the next decade [1]. The climate changes induced by human activities have been accelerating alarmingly as reported by scientists since 1979 [2]. Scientists can observe an increase in pollution (e.g., depletion of oxygen in water, eutrophication), natural resource scarcity, and a significant and accelerated loss of biodiversity. All these changes have led geologists to propose the Anthropocene as a new geological epoch, reflecting the impact of human activities on Earth’s ecosystems[3]. To mitigate this effect, a paradigmatic shift represented by sustainable development is needed in all fields of human activities, including research. As individuals and researchers, we are concerned with these challenges and deeply aware of the necessity to be more sustainable. But in practice, what does this entail? How can a researcher’s activity be “sustainable,” and how do we integrate sustainable practices into research projects? Where do we start? There is currently no global policy from French research institutes to federate collective action of the scientific community towards sustainable development goals, but working groups focusing on sustainable development have published recommendations [4]. There are also examples of good practice in the United Kingdom (S-Labs [5], Laboratory Efficiency Assessment Framework [6]) and the United States (International Institute for Sustainable Laboratories [7], My Green Lab [8]). Taking action to address the emergency situation is not only a moral responsibility we have as citizens but also a necessary contribution to gathering an understanding of the impact of research activities on the environment and how to make them more sustainable. This article is the result of the work carried out by the ""sustainable development"" committee created at the French Laboratoire d’informatique pour la Mécanique et les Sciences de l’Ingénieur (LIMSI) [9] in 2019 to bring together researchers interested in addressing these questions in the context of the activities of our laboratory, which conducts theoretical and experimental research in a diversity of scientific fields, including fluid mechanics, energetics, human language technology, human machine interaction, medical informatics, and augmented and virtual reality. The first task of the committee was to assess the carbon footprint of research activities over the year 2018 [10]. The next task is to analyze the results of this study and draw a roadmap PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
Network Modeling Identifies Molecular Functions Targeted by miR-204 to Suppress Head and Neck Tumor Metastasis,"Due to the large number of putative microRNA gene targets predicted by sequence-alignment databases and the relative low accuracy of such predictions which are conducted independently of biological context by design, systematic experimental identification and validation of every functional microRNA target is currently challenging. Consequently, biological studies have yet to identify, on a genome scale, key regulatory networks perturbed by altered microRNA functions in the context of cancer. In this report, we demonstrate for the first time how phenotypic knowledge of inheritable cancer traits and of risk factor loci can be utilized jointly with gene expression analysis to efficiently prioritize deregulated microRNAs for biological characterization. Using this approach we characterize miR-204 as a tumor suppressor microRNA and uncover previously unknown connections between microRNA regulation, network topology, and expression dynamics. Specifically, we validate 18 gene targets of miR-204 that show elevated mRNA expression and are enriched in biological processes associated with tumor progression in squamous cell carcinoma of the head and neck (HNSCC). We further demonstrate the enrichment of bottleneckness, a key molecular network topology, among miR-204 gene targets. Restoration of miR-204 function in HNSCC cell lines inhibits the expression of its functionally related gene targets, leads to the reduced adhesion, migration and invasion in vitro and attenuates experimental lung metastasis in vivo. As importantly, our investigation also provides experimental evidence linking the function of microRNAs that are located in the cancer-associated genomic regions (CAGRs) to the observed predisposition to human cancers. Specifically, we show miR-204 may serve as a tumor suppressor gene at the 9q21.1–22.3 CAGR locus, a well established risk factor locus in head and neck cancers for which tumor suppressor genes have not been identified. This new strategy that integrates expression profiling, genetics and novel computational biology approaches provides for improved efficiency in characterization and modeling of microRNA functions in cancer as compared to the state of art and is applicable to the investigation of microRNA functions in other biological processes and diseases.",2010,PLoS Comput. Biol.
Ten simple rules for reading a scientific paper,"A brief overview of the research question, approach, results, and interpretation. This is the road map or elevator pitch for an article. Introduction Several paragraphs (or less) to present the research question and why it is important. A newcomer to the field should get a crash course in the field from this section. Methods What was done? How was it done? Ideally, one should be able to recreate a project by reading the methods. In reality, the methods are often overly condensed. Sometimes greater detail is provided within a “Supplemental” section available online (see below). Results What was found? Paragraphs often begin with a statement like this: “To do X, we used approach Y to measure Z.” The results should be objective observations. Figures, tables, legends, and captions The data are presented in figures and tables. Legends and captions provide necessary information like abbreviations, summaries of methods, and clarifications. Discussion What do the results mean and how do they relate to previous findings in the literature? This is the perspective of the author(s) on the results and their ideas on what might be appropriate next steps. Often it may describe some (often not all!) strengths and limitations of the study: Pay attention to this self-reflection of the author(s) and consider whether you agree or would add to their ideas. Conclusion A brief summary of the implications of the results. References A list of previously published papers, datasets, or databases that were essential for the implementation of this project or interpretation of data. This section may be a valuable resource listing important papers within the field that are worth reading as well. Supplemental material Any additional methods, results, or information necessary to support the results or interpretations presented in the discussion. Supplemental data Essential datasets that are too large or cumbersome to include in the paper. Especially for papers that include “big data” (like sequencing or modeling results), this is often where the real, raw data is presented. Research articles typically contain each of these sections, although sometimes the “results” and “discussion” sections (or “discussion” and “conclusion” sections) are merged into one section. Additional sections may be included, based on request of the journal or the author(s). Keep in mind: If it was included, someone thought it was important for you to read. https://doi.org/10.1371/journal.pcbi.1008032.t002 PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008032 July 30, 2020 4 / 6",2020,PLoS Comput. Biol.
"Genome-Scale Reconstruction of Escherichia coli's Transcriptional and Translational Machinery: A Knowledge Base, Its Mathematical Formulation, and Its Functional Characterization","Metabolic network reconstructions represent valuable scaffolds for ‘-omics’ data integration and are used to computationally interrogate network properties. However, they do not explicitly account for the synthesis of macromolecules (i.e., proteins and RNA). Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations. Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron–sulfur cluster biogenesis. This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope. Furthermore, it was converted into a mathematical model and used to: (1) quantitatively integrate gene expression data as reaction constraints and (2) compute functional network states, which were compared to reported experimental data. For example, the model predicted accurately the ribosome production, without any parameterization. Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers. Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins. This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of ‘-omics’ datasets and thus the study of the mechanistic principles underlying the genotype–phenotype relationship.",2009,PLoS Comput. Biol.
Ten Simple Rules for organizing a non–real-time web conference,"The present work describes the 100% virtual ATIDES (Avances en Tecnologı́as, Innovación y Desafı́os de la Educación Superior) conference that was held between October 15 and 31, 2018, sponsored by Universitat Jaume I (UJI), Spain. Online conferences like this have been the subject of some controversy in the field of education over the last decade. Indeed, we have found a few texts that are against them. One of these is [1], whose authors claim that “interaction is not enough” to ensure efficient simulation of face-to-face contact. However, the Canadian academic community (for instance, the Centre for Distance Education at Athabasca University) is a strong advocate of online conferences (see [2,3]). Among other advantages, this kind of conference is “family-friendly,” i.e., they break barriers for researchers with family obligations [4], in particular many women [5]. In addition, these conferences overcome the drawback of parallel sessions at face-to-face conferences, at which participants must choose certain talks and miss others. Anderson and Anderson [6] even put forward environmental and economic arguments: “Transportation is a major contributor of carbon dioxide (CO2) emissions.” On the other hand, Abdullah [7] and Kear, Chetwynd, and Jefferis [8] look at the matter from another point of view that is also important: Social presence at online conferences. Gichora and colleagues [9] propose 10 rules for organizing a virtual conference, but they only focus on real-time virtual conferences. Real-time virtual conferences emulate the structure of well-known on-site conferences, with the use of video-conferencing tools for connecting the participants in a virtual meeting. We, on the other hand, focus on non–real-time web conferences. In this kind of conference, a web platform connects the participants, but the event does not take place live. Communication can be made through posts on a forum, which can sometimes take hours or even days. Non–real-time web conferences share many of the advantages of real-time virtual conferences, such as being family friendly and allowing the involvement of participants with a low budget for traveling, which makes this kind of conference more participative and inclusive. However, non–real-time web conferences have other advantages that real-time web conferences do not. Asynchronous interaction allows the attendees to have more time for reflection before they put forward their questions and comments, and, likewise, the speaker has enough time to prepare an appropriate answer. This makes up for the lack of face-to-face social interaction in this type of conference. In fact, numerous interactions occur via forums and are, in many cases, numerous and more informal and flexible than at classical scientific meetings [10]. Forums give rise to stimulating discussions like those that can occur at unconferences [11]. Furthermore, this asynchrony allows participants from all over the world to take part without worrying about time zone differences. PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
Seven quick tips for analysis scripts in neuroimaging,"In neuroimaging, as in many fields, the journey of the data from the measurement equipment to a figure in a publication is growing longer and more complicated. New preprocessing steps are added at the beginning of the pipeline [1–3], new multivariate techniques in the middle [4–6], and new statistical methods at the end [7]. Using these new techniques often requires writing pieces of programming code referred to as ""scripts,"" and, in accordance with the growing data analysis pipelines, these scripts also tend to increase in length and complexity. When programming code becomes sufficiently convoluted, even the most experienced programmers will make mistakes, which can ultimately lead to erroneous conclusions. Papers have had to be retracted due to programmer error [8–10]. Ideally, analysis code should be a testament to the scientific rigor and technological creativity of the researcher, in much the same way as the scientific articles they write. This means dealing with the disorder that arises as ideas get tried and discarded, mistakes are made and fixed, and smaller analyses are made on the side. The tips in this article aim to facilitate a successful organization of the analysis code, thereby keeping the complexity of data analysis code within tolerable limits and reducing errors. This article is a guide for those who have already written their own data analysis pipelines and wish to improve their designs. Those looking for information on how to get started writing a data analysis pipeline for neuroimaging are referred to other works [11–15]. There is a large body of literature on managing complexity and reducing the chance for programmer error in software [16–20]. However, most of it deals with the construction of programs and software libraries intended to be used across projects. This article focuses on analysis scripts that sit at the point at which the general purpose functionality that is exposed by software libraries meets the specific demands of a single study [21]. This makes scripts somewhat of an outlier in the software landscape, as they are pieces of code that are not reusable (pieces that need to be reusable are better implemented in a software library), only have to function correctly on one specific data set (hence there is little need to test for ""edge cases""), and will generally only be used by yourself and your collaborators (save the occasional run for review purposes and replication studies). Therefore, many of the standard practices of the software industry do not apply or need translation in order to arrive at concrete advice of what to do and what to avoid when writing analysis scripts. To move beyond mere truisms, the analysis pipeline developed by van Vliet and colleagues [22] has been extended to implement all tips in this paper and will be used as case study to discuss the practical consequences of each tip in terms of code. The example pipeline starts from the raw magnetoencephalography (MEG) and structural magnetic resonance imaging (MRI) data from the Wakeman and colleagues [23] data set and performs several artifact reduction steps, source estimations, functional connectivity analyses, cluster permutation statistics, and PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
Ten simple rules to make your publication look better,"and full text are considered independent text entities. They can be published in different places and must be readable when separated from each other. Repetition of an abbreviation may be useful at the beginning of a new section. Keep in mind that readers might read your paper in a different order than you wrote it, and especially, things like figure captions (see, e.g., “Writing an Effective Figure Legend” by M. Panter https:// www.aje.com/en/arc/writing-effective-figure-legend/) and table headers should be readable by themselves. Gene symbols are not abbreviations and may be used without explanation. There are a few abbreviations that are so common (in their respective domain) that you do not need to introduce them, e.g., DNA, RNA, et al., or PCR. Stick to one abbreviation per term, and once introduced, use it consequently. It helps to make a list of abbreviations while writing to keep an overview, although not all journals want a separate list of abbreviations submitted with the text. Rule 5: Use the correct form of Latin expressions and abbreviations Latin expressions and their abbreviations are traditionally written in italics, although some of them became so common that they are integrated in English language by now and not all writing guides require use of italics any more. The most commonly used Latin abbreviation is et alii (and others), whereas the correct abbreviation is et al. (note the single dot). Common examples are in vivo, in vitro, and in situ for experimental descriptions, vice versa (the opposite of what was said is also applicable), and e.g. (exempli gratia—for example) and i.e. (id est—in other words) which are frequently mixed up. Less often seen but still used is i.a. (inter alia) which translates to “among other(s)”. Rule 6: Use non-breaking spaces Non-breaking space (also referred as “no-break space” or “hard space”) is a special space character that does not allow a line break in that exact position. These are used to prevent a new line starting with something strange. The most common and only obligatory use in scientific literature is between numbers and their units (e.g., 10_μg), but non-breaking spaces can also be used between days and months (e.g., June_3rd), within a name (e.g., Jon_Doe, Elizabeth_II), or in any other situations in which breaking lines may be disruptive for the reader (e.g., in an infobox). Different text editing programs have different shortcuts to add those, e.g., in Microsoft Word, it is Ctrl-Shift-space, and in LaTeX, you can use “~”. Rule 7: Pay attention to the supplementary data and code Supplementary data and code are often very valuable as they allow reproduction and reuse of the materials described in the paper, and several data types like genetic sequences or omics data are mandatory to deposit and publish together with the paper. Make sure the link is persistent, and the data are actually there. The proper way to store raw or processed data or cite data (data citation according to DataCite standard) is described elsewhere [5–7]. For software, including code that is used to generate figures, software packages, and workflows, there are specialized repositories that allow version control and guarantee accessibility and online availability (e.g., GitHub and Bioconductor). See also reference [8] for a more detailed description of how to document or to cite [9] scientific software properly. Supplementary tables are often given as Microsoft Excel formats. The automatic recognition of gene symbols in Excel and transfer to a date format, e.g., SEPT7 becomes September 7, was a common problem; it occurred in up to 20% of supplementary data Excel files in the past [10]. However, this has been resolved by the latest update of the HGNC symbols. For example, SEPT7 is now SEPTIM7 and no longer triggers Excel’s auto format function. PLOS COMPUTATIONAL BIOLOGY Ten simple rules to make your publication look better PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008938 May 20, 2021 3 / 5 Make sure to reference any supplementary data or software appropriately in the text, whether in an external repository or supplementary table file, and link the identifier from the respective repository (e.g., Gene Expression Omnibus and ArrayExpress). Rule 8: Do a spelling and grammar check This is the simplest rule and possibly the one most often disregarded. Every advanced editing program has a built-in spelling and grammar check, which should be used at least once before submitting to find the most obvious and most embarrassing errors. Check also whether your own (language specific) automatic spelling correction mechanism did not change anything into more common words, especially when it comes to gene names and symbols (see Rule 7). Then, there are the errors that the built-in spelling and grammar check will not find (yet). For example, differentiating between “a treating physician” and “a threatening physician” cannot currently be performed automatically, but only by careful proofreading. In addition to proofreading, make sure to write in the correct tense. We recommend the present tense for the Introduction because it is a description of the present state of the art. Materials and methods and Results are usually written in the past tense as these are things you have performed and have found in the past. Try to minimize passive language for better readability. Good spelling and grammar checking is, however, not a replacement for well-written English. If none of the authors are fluent in English, it remains a good idea to have someone check it who is. Rule 9: Review the document again after creating the final version Before creating a pdf for submission or for printing, make sure that all track changes and comments are resolved and removed from the text. During the creation of a pdf, weird things may happen to your text, so it is important to review the product again at least one more time. Text blocks and items like figures, tables, or headings may have moved. Make sure that headings are not left at the bottom of a page when they should be at the top of the next page. If you have figures or tables in the text, check that they are not separated from their captions or headings. In contrast to graduation thesis, journals usually require the figures to be submitted separately and the tables to be added at the end. Check if specifically formatted text or inserted items (e.g., formulas, special characters, or references) are still shown correctly, and check if hyperlinks to internal (headings) or external (websites) sources work. Rule 10: Accept that there will be errors The aforementioned nine simple rules should help to catch most of the errors and minimize mistakes for a more professional presentation of your work. Despite all effort and proofreading it may, and at some point will, happen that you find an error in your published work. Forgive yourself; absolute perfection is unrealistic. Remember the perfect can even be the enemy of the good—at some stage, you have a valuable manuscript that you want to be published without further delay. If the error is of such a type that you really, really want to remove it from the publication, the first thing to do is talk to and reach an agreement with the coauthors so they support the decision, and then contact the editor of the journal. Together with the editor, you decide if a new version of record needs to be created. For this, you will have to write a corrigendum or an erratum, which will accompany the paper forever. PLOS COMPUTATIONAL BIOLOGY Ten simple rules to make your publication look better PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008938 May 20, 2021 4 / 5",2021,PLoS Comput. Biol.
A Quick Guide for Building a Successful Bioinformatics Community,"“Scientific community” refers to a group of people collaborating together on scientific-research-related activities who also share common goals, interests, and values. Such communities play a key role in many bioinformatics activities. Communities may be linked to a specific location or institute, or involve people working at many different institutions and locations. Education and training is typically an important component of these communities, providing a valuable context in which to develop skills and expertise, while also strengthening links and relationships within the community. Scientific communities facilitate: (i) the exchange and development of ideas and expertise; (ii) career development; (iii) coordinated funding activities; (iv) interactions and engagement with professionals from other fields; and (v) other activities beneficial to individual participants, communities, and the scientific field as a whole. It is thus beneficial at many different levels to understand the general features of successful, high-impact bioinformatics communities; how individual participants can contribute to the success of these communities; and the role of education and training within these communities. We present here a quick guide to building and maintaining a successful, high-impact bioinformatics community, along with an overview of the general benefits of participating in such communities. This article grew out of contributions made by organizers, presenters, panelists, and other participants of the ISMB/ECCB 2013 workshop “The ‘How To Guide’ for Establishing a Successful Bioinformatics Network” at the 21st Annual International Conference on Intelligent Systems for Molecular Biology (ISMB) and the 12th European Conference on Computational Biology (ECCB).",2015,PLoS Comput. Biol.
Ten simple rules for more objective decision-making,"Scientists spend their lives analyzing data by the systematic study of the structure and behavior of the physical and natural world using both observation and experiment—objective analysis. But when it comes to decision-making, scientists are also humans with accompanying subjectivity. Put colloquially, we have both heart and head—and are capable of being simultaneously subjective and objective. Here we posit that bringing more objectivity (""head"") to decisions is a good thing. It’s a key part of ""critical thinking,"" the ""Socratic questioning"" method. We are not suggesting, that like Mr. Spock, we should be driven entirely by rationality, nor are we considering the merits of various reasoning systems [1]; we are simply examining why greater objectivity helps in providing a simple way to achieve improved objectivity. So, to start, is objectivity indeed better than subjectivity? To address this question, it’s useful to look at the 2 opposite ends of the spectrum: objectivity is really the application of pure logic (something is either right or wrong, more or less, etc.), whereas subjectivity [2] is embodied in the form of what is often called Cartesian Doubt or skepticism (that the knowledge of anything outside ones direct experience has to be considered as unsure). In certain cases, increased objectivity is superior, for example, when the decision being taken leads toward a measurable or quantifiable outcome: if there is a specific goal in mind, then it’s very useful to be able to estimate how close that decision might get you to that goal before you set out on the path. In real life, most decisions are a mixture of head and heart, but with these rules, we hope to increase both the accuracy and quantity of the head part while not neglecting the heart. But enough of the epistemological concepts, what we want is to make better decisions (better here being more objective) and look at 10 ways in which we might do this, culminating in a simple tool that anyone with a spreadsheet (or even a pen and paper) can use. Each rule is accompanied by a use case, some drawn from 2 previous Ten simple rules: Ten simple rules for graduate students [3] and Ten simple rules for selecting a postdoctoral position [4]. We will culminate with a worked example that illustrates this approach. Every lab needs a good coffee machine, and we are inspired by the example of the famous Trojan Room coffee pot. Based in the old computer laboratory of the University of Cambridge, England, in 1991, it provided the inspiration for the world’s first webcam [5]. So here we show how to make sure your coffee is well up to par! PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
The Roots of Bioinformatics in Protein Evolution,"Perspective The Roots of Bioinformatics in Protein Evolution Russell F. Doolittle 1,2 * 1 Department of Chemistry & Biochemistry, University of California, San Diego, La Jolla, California, United States of America, 2 Department of Molecular Biology, University of California, San Diego, La Jolla, California, United States of America Introduction Bioinformatics as a formal discipline came of age in the late 1980s, greatly stimulated by the 1989 Human Genome Initiative. The roots of the field go back several decades earlier, however, to an era when computers were not needed to manage the data. In this personal reflec- tion, I review the confluence of events beginning in the 1950s that brought a number of fields together in a common pursuit. Particularly, I offer some com- ments about early amino acid sequence comparisons, the results of which revealed so much about evolution, and how the computer became necessary only when the number of known sequences began to grow exponentially. Many other authors have already re- corded their thoughts on the evolutionary roots of bioinformatics in accounts that are doubtless more thorough and balanced than can be recorded in this brief personal reflection ([1,2], inter alia). All are in agreement about certain pivotal events that were true milestones: the double-helix model of DNA, the first determination of the amino acid sequence of a protein, and the conceptual linking of DNA sequences and protein sequences. My plan is to expand on some related matters with the hope of providing some additional back- ground on those early scenes. Sequences Sequences, the simple order of individual units in biological polymers, are at the heart of bioinformatics, and the search for relationships among them and the recon- struction of their histories has arguably proved the most informative of biological inquiries. Today dozens of giant data banks store what seem to be countless numbers of nucleic acid and protein sequences. But there was a time, only 50 or 60 years ago, when hardly any sequences were known at all. Nonetheless, there were those who already appreciated that the web of all life would eventually be reconstructed on the basis of sequence data alone. There was an obligatory progression of events, beginning with chemistry, then biology, and, finally, the need for computers. Among the technological advances that made sequence determinations possible, two are extremely notable: the introduc- tion in the 1940s of paper chromatogra- phy as a simple tool for identifying amino acids and their derivatives [3], for one, and the use of suitable chemical reagents that reacted (more or less) exclusively with amino groups, for another—particularly an amino-tagging reagent by Sanger [4] and an amino acid-labilizing reagent by Edman [5]. Some important details of their seminal and unique contributions need to be described here, however briefly. Chemistry It must be difficult for a young scientist today to imagine how primitive circum- stances were in the mid-20th century. The effort needed to determine even a short amino acid sequence was more than considerable; it was daunting (some of that tedium may carry through in the following description). Typically, the first step in determining the sequence of a peptide or protein was to establish its amino acid composition. It was well known that heating a protein or peptide with strong aqueous acid broke the bonds between the constituent amino acids (unhappily, glutamines and aspara- gines were changed into glutamic and aspartic acids in the process, and a few other amino acids like tryptophan dam- aged). The resulting hydrolysate could be spotted on a large piece of filter paper and separation of the various amino acids obtained by letting an organic solvent creep over the paper, partitioning the amino acids according to their relative solubilities in one phase or the other. The locations of the amino acids could be found by staining the dried paper with ninhydrin, a compound that gave a blue color with amino groups. After a preliminary amino acid compo- sition was in hand, the next step was to break the protein or peptide into smaller pieces (the ‘‘divide and conquer’’ strategy). The simplest method was to use partial acid hydrolysis, taking advantage of the fact that bonds next to some amino acids break more easily than others. The other popular option was to use proteolytic enzymes like trypsin or chymotrypsin. In either case, the peptide fragments were purified, often by paper chromatography, and their individual amino acid composi- tions determined. Indeed, one reason that protein se- quences were attacked first, rather than RNA or DNA, was because there were 20 different amino acids, and a random, partial hydrolysis of a polypeptide chain could give rise to smaller peptides with unique compositions. The logistics of the same approach for a polymer made of only four different things was impossible to contemplate. More Chemistry The Sanger reagent, fluorodinitroben- zene (FDNB), had several important features. First, the bond between it and the tagged amino acid was resistant to acid hydrolysis; second, the derivatized amino acid was sufficiently non-polar that it could be extracted from the acid hydroly- sate with an organic solvent like ether; and finally, the derivatives were bright yellow and could be readily identified by paper chromatography. The operation could be conducted on the starting peptide or protein, as well as on the fragments generated by various means. It was a slow Citation: Doolittle RF (2010) The Roots of Bioinformatics in Protein Evolution. PLoS Comput Biol 6(7): e1000875. doi:10.1371/journal.pcbi.1000875 Editor: David B. Searls, Philadelphia, United States of America Published July 29, 2010 Copyright: s 2010 Russell F. Doolittle. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: The author received no specific funding for this work. Competing Interests: The author has declared that no competing interests exist. * E-mail: rdoolittle@ucsd.edu PLoS Computational Biology | www.ploscompbiol.org July 2010 | Volume 6 | Issue 7 | e1000875",2010,PLoS Comput. Biol.
“Broadband” Bioinformatics Skills Transfer with the Knowledge Transfer Programme (KTP): Educational Model for Upliftment and Sustainable Development,"A shortage of practical skills and relevant expertise is possibly the primary obstacle to social upliftment and sustainable development in Africa. The “omics” fields, especially genomics, are increasingly dependent on the effective interpretation of large and complex sets of data. Despite abundant natural resources and population sizes comparable with many first-world countries from which talent could be drawn, countries in Africa still lag far behind the rest of the world in terms of specialized skills development. Moreover, there are serious concerns about disparities between countries within the continent. The multidisciplinary nature of the bioinformatics field, coupled with rare and depleting expertise, is a critical problem for the advancement of bioinformatics in Africa. We propose a formalized matchmaking system, which is aimed at reversing this trend, by introducing the Knowledge Transfer Programme (KTP). Instead of individual researchers travelling to other labs to learn, researchers with desirable skills are invited to join African research groups for six weeks to six months. Visiting researchers or trainers will pass on their expertise to multiple people simultaneously in their local environments, thus increasing the efficiency of knowledge transference. In return, visiting researchers have the opportunity to develop professional contacts, gain industry work experience, work with novel datasets, and strengthen and support their ongoing research. The KTP develops a network with a centralized hub through which groups and individuals are put into contact with one another and exchanges are facilitated by connecting both parties with potential funding sources. This is part of the PLOS Computational Biology Education collection.",2015,PLoS Comput. Biol.
Ten simple rules for typographically appealing scientific texts,"Text is ubiquitous in everyday scientific work—when was the last time you spent 5 minutes working without writing, reading, or interacting with any kind of equipment that had text (scales, labels, brand name, etc.) on it? Most forms of communicating ideas and findings in science are based on text, e.g., BSc/MSc/PhD theses, manuscript drafts, grant proposals, reports, or job applications. In addition, text appears in figures, (electronic) slides for presentations, and posters, i.e., in formats focused more strongly on a graphical presentation. All these documents are usually written to convince the audience of the quality of your ideas or results, ultimately with the goal of a positive evaluation (grading, decision on funding/ hiring, etc.). A good visual appearance of the text and graphical elements is key for making a good first impression on the audience. When sustaining this impression by clearly structured and well-written text, professional layout is again important because less-than-optimally typeset texts distract the audience from fully appreciating the high-quality content [1]. Even though single visual inconsistencies cost the readers only a fraction of a second, these interruptions to the flow of reading add up and subconsciously frustrate the readers, possibly undermining your credibility. Poor visual appearance and language can be spotted at first glance in Fig 1, and incorrect content (or a confusing structure, not shown in Fig 1) take much longer to notice. Properly formatting text is particularly challenging in interdisciplinary fields like Computational Biology, where authors are faced with a variety of text elements, e.g., Greek characters, mathematical formulas, chemical formulas, and source code listings. Similar to inconsistent writing style, inconsistent formatting may indicate plagiarism, e.g., stray dashes resulting from copying and pasting hyphenated text, garbled characters, and fonts/formatting copied from the source. Scientists frequently need to produce final document layout themselves, either from scratch or based on a template—where some templates are well designed and others are, well, “designed.” If a template is given, fewer decisions need to be made, but some typographical knowledge is still helpful to understand the template and to deal with issues unforeseen therein. Ideally, the actual typesetting is subsequently done by trained professionals, e.g., working with publishers, who know what they are doing [2]. Submissions should, in this case, follow the publishers’ guidelines and templates, but still be prepared carefully, as “reviewers’ opinion about a manuscript can be skewed by careless formatting” [3]. Typography is thus one of the tools of the trade for scientists. This article is meant as a practical guide for typesetting scientific texts, including motivation for the recommendations. While focusing on the intended layout, the rules also provide hints on how these results can be obtained in common text processing/typesetting tools (such as Microsoft Word/LibreOffice Writer, Google Docs, and LaTeX). These rules are meant to complement PLOS COMPUTATIONAL BIOLOGY",2020,PLoS Comput. Biol.
Let's Make Those Book Chapters Open Too!,"As authors, many of us have had less than satisfactory experiences in writing book chapters as part of a themed volume or textbook that, when published, are expensive, inaccessible, and cited and used infrequently through lack of availability [1]. It could even be argued we write them from some sense of obligation and need, but do not put our best science and efforts into them because we know they won't be read and hence cited. Putting that thought aside, let's just say that a lot of good science goes underutilized. Some finds its way into journal reviews and journals that specialize in such content, for example the Elsevier Current Opinions series, but much languishes. I, many of the PLOS editors, and PLOS management have long wanted this situation to change; well, now it has. 
 
The value of themed hardcover volumes and textbooks was understandable in a purely print era, an era during which we frequented the library more, which is where these volumes resided, being too expensive for individuals to purchase. These volumes make no sense today in a digital, open-access world. We are proud to report that PLOS Computational Biology has taken the first steps to address this nonsense. Translational Bioinformatics, edited by Guest Editor Maricel Kann and Education Editor Fran Lewitter, is the first complete PLOS “book” that can be accessed online as individual chapters or downloaded as a complete volume [2]. An ePub version is now available too. The content is indexed, each chapter has a Digital Object Identifier (doi) assigned and hence is resolvable (i.e., uniquely findable), and is indexed in PubMed and available as full text from PubMed Central, like all PLOS content. 
 
Translational Bioinformatics can be used as a reference guide or textbook, and includes exercises. After review, the authors have had their hard work rewarded through a PLOS citation and greater accessibility to their work. The book uses the PLOS collection feature to bring the content together into a single entity while retaining the individuality of each article. While we regard this as an important step forward, it does raise some questions. 
 
The first question is, who pays? Obviously, we are strong proponents of open access, but also the first to admit there must be a business model if open-access content is to be persistent. Certainly, most of us have never made any money from writing specialized book chapters contributed to a volume, but would we pay a modest amount to have them published under an open–access license? This remains an open question at this time. PLOS met the cost of publishing Translational Bioinformatics, but if this approach to book chapters were to take off, someone will have to foot the bill. At this time, PLOS is interested in furthering this cause, and, as with all front matter within the Education Section of the journal, book chapters are not subject to publishing fees. If the demand becomes too great we will need to revisit this. Support for chapter content would seem an opportunity for a wonderful contribution by an individual philanthropist or foundation in furthering scientific dissemination. 
 
The second question is, what quality of review do we require of chapter content? In general terms, solicited book chapters do not undergo the level of review found in a research article. Unless the content is terrible, the editors soliciting the material are hard pressed to reject it, having persuaded the authors to write it in the first place. Good editors, as we have here, will provide the level of review found in a research article. Moreover, with greater exposure and article-level metrics (ALMs) applied to each chapter, the content will rise or fall on its own merits and the end result will likely be higher quality content than we have traditionally seen from book chapters. With regard to PLOS Computational Biology specifically, we regard this book content as front matter in the Education Section, and as such it has had significant review. We will be revisiting the issue of review, and indeed all aspects of the scope of our support for chapters, as demand increases. 
 
The third question is, what do we lose and gain in an online book? Of course there are the obvious issues, now long debated, regarding ebooks versus physical books, and there is no need to revisit that here. What is worth visiting are the specific issues surrounding a book publication by PLOS, an organization that is currently set up to operate as a journal publisher. PLOS has been wonderful in making this project happen and paving the way for more through the notion of collections. Collections do present challenges when used to represent a book. For example, the collection has no ISBN or other book-like identifier defining the citation; books have editions, whereas there is no notion of versioning in a collection. On the positive side, the collection can become a dynamic entity, such that new chapters (with their own journal-like citation) can be added to the collection at any time. 
 
Open questions there might be, but an exciting time nevertheless, with PLOS continuing to push the envelope regarding scholarly communication. Over time we will sort this out, but in the meantime, enjoy Translational Bioinformatics, a new innovation in open-access publishing.",2013,PLoS Comput. Biol.
Ten simple rules to aid in achieving a vision,"In a career that now spans 40 years, I have had, on several occasions, opportunities to turn loose ideas into a unified vision and the resources to implement that vision. What do I mean by a vision? A vision, at least in my mind, is the ability to see something important to the future, perhaps before others do. Fulfilling that vision does not have to change the whole world (although that would be nice) but only to impact others in a positive way. Consider what I perceive has been my own visioning to provide some context. My visioning began in the 1990s when, seeing what computation was bringing to the life sciences through work on the human genome project, I was lucky enough to be able to envision and establish a bioinformatics laboratory before the idea became mainstream. In the early 2000s, it was a collective vision for what an exemplar data resource, namely, the Research Collaboratory for Structural Bioinformatics (RCSB) Protein Data Bank (PDB), should achieve. Around 2005, it was something dear to this readership, a vision for a new journal, PLOS Computational Biology, for which I was cofounder and Founding Editor-in-Chief for 7 years. Around 2007, it was forming a company, SciVee.tv, to envision how digital media other than print could be used to communicate science. Around 2014, as the first Associate Director for Data Science (ADDS) for the United States National Institutes of Health (NIH), the vision was how big data could catalyze change in life sciences research. Finally, now in 2019, the vision is how one of the first academic Schools of Data Science should be established and run. These either were, or are, great opportunities to lay out a vision and act upon that blueprint. Not all were successful (PLOS Computational Biology was), but all were learnt from. So, what did I learn? Here is at least part of my life’s lesson, in that now familiar and comfortable Ten Simple Rules format. In this article, the rules are generic and can be considered beyond our own discipline.",2019,PLoS Comput. Biol.
What Do I Want from the Publisher of the Future?,"When I took on the role of Editor-in-Chief of this open-access journal, I began,for the first time, to think about scholarlycommunication beyond submitting my pa-pers and getting them published. Thisthinking led to previous Perspectives [1–3],all of which shared an underlying theme—there are many opportunities to achievebetter dissemination and comprehension ofour science, and as producers of that outputI believe authors have a responsibility to seeit used in the best possible way.No need to take my word regarding theopportunities that exist to improve schol-arly communication and comprehension. Irecommend reading ‘‘Part 4: ScholarlyCommunication’’ from the free onlinebook the Fourth Paradigm: Data Intensive Scien-tific Discovery [4] (http://research.microsoft.com/en-us/collaboration/fourthparadigm/),which is a tribute to the late Turing Awardwinner Jim Gray. Jim, and many of theauthors who pay homage to his vision, havethought deeply about the subject of scholarlycommunication. They conclude that dataand knowledge-driven computation is indeeda fourth wave, as computation has impactedscience to the point where every aspect of itis touched by computation (hence the nameeScience), including dissemination and com-prehension. These visionaries recognize thatwe are at a tipping point at which scholarlycommunication will change from a tradi-tional print-oriented medium (albeit an on-line version of the print journal) to somethingelse. That something else begins to transformtoday’s research article as we realize thepower of the medium, establish new forms ofknowledge discovery, and measure theimpact of scholarly contributions in newways. For all that vision, these luminaries donot address the question that I have beenpondering, and which I would like to raisehere. Assuming all this innovation takesplace, what will the publisher of the futurelook like, and as a contributor and consumerof a publisher’s services in this new era, whatdo I want from the publisher of the future?Recently, at gatherings of publisherswhere I have been invited to speak, I havebeen trying to pose and then answer thisquestion. Unfortunately, I fear that what Ipropose appears so radical as to be greetedwitheitherblankstaresorlooksof getreal.LetmetryheretodoabetterjobatstatingwhatI want from my publisher in the future.Many of you are undoubtedly thinkingthat just accepting your papers will beenough, but bear with me. Presumably,publishing will continue in the life sciences(unless we go over completely to anArXiv.org or similar model where articlesare simply deposited without peer reviewand impact measured by how much theyare accessed), and if so, will continue to beoverseen by the publishers we, as scientists,work with today. A few new and innova-tive publishers like the Public Library ofScience (PLoS) will continue to emerge asbusiness models and practices change, butexisting publishers will probably adapt inthis new era. I anticipate similarities toearlier phases of the Internet revolution.Amazon.com emerged as a new and majoronline-only shopping entity, but Sears,Wal-Mart, Harrods, etc., while beingslower in adopting the new medium, dideventually successfully support onlineshopping and a range of new services. Bycomparison, a few innovators have hadsome impact on scholarly communication,but traditional science, technology, andmedical (STM) publishers will continue todominate the conservative and relativelyslow-moving market. These pioneeringpublishers are now experimenting withinteractive PDFs, ‘‘articles of the future,’’semantic tagging, data integration withresearch articles, incorporating rich media(video and podcasts), and so on. Mostlikely, at some point these innovations willbecome mainstream through increasedintroduction by traditional publishers, butthen what? Stated another way, if wefinally move away from the traditionalPDF to something more dynamic thatintegrates data, rich media, and includesinteractive access, what do I as a scientistwant from publishers at that point?To answer this question, let us start withwhere we are today. As authors, we put anenormous amount of effort into producingapublishablemanuscript.Atsomepointwepass it over to the publisher without asecondthought.Subsequently,wewillputalarge amount of effort into a revision orrebuttal letter, but again, there is nothought on what will happen to our workafterithasbeenacceptedbeyondthedateitwill be published and appear in PubMed.There is an enormous amount of trust inour publisher that our creations will behandled in the best possible way and, whenpublished, that they will be disseminated toallwhowanttoreadourwork.Openaccessintroduced a hairline fracture in this trustwith some scientists realizing that perhapstheir work was notbeingaswidely accessedaspossible.Nevertheless,mostscientistsstilldo not think seriously about limited accessand signing away the copyright. After alloureffortsatproducingapaper,veryfewofus have asked the question, is journal xpresenting my work in a way that maxi-mizes the understanding of what has beendone, providing the means to ensuremaximum reproducibility of what has beendone, and maximizing the outreach of mywork? I would suggest that now is the timenottojusttossthepaperoverahighbarrierto the journal and forget about it, but tobreak down the barrier and have a newform of interaction and dialog with apublisher who is prepared to embrace achanging publishing model and cananswerthe question in a satisfactory manner. Inother words, we have an interaction withthe publisher that does not begin when thescientific process ends, but begins at thebeginning of the scientific process itself.Perhaps you are beginning to see why Iget so many blank stares when I raise thisissue with scientists (producers and consum-",2010,PLoS Comput. Biol.
The Signal in the Genomes,"Nostra culpa. Not only did we foist a hastily conceived and incorrectly executed simulation on an overworked RECOMB conference program committee, but worse—nostra maxima culpa—we obliged a team of high-powered researchers to clean up after us! It was never our intention to introduce an alternative way of constructing synteny blocks; the so-called ST-synteny was only a (bungled) attempt to mimic Pevzner and Tesler's method, based on our reading or misreading of their paper [1]. Moreover, shortly after the conference, before preparing the full journal version of our article, we recognized through a back-of-an-envelope calculation that realistic values of the parameters in our simulations would not produce much increase in reuse rate. Consequently, our published article [2] develops only the main part of our communication, modeling and simulating the artifactual increase in reuse rates due to deleting synteny blocks but not that due to the construction of synteny blocks. 
 
Unfortunately, our makeshift work distracted from the main point of our communication. The theme in our full article [2], in the RECOMB extended abstract, and elsewhere is not substantially confronted in the recently published PLoS Computational Biology paper by Glenn Tesler and colleagues [3]. Wherever high rates of breakpoint reuse are inferred, whether they are due to bona fide reuse or rather to violations in the assumptions justifying the use of particular algorithms (relating to the construction of synteny blocks or their size thresholds, or to the unrealistically limited repertoire of rearrangement processes recognized by the algorithm), there is a correspondingly high rate of loss in the historical signal. 
 
While two genomes diverge without breakpoint reuse, the historical signal is conserved in the breakpoint graph, which consists entirely of four-vertex cycles, specifying exactly which pairs of breakpoints must be healed by reversals or translocations. As breakpoints are reused—as they eventually must be for finite gene orders, or for genomic sequence, where there are criteria for deciding when two breakpoints are too close together to be considered distinct—the four-vertex cycles are merged into larger structures, and the breakpoint graph becomes ambiguous concerning the rearrangements that produced it. The two divergent genomes eventually become randomized with respect to each other. But this randomization also occurs, even if divergence involves only distinct breakpoints, when the assumptions underlying the use of genome rearrangement algorithms are violated, which can happen in many possible ways [4,5]. And we cannot infer whether mutually randomized synteny block orderings derived from two divergent genomes were created through bona fide breakpoint reuse or rather through noise introduced in block construction or through processes other than reversal and translocation. 
 
I illustrate this point with data on the human/mouse comparison from Pevzner and Tesler's more detailed paper [6]. We simulated 100 pairs of genomes constructed of 22 and 19 human and mouse autosomes, with 270 blocks distributed exactly as in the human and mouse genomes, except that the blocks were randomly permuted and sign—or strandedness—was assigned randomly to each block. Permutations are within, not between, chromosomes, assuring a realistic reversals/translocations ratio. Output from the standard rearrangement algorithm [7] is summarized in Table 1. 
 
 
 
Table 1 
 
Human/Mouse Comparison Resembles Randomized Genome Comparison 
 
 
 
The human/mouse comparison parallels the randomized genomes, and both deviate drastically from the hypothetical case of 270 blocks evolving without breakpoint reuse. There is an excess of 22 four-cycles and three other small cycles in the real data, largely due to reversals within concatenated blocks from a single chromosome in both human and mouse, largely dispersed in the randomized chromosomes. These 25 are what remains of the detailed evolutionary signal; they account for the small differences in distance, in breakpoint reuse, and in the total number of cycles. The giant cycles celebrated in Pevzner and Tesler's paper [6] and Tesler and colleagues' paper [3] have almost identical structure in the human/mouse and randomized comparisons. 
 
Note that in contrast to the autosomes, the rearrangement analysis of the human and mouse X chromosomes involves only short cycles, a breakpoint reuse rate close to 1.0 and a clear evolutionary signal. 
 
In conclusion, I take issue neither with Pevzner and Tesler's ingenious method for constructing synteny blocks nor with the notion that genomes are spatially heterogeneous in their susceptibility to rearrangement; many types of genomic regions, as reviewed in a previously published paper [5], have documented elevated rates of rearrangement. Nevertheless, a high reuse rate in the output of rearrangement algorithms, which simply indicates loss of signal, is not good evidence for fragile regions. The output of comparisons of randomized genomes has the same characteristics—namely, similar rearrangement distance, similar cycle/path sizes, similar number of chromosomes touched by each large cycle, similar reuse rates, and similar estimates [8] of the number of translocations and reversals.",2006,PLoS Comput. Biol.
Ten Simple Rules for Organizing a Scientific Meeting,"Scientific meetings come in various flavors—from one-day focused workshops of 1–20 people to large-scale multiple-day meetings of 1,000 or more delegates, including keynotes, sessions, posters, social events, and so on. These ten rules are intended to provide insights into organizing meetings across the scale. 
 
Scientific meetings are at the heart of a scientist's professional life since they provide an invaluable opportunity for learning, networking, and exploring new ideas. In addition, meetings should be enjoyable experiences that add exciting breaks to the usual routine in the laboratory. Being involved in organizing these meetings later in your career is a community responsibility. Being involved in the organization early in your career is a valuable learning experience [1]. First, it provides visibility and gets your name and face known in the community. Second, it is useful for developing essential skills in organization, management, team work, and financial responsibility, all of which are useful in your later career. Notwithstanding, it takes a lot of time, and agreeing to help organize a meeting should be considered in the context of your need to get your research done and so is also a lesson in time management. What follows are the experiences of graduate students in organizing scientific meetings with some editorial oversight from someone more senior (PEB) who has organized a number of major meetings over the years. 
 
The International Society for Computational Biology (ISCB) Student Council [2] is an organization within the ISCB that caters to computational biologists early in their career. The ISCB Student Council provides activities and events to its members that facilitate their scientific development. From our experience in organizing the Student Council Symposium [3],[4], a meeting that so far has been held within the context of the ISMB [5],[6] and ECCB conferences, we have gained knowledge that is typically not part of an academic curriculum and which is embodied in the following ten rules.",2008,PLoS Comput. Biol.
